{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DummyXarray","text":"<p>A lightweight xarray-like object for building dataset metadata specifications before creating actual xarray datasets.</p>"},{"location":"#overview","title":"Overview","text":"<p>dummyxarray allows you to define the structure of your dataset including dimensions, coordinates, variables, and metadata before actually creating the xarray.Dataset with real data. This is particularly useful for:</p> <ul> <li>Dataset Planning: Define your dataset structure before generating data</li> <li>Template Generation: Create reusable dataset templates</li> <li>CF Compliance: Ensure metadata follows CF conventions with automatic validation</li> <li>Zarr Workflow: Define chunking and compression strategies upfront</li> <li>Metadata Validation: Catch dimension mismatches early</li> <li>Reproducible Workflows: Track and replay all operations</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#core-functionality","title":"Core Functionality","text":"<p>\u2705 Metadata-first design - Define structure before data \u2705 xarray compatibility - Convert to/from xarray.Dataset \u2705 Automatic dimension inference - Infer from data shape \u2705 xarray-style attribute access - <code>ds.time</code>, <code>ds.temperature</code> \u2705 Rich repr - Interactive exploration in notebooks</p>"},{"location":"#cf-compliance-phase-1","title":"CF Compliance (Phase 1)","text":"<p>\u2705 Axis detection - Automatic X/Y/Z/T axis inference \u2705 CF validation - Check for CF convention compliance \u2705 Standard names - Support for CF standard_name vocabulary \u2705 Dimension ordering - Validate T, Z, Y, X ordering</p>"},{"location":"#history-provenance","title":"History &amp; Provenance","text":"<p>\u2705 Operation tracking - Record all dataset modifications \u2705 History export - Export as Python, JSON, or YAML \u2705 History visualization - Text, DOT, or Mermaid diagrams \u2705 Provenance tracking - Track what changed (added/removed/modified) \u2705 History replay - Recreate datasets from operation history</p>"},{"location":"#data-generation-io","title":"Data Generation &amp; I/O","text":"<p>\u2705 Smart data generation - Populate with realistic random data \u2705 Multiple formats - Export to YAML, JSON, Zarr, NetCDF \u2705 Template support - Save/load dataset specifications \u2705 Encoding support - dtype, chunks, compression settings \u2705 Intake catalogs - Export and import Intake catalog YAML files</p>"},{"location":"#multi-file-dataset-support-phase-2","title":"Multi-File Dataset Support (Phase 2)","text":"<p>\u2705 Multi-file datasets - Open multiple NetCDF files as one dataset \u2705 Automatic frequency inference - Detect time frequency from coordinates \u2705 Time-based grouping - Group datasets by decades, years, months \u2705 File tracking - Track which files contain which data ranges \u2705 Metadata-only - No data loading, only metadata operations</p>"},{"location":"#architecture","title":"Architecture","text":"<p>\u2705 Modular design - Mixin-based architecture for maintainability \u2705 Well-tested - 188 tests with comprehensive coverage \u2705 Type-safe - Clear API with validation</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from dummyxarray import DummyDataset\n\n# Create a CF-compliant dataset\nds = DummyDataset()\nds.assign_attrs(Conventions=\"CF-1.8\", title=\"Climate Model Output\")\n\n# Add dimensions and coordinates\nds.add_dim(\"time\", 12)\nds.add_dim(\"lat\", 180)\nds.add_dim(\"lon\", 360)\n\nds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\nds.add_coord(\"lat\", dims=[\"lat\"], attrs={\"units\": \"degrees_north\"})\nds.add_coord(\"lon\", dims=[\"lon\"], attrs={\"units\": \"degrees_east\"})\n\n# Add variable with encoding\nds.add_variable(\n    \"temperature\",\n    dims=[\"time\", \"lat\", \"lon\"],\n    attrs={\"standard_name\": \"air_temperature\", \"units\": \"K\"},\n    encoding={\"dtype\": \"float32\", \"chunks\": (6, 32, 64)}\n)\n\n# Infer CF axis attributes (X, Y, Z, T)\nds.infer_axis()\nds.set_axis_attributes()\n\n# Validate CF compliance\nresult = ds.validate_cf()\nprint(f\"Warnings: {len(result['warnings'])}\")\n\n# Populate with realistic data\nds.populate_with_random_data(seed=42)\n\n# Export or convert\nds.save_yaml(\"template.yaml\")\nxr_dataset = ds.to_xarray()\nds.to_zarr(\"output.zarr\")\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation Guide - Set up dummyxarray</li> <li>Quick Start - Hands-on introduction</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Basic Usage - Core concepts and workflows</li> <li>CF Compliance - Working with CF conventions</li> <li>CF Standards - CF standard names and vocabulary</li> <li>Multi-File Datasets - Work with multiple NetCDF files</li> <li>History Tracking - Track and replay operations</li> <li>Validation - Validate dataset structure</li> <li>Encoding - Configure chunking and compression</li> <li>YAML Export - Save and load specifications</li> <li>Intake Catalogs - Export and import Intake catalogs</li> <li>ncdump Import - Import from ncdump output</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<ul> <li>DummyDataset - Main dataset class</li> <li>DummyArray - Array class for variables and coordinates</li> </ul>"},{"location":"#project-architecture","title":"Project Architecture","text":"<ul> <li>Design Overview - Mixin-based architecture</li> <li>Testing - Test structure and fixtures</li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>Phase 1 Complete: CF compliance, history tracking, and modular architecture Phase 2 Complete: Multi-file datasets, time-based grouping, CF standards Future: CMIP table integration and spatial grouping</p> <p>Contributions and feedback are welcome!</p>"},{"location":"CF_STANDARDS_APPROACH/","title":"CF Standards Approach","text":""},{"location":"CF_STANDARDS_APPROACH/#design-decision-cf_xarray-as-required-dependency","title":"Design Decision: cf_xarray as Required Dependency","text":"<p>This document explains why we use <code>cf_xarray</code> as a required dependency for CF standards compliance instead of creating our own implementation or using IOOS compliance-checker.</p>"},{"location":"CF_STANDARDS_APPROACH/#the-problem","title":"The Problem","text":"<p>We need to ensure datasets follow CF conventions, particularly for: - Coordinate axis detection (X, Y, Z, T) - Appropriate metadata attributes - Standard names - Community-agreed conventions</p>"},{"location":"CF_STANDARDS_APPROACH/#options-considered","title":"Options Considered","text":""},{"location":"CF_STANDARDS_APPROACH/#1-build-our-own-not-recommended","title":"1. Build Our Own (\u274c Not Recommended)","text":"<p>Pros: - No external dependencies - Full control</p> <p>Cons: - Reinventing the wheel - May diverge from community standards - Maintenance burden - Potential incompatibilities with ecosystem</p>"},{"location":"CF_STANDARDS_APPROACH/#2-ioos-compliance-checker-limited-use","title":"2. IOOS Compliance Checker (\u26a0\ufe0f Limited Use)","text":"<p>Pros: - Comprehensive validation - Industry standard - Well-tested</p> <p>Cons: - Requires actual NetCDF files - Can't validate metadata-only - Requires data - Must populate arrays first - Heavy dependency - Slow for quick checks - Overkill for metadata validation</p>"},{"location":"CF_STANDARDS_APPROACH/#3-cf_xarray-chosen-required-dependency","title":"3. cf_xarray (\u2705 CHOSEN - Required Dependency)","text":"<p>Pros: - Metadata-focused - Works without data (we create temporary arrays) - Community standards - Based on CF conventions + MetPy + Iris - Lightweight - Minimal dependencies - Ecosystem integration - Works with xarray - Actively maintained - Regular updates - Flexible - Can work with in-memory objects</p> <p>Decision: - Required dependency - Ensures all users get same CF validation - No fallback needed - Simpler codebase, consistent behavior</p>"},{"location":"CF_STANDARDS_APPROACH/#our-approach-hybrid-strategy","title":"Our Approach: Hybrid Strategy","text":"<p>We implement a three-tier approach:</p>"},{"location":"CF_STANDARDS_APPROACH/#tier-1-built-in-basic-validation-always-available","title":"Tier 1: Built-in Basic Validation (Always Available)","text":"<pre><code># No dependencies, fast, essential checks\nds.infer_axis()\nds.validate_cf()\n</code></pre> <p>Use for: - Quick prototyping - Development iteration - When no dependencies allowed - Basic compliance checks</p>"},{"location":"CF_STANDARDS_APPROACH/#tier-2-cf_xarray-standards-required-recommended","title":"Tier 2: cf_xarray Standards (Required, Recommended)","text":"<pre><code># Required dependency, community standards, works without data!\nds.apply_cf_standards()\nds.validate_cf_metadata()\n</code></pre> <p>Use for: - Production datasets - Publishing data - Ecosystem compatibility - Following community standards - Metadata-only workflows (no data needed!)</p>"},{"location":"CF_STANDARDS_APPROACH/#tier-3-ioos-compliance-checker-optional-comprehensive","title":"Tier 3: IOOS Compliance Checker (Optional, Comprehensive)","text":"<pre><code># For final validation of complete datasets\nds.populate_with_random_data()\nresult = validate_with_compliance_checker(ds)\n</code></pre> <p>Use for: - Final validation before publication - Comprehensive compliance reports - When you have complete datasets with data</p>"},{"location":"CF_STANDARDS_APPROACH/#why-cf_xarray-for-metadata","title":"Why cf_xarray for Metadata?","text":""},{"location":"CF_STANDARDS_APPROACH/#1-community-agreement","title":"1. Community Agreement","text":"<p>cf_xarray's criteria are based on: - Official CF Conventions - MetPy's coordinate detection (widely used in meteorology) - Iris coordinate system (used by UK Met Office) - Community feedback and contributions</p> <p>This means we're not making up our own rules - we're using what the community has already agreed upon.</p>"},{"location":"CF_STANDARDS_APPROACH/#2-metadata-first-design","title":"2. Metadata-First Design \u2728","text":"<p>NEW: cf_xarray now works with metadata alone - no data required!</p> <pre><code># Works without data! (we create temporary arrays internally)\nds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\nds.add_variable(\"temperature\", dims=[\"time\"], attrs={\"units\": \"K\"})\nds.apply_cf_standards()  # Detects T axis, adds attributes\n# Data is still None!\n</code></pre> <p>IOOS compliance-checker still requires: <pre><code># Must have data\nds.populate_with_random_data()  # Required!\nvalidate_with_compliance_checker(ds)  # Only then can validate\n</code></pre></p>"},{"location":"CF_STANDARDS_APPROACH/#3-lightweight-and-fast","title":"3. Lightweight and Fast","text":"<pre><code># cf_xarray: ~0.01s\nds.apply_cf_standards()\n\n# compliance-checker: ~1-5s (must write NetCDF file)\nvalidate_with_compliance_checker(ds)\n</code></pre>"},{"location":"CF_STANDARDS_APPROACH/#4-ecosystem-integration","title":"4. Ecosystem Integration","text":"<p>Using cf_xarray ensures compatibility with: - xarray (native integration) - MetPy (meteorological calculations) - Iris (climate data analysis) - Cartopy (map projections) - Other CF-aware tools</p>"},{"location":"CF_STANDARDS_APPROACH/#implementation-details","title":"Implementation Details","text":""},{"location":"CF_STANDARDS_APPROACH/#how-cf_xarray-detects-axes","title":"How cf_xarray Detects Axes","text":"<p>cf_xarray uses multiple criteria (in order of precedence):</p> <ol> <li>Explicit axis attribute: <code>axis=\"T\"</code></li> <li>Standard name: <code>standard_name=\"time\"</code></li> <li>Units: <code>units=\"days since 2000-01-01\"</code></li> <li>Name patterns: Variable named <code>time</code>, <code>lat</code>, <code>lon</code>, etc.</li> <li>Coordinate type: 1D coordinate with matching dimension</li> </ol>"},{"location":"CF_STANDARDS_APPROACH/#what-we-add","title":"What We Add","text":"<p>We provide convenience methods:</p> <pre><code>class CFStandardsMixin:\n    def apply_cf_standards(self, verbose=False):\n        \"\"\"Apply community-agreed CF standards.\"\"\"\n        # Uses cf_xarray.guess_coord_axis()\n\n    def validate_cf_metadata(self, strict=False):\n        \"\"\"Validate against CF standards.\"\"\"\n        # Uses cf_xarray criteria\n\n    def check_cf_standards_available(self):\n        \"\"\"Check if cf_xarray is installed.\"\"\"\n</code></pre>"},{"location":"CF_STANDARDS_APPROACH/#usage-recommendations","title":"Usage Recommendations","text":""},{"location":"CF_STANDARDS_APPROACH/#for-users","title":"For Users","text":"<p>Development (Metadata-Only): <pre><code># Fast iteration, no data needed!\nds.apply_cf_standards()\nds.validate_cf_metadata()\n</code></pre></p> <p>Production: <pre><code># Apply community standards (always available)\nds.apply_cf_standards()\nresult = ds.validate_cf_metadata()\n</code></pre></p> <p>Publication: <pre><code># Final comprehensive check (requires data)\nds.populate_with_random_data()\nresult = validate_with_compliance_checker(ds, cf_version=\"1.8\")\n</code></pre></p>"},{"location":"CF_STANDARDS_APPROACH/#for-contributors","title":"For Contributors","text":"<p>When adding CF-related features:</p> <ol> <li>Don't reinvent - Check if cf_xarray already does it</li> <li>Stay compatible - Ensure our built-in methods align with cf_xarray</li> <li>Document differences - Explain when to use built-in vs cf_xarray</li> <li>Test both - Ensure features work with and without cf_xarray</li> </ol>"},{"location":"CF_STANDARDS_APPROACH/#benefits-of-this-approach","title":"Benefits of This Approach","text":"<ol> <li>Community alignment - Uses agreed standards (cf_xarray required)</li> <li>Metadata-only workflows - Works without data (temporary arrays)</li> <li>Flexibility - Users choose their level of validation</li> <li>Performance - Fast for metadata-only, thorough for production</li> <li>Ecosystem compatibility - Datasets work with other tools</li> <li>Simpler codebase - No optional dependency handling</li> </ol>"},{"location":"CF_STANDARDS_APPROACH/#future-considerations","title":"Future Considerations","text":""},{"location":"CF_STANDARDS_APPROACH/#potential-enhancements","title":"Potential Enhancements","text":"<ol> <li>Cache cf_xarray results - Avoid repeated detection</li> <li>Expose cf_xarray criteria - Let users see detection rules</li> <li>Custom criteria - Allow users to add domain-specific rules</li> <li>Validation profiles - Pre-defined validation levels</li> </ol>"},{"location":"CF_STANDARDS_APPROACH/#monitoring","title":"Monitoring","text":"<p>We should track: - cf_xarray API changes - New CF convention versions - Community feedback on our approach</p>"},{"location":"CF_STANDARDS_APPROACH/#conclusion","title":"Conclusion","text":"<p>By using cf_xarray as a required dependency for CF standards, we: - \u2705 Follow community-agreed conventions - \u2705 Avoid reinventing the wheel - \u2705 Ensure ecosystem compatibility - \u2705 Enable metadata-only workflows (no data required!) - \u2705 Simplify codebase (no optional dependency handling) - \u2705 Guarantee consistent behavior for all users</p> <p>This is the right approach for a metadata-focused library that wants to integrate with the broader scientific Python ecosystem while maintaining true metadata-only capabilities.</p>"},{"location":"CF_STANDARDS_APPROACH/#references","title":"References","text":"<ul> <li>cf_xarray Documentation</li> <li>CF Conventions</li> <li>IOOS Compliance Checker</li> <li>MetPy</li> <li>Iris</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>dummyxarray uses a modular, mixin-based architecture for maintainability and extensibility.</p>"},{"location":"architecture/#design-philosophy","title":"Design Philosophy","text":"<p>The codebase follows these principles:</p> <ol> <li>Separation of Concerns - Each module has a single, clear responsibility</li> <li>Composition over Inheritance - Mixins provide functionality without deep hierarchies</li> <li>Maintainability - Small, focused modules are easier to understand and modify</li> <li>Extensibility - New features can be added as new mixins</li> <li>Testability - Each mixin can be tested independently</li> </ol>"},{"location":"architecture/#module-structure","title":"Module Structure","text":"<pre><code>src/dummyxarray/\n\u251c\u2500\u2500 __init__.py (9 lines)           # Public API exports\n\u251c\u2500\u2500 core.py (896 lines)             # Core classes (DummyArray, DummyDataset)\n\u251c\u2500\u2500 history.py (331 lines)          # HistoryMixin\n\u251c\u2500\u2500 provenance.py (157 lines)       # ProvenanceMixin\n\u251c\u2500\u2500 cf_compliance.py (318 lines)    # CFComplianceMixin\n\u251c\u2500\u2500 cf_standards.py (388 lines)     # CFStandardsMixin\n\u251c\u2500\u2500 io.py (246 lines)               # IOMixin\n\u251c\u2500\u2500 validation.py (82 lines)        # ValidationMixin\n\u251c\u2500\u2500 data_generation.py (169 lines)  # DataGenerationMixin\n\u251c\u2500\u2500 mfdataset.py (454 lines)        # Multi-file dataset support\n\u251c\u2500\u2500 time_utils.py (346 lines)       # Time calculation utilities\n\u2514\u2500\u2500 ncdump_parser.py (280 lines)    # NetCDF metadata parser\n</code></pre>"},{"location":"architecture/#architecture-evolution","title":"Architecture Evolution","text":""},{"location":"architecture/#phase-1-initial-refactoring","title":"Phase 1 (Initial Refactoring)","text":"<ul> <li>Before: Single file with 2041 lines</li> <li>After: 7 focused modules, average ~230 lines each</li> </ul>"},{"location":"architecture/#current-state-phase-2","title":"Current State (Phase 2)","text":"<ul> <li>12 modules: Total 3,676 lines</li> <li>Average: ~306 lines per module</li> <li>New capabilities: Multi-file datasets, time-based grouping, CF standards</li> <li>Maintainability: Each module remains focused and testable</li> <li>Scalability: New features added as new modules (mfdataset, time_utils)</li> </ul>"},{"location":"architecture/#core-classes","title":"Core Classes","text":""},{"location":"architecture/#dummyarray","title":"DummyArray","text":"<p>Represents a single array (variable or coordinate) with metadata.</p> <p>Location: <code>core.py</code></p> <p>Attributes: - <code>dims</code> - List of dimension names - <code>attrs</code> - Metadata dictionary - <code>data</code> - Optional numpy array - <code>encoding</code> - Encoding parameters</p> <p>Methods: - <code>infer_dims_from_data()</code> - Infer dimension names from shape - <code>assign_attrs()</code> - Set attributes (xarray-compatible) - <code>get_history()</code> - Get operation history - <code>replay_history()</code> - Recreate from history</p>"},{"location":"architecture/#dummydataset","title":"DummyDataset","text":"<p>Main dataset class composed of multiple mixins.</p> <p>Location: <code>core.py</code></p> <p>Inheritance: <pre><code>class DummyDataset(\n    HistoryMixin,\n    ProvenanceMixin,\n    CFComplianceMixin,\n    CFStandardsMixin,\n    IOMixin,\n    ValidationMixin,\n    DataGenerationMixin,\n    FileTrackerMixin,\n):\n    ...\n</code></pre></p> <p>Core Attributes: - <code>dims</code> - Dictionary of dimension names to sizes - <code>coords</code> - Dictionary of coordinate names to DummyArray - <code>variables</code> - Dictionary of variable names to DummyArray - <code>attrs</code> - Global attributes dictionary - <code>_history</code> - Operation history (if tracking enabled)</p> <p>Core Methods (in <code>core.py</code>): - <code>add_dim()</code> - Add a dimension - <code>add_coord()</code> - Add a coordinate - <code>add_variable()</code> - Add a variable - <code>assign_attrs()</code> - Set global attributes - <code>rename_dims()</code>, <code>rename_vars()</code>, <code>rename()</code> - Renaming operations</p>"},{"location":"architecture/#mixins","title":"Mixins","text":""},{"location":"architecture/#historymixin","title":"HistoryMixin","text":"<p>Purpose: Track and replay all dataset operations</p> <p>Location: <code>history.py</code> (331 lines)</p> <p>Methods: - <code>_record_operation()</code> - Record an operation - <code>get_history()</code> - Get operation list - <code>export_history()</code> - Export as Python/JSON/YAML - <code>replay_history()</code> - Recreate dataset from history - <code>reset_history()</code> - Clear history - <code>visualize_history()</code> - Visualize as text/DOT/Mermaid</p> <p>Dependencies: - Requires <code>self._history</code> attribute - Used by all operations that modify the dataset</p>"},{"location":"architecture/#provenancemixin","title":"ProvenanceMixin","text":"<p>Purpose: Track what changed in each operation</p> <p>Location: <code>provenance.py</code> (157 lines)</p> <p>Methods: - <code>get_provenance()</code> - Get provenance information - <code>visualize_provenance()</code> - Visualize changes</p> <p>Provenance Information: - <code>added</code> - Items added - <code>removed</code> - Items removed - <code>modified</code> - Items modified (before/after) - <code>renamed</code> - Items renamed (old -&gt; new)</p>"},{"location":"architecture/#cfcompliancemixin","title":"CFComplianceMixin","text":"<p>Purpose: CF convention support and validation</p> <p>Location: <code>cf_compliance.py</code> (318 lines)</p> <p>Methods: - <code>infer_axis()</code> - Detect X/Y/Z/T axes - <code>_detect_axis_type()</code> - Axis detection logic - <code>set_axis_attributes()</code> - Set axis attributes - <code>get_axis_coordinates()</code> - Query by axis - <code>validate_cf()</code> - CF compliance validation</p> <p>Detection Rules: - Coordinate names (time, lat, lon, lev) - Units (degrees_north, days since, etc.) - Standard names (latitude, longitude, time)</p>"},{"location":"architecture/#cfstandardsmixin","title":"CFStandardsMixin","text":"<p>Purpose: CF standard names and vocabulary support</p> <p>Location: <code>cf_standards.py</code> (388 lines)</p> <p>Methods: - <code>validate_standard_names()</code> - Validate CF standard names - <code>get_standard_name_info()</code> - Get standard name metadata - <code>suggest_standard_names()</code> - Suggest appropriate standard names</p> <p>Features: - Access to CF standard name table - Validation against official CF vocabulary - Metadata lookup for standard names</p>"},{"location":"architecture/#iomixin","title":"IOMixin","text":"<p>Purpose: Serialization and format conversion</p> <p>Location: <code>io.py</code> (243 lines)</p> <p>Methods: - <code>to_dict()</code>, <code>to_json()</code>, <code>to_yaml()</code> - Export formats - <code>save_yaml()</code>, <code>load_yaml()</code> - File I/O - <code>from_xarray()</code> - Import from xarray - <code>to_xarray()</code> - Convert to xarray - <code>to_zarr()</code> - Write to Zarr</p> <p>Supported Formats: - Dictionary (Python native) - JSON (human-readable, version control) - YAML (human-readable, configuration) - xarray.Dataset (interoperability) - Zarr (cloud-optimized storage)</p>"},{"location":"architecture/#validationmixin","title":"ValidationMixin","text":"<p>Purpose: Dataset structure validation</p> <p>Location: <code>validation.py</code> (82 lines)</p> <p>Methods: - <code>validate()</code> - Validate structure - <code>_infer_and_register_dims()</code> - Auto-register dimensions</p> <p>Validation Checks: - Unknown dimensions - Shape mismatches - Missing coordinates (strict mode)</p>"},{"location":"architecture/#datagenerationmixin","title":"DataGenerationMixin","text":"<p>Purpose: Generate realistic random data</p> <p>Location: <code>data_generation.py</code> (169 lines)</p> <p>Methods: - <code>populate_with_random_data()</code> - Fill with data - <code>_generate_coordinate_data()</code> - Coordinate data - <code>_generate_variable_data()</code> - Variable data</p> <p>Smart Generation: - Time: Sequential integers - Latitude: -90 to 90 - Longitude: -180 to 180 - Temperature: Realistic ranges based on units - Precipitation: Non-negative, skewed distribution - Wind: Appropriate ranges for components</p>"},{"location":"architecture/#filetrackermixin","title":"FileTrackerMixin","text":"<p>Purpose: Track source files in multi-file datasets</p> <p>Location: <code>core.py</code> (part of core module)</p> <p>Methods: - <code>enable_file_tracking()</code> - Enable file tracking - <code>add_file_source()</code> - Register a file source - <code>get_source_files()</code> - Query files by coordinate range - <code>get_file_info()</code> - Get metadata for a specific file - <code>get_all_file_info()</code> - Get all tracked files</p> <p>Features: - Track which files contain which coordinate ranges - Query files for specific time/coordinate slices - Preserve file provenance in grouped datasets</p>"},{"location":"architecture/#method-resolution-order-mro","title":"Method Resolution Order (MRO)","text":"<p>Python resolves methods left-to-right through the inheritance chain:</p> <pre><code>DummyDataset.__mro__\n# (DummyDataset, HistoryMixin, ProvenanceMixin, CFComplianceMixin,\n#  CFStandardsMixin, IOMixin, ValidationMixin, DataGenerationMixin,\n#  FileTrackerMixin, object)\n</code></pre> <p>Important: No method name conflicts exist between mixins (verified during development).</p>"},{"location":"architecture/#adding-new-mixins","title":"Adding New Mixins","text":"<p>To add a new mixin (e.g., for Phase 2 CMIP integration):</p> <ol> <li> <p>Create the module: <pre><code># src/dummyxarray/cmip.py\nclass CMIPMixin:\n    \"\"\"CMIP table integration.\"\"\"\n\n    def validate_cmip_table(self, table_name):\n        \"\"\"Validate against CMIP table.\"\"\"\n        ...\n\n    def map_to_cmip_vocabulary(self):\n        \"\"\"Map to CMIP vocabulary.\"\"\"\n        ...\n</code></pre></p> </li> <li> <p>Add to DummyDataset: <pre><code># src/dummyxarray/core.py\nfrom .cmip import CMIPMixin\n\nclass DummyDataset(\n    HistoryMixin,\n    ProvenanceMixin,\n    CFComplianceMixin,\n    CMIPMixin,  # \u2190 Add here\n    IOMixin,\n    ValidationMixin,\n    DataGenerationMixin,\n):\n    ...\n</code></pre></p> </li> <li> <p>Create tests: <pre><code># tests/unit/test_cmip.py\nclass TestCMIPValidation:\n    def test_validate_cmip_table(self):\n        ...\n</code></pre></p> </li> </ol>"},{"location":"architecture/#testing-architecture","title":"Testing Architecture","text":"<p>Tests mirror the source structure:</p> <pre><code>tests/\n\u251c\u2500\u2500 conftest.py                    # Shared fixtures\n\u251c\u2500\u2500 unit/                          # Unit tests per module\n\u2502   \u251c\u2500\u2500 test_core.py              # Core functionality\n\u2502   \u251c\u2500\u2500 test_history.py           # HistoryMixin\n\u2502   \u251c\u2500\u2500 test_provenance.py        # ProvenanceMixin\n\u2502   \u251c\u2500\u2500 test_cf_compliance.py     # CFComplianceMixin\n\u2502   \u251c\u2500\u2500 test_io.py                # IOMixin\n\u2502   \u251c\u2500\u2500 test_validation.py        # ValidationMixin\n\u2502   \u251c\u2500\u2500 test_data_generation.py   # DataGenerationMixin\n\u2502   \u251c\u2500\u2500 test_mfdataset.py         # Multi-file dataset support\n\u2502   \u2514\u2500\u2500 test_ncdump_parser.py     # NetCDF metadata parser\n\u2514\u2500\u2500 integration/                   # Integration tests\n    \u2514\u2500\u2500 test_workflows.py         # End-to-end workflows\n</code></pre> <p>Total: 188 tests with comprehensive coverage</p> <p>See Testing Documentation for details.</p>"},{"location":"architecture/#design-patterns","title":"Design Patterns","text":""},{"location":"architecture/#mixin-pattern","title":"Mixin Pattern","text":"<p>Advantages: - Composition over inheritance - Clear separation of concerns - Easy to add/remove features - Independent testing</p> <p>Considerations: - Method name conflicts (avoided through naming conventions) - Shared state through <code>self</code> attributes - Order matters in MRO</p>"},{"location":"architecture/#dependency-injection","title":"Dependency Injection","text":"<p>Mixins depend on attributes from <code>DummyDataset</code>: - <code>self.dims</code> - <code>self.coords</code> - <code>self.variables</code> - <code>self.attrs</code> - <code>self._history</code></p>"},{"location":"architecture/#factory-pattern","title":"Factory Pattern","text":"<p>Class methods for alternative construction: - <code>DummyDataset.from_xarray()</code> - <code>DummyDataset.load_yaml()</code> - <code>DummyDataset.replay_history()</code></p>"},{"location":"architecture/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>History tracking: Minimal overhead (~1% for typical operations)</li> <li>Validation: Only runs when explicitly called</li> <li>Data generation: Uses numpy for efficiency</li> <li>Serialization: JSON/YAML are human-readable but slower than pickle</li> </ul>"},{"location":"architecture/#utility-modules","title":"Utility Modules","text":""},{"location":"architecture/#time_utilspy","title":"time_utils.py","text":"<p>Purpose: Time calculation utilities for multi-file datasets</p> <p>Location: <code>time_utils.py</code> (346 lines)</p> <p>Functions: - <code>infer_time_frequency()</code> - Detect time frequency from coordinate values - <code>count_timesteps()</code> - Calculate timesteps between dates - <code>add_frequency()</code> - Add time periods to dates - <code>create_time_periods()</code> - Generate time period ranges - <code>check_time_range_overlap()</code> - Check if time ranges overlap</p> <p>Features: - Full cftime calendar support (standard, noleap, 360_day, etc.) - Handles extended time ranges beyond pandas limits - CF-compliant time unit parsing</p>"},{"location":"architecture/#mfdatasetpy","title":"mfdataset.py","text":"<p>Purpose: Multi-file dataset support</p> <p>Location: <code>mfdataset.py</code> (454 lines)</p> <p>Functions: - <code>open_mfdataset()</code> - Open multiple NetCDF files as one dataset - <code>groupby_time_impl()</code> - Group dataset by time periods - <code>_read_file_metadata()</code> - Read metadata from NetCDF files - <code>_combine_file_metadata()</code> - Combine metadata from multiple files - <code>_create_time_subset_metadata()</code> - Create time-based subsets</p> <p>Features: - Metadata-only approach (no data loading) - Automatic frequency inference - Time-based grouping (decades, years, months, etc.) - File tracking and provenance</p>"},{"location":"architecture/#ncdump_parserpy","title":"ncdump_parser.py","text":"<p>Purpose: Parse ncdump output for metadata extraction</p> <p>Location: <code>ncdump_parser.py</code> (280 lines)</p> <p>Functions: - <code>parse_ncdump()</code> - Parse ncdump -h output - <code>_parse_dimensions()</code> - Extract dimensions - <code>_parse_variables()</code> - Extract variables - <code>_parse_attributes()</code> - Extract attributes</p> <p>Features: - Alternative to opening NetCDF files directly - Useful for remote or restricted file access - Handles complex ncdump output formats</p>"},{"location":"architecture/#future-extensions","title":"Future Extensions","text":"<p>Potential additions:</p> <ul> <li>CMIPMixin: CMIP table integration and validation</li> <li>BoundsMixin: Automatic bounds generation for coordinates</li> <li>PluginMixin: Custom validator plugins</li> <li>SpatialGroupingMixin: Group by spatial regions</li> </ul>"},{"location":"architecture/#best-practices","title":"Best Practices","text":"<ol> <li>Keep mixins focused: One responsibility per mixin</li> <li>Avoid method conflicts: Use descriptive, specific names</li> <li>Document dependencies: What attributes does the mixin need?</li> <li>Test independently: Unit test each mixin</li> <li>Use private methods: Prefix with <code>_</code> for internal helpers</li> </ol>"},{"location":"architecture/#references","title":"References","text":"<ul> <li>Django Class-Based Views - Mixin inspiration</li> <li>Python MRO - Method resolution order</li> <li>Composition over Inheritance - Design principle</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>This page contains comprehensive examples demonstrating the features of Dummy Xarray.</p>"},{"location":"examples/#basic-metadata-only-dataset","title":"Basic Metadata-Only Dataset","text":"<p>Create a dataset specification without any data:</p> <pre><code>from dummyxarray import DummyDataset\n\nds = DummyDataset()\n\n# Set global attributes\nds.set_global_attrs(\n    title=\"Test Climate Dataset\",\n    institution=\"DKRZ\",\n    experiment=\"historical\",\n    source=\"Example Model v1.0\"\n)\n\n# Add dimensions\nds.add_dim(\"time\", 12)\nds.add_dim(\"lat\", 180)\nds.add_dim(\"lon\", 360)\n\n# Add coordinates (without data for now)\nds.add_coord(\"time\", [\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\nds.add_coord(\"lat\", [\"lat\"], attrs={\"units\": \"degrees_north\"})\nds.add_coord(\"lon\", [\"lon\"], attrs={\"units\": \"degrees_east\"})\n\n# Add variables (without data)\nds.add_variable(\n    \"tas\",\n    [\"time\", \"lat\", \"lon\"],\n    attrs={\"long_name\": \"Near-Surface Air Temperature\", \"units\": \"K\"}\n)\n\n# Export to YAML for documentation\nprint(ds.to_yaml())\nds.save_yaml(\"dataset_spec.yaml\")\n</code></pre>"},{"location":"examples/#automatic-dimension-inference","title":"Automatic Dimension Inference","text":"<p>Let dimensions be inferred from your data:</p> <pre><code>import numpy as np\nfrom dummyxarray import DummyDataset\n\nds = DummyDataset()\nds.set_global_attrs(title=\"Auto-inferred Dataset\")\n\n# Create data with specific shape\ntemp_data = np.random.rand(12, 64, 128)\n\n# Add variable with data - dimensions will be auto-inferred\nds.add_variable(\n    \"tas\",\n    data=temp_data,\n    attrs={\"units\": \"K\", \"long_name\": \"air_temperature\"}\n)\n\nprint(\"Dimensions were automatically inferred:\")\nprint(ds.dims)  # {'dim_0': 12, 'dim_1': 64, 'dim_2': 128}\n</code></pre>"},{"location":"examples/#with-encoding-for-zarrnetcdf","title":"With Encoding for Zarr/NetCDF","text":"<p>Specify encoding parameters for optimal storage:</p> <pre><code>import numpy as np\nfrom dummyxarray import DummyDataset\n\nds = DummyDataset()\nds.set_global_attrs(title=\"Dataset with Encoding\")\n\n# Add dimensions\nds.add_dim(\"time\", 12)\nds.add_dim(\"lat\", 64)\nds.add_dim(\"lon\", 128)\n\n# Add coordinate with encoding\ntime_data = np.arange(12)\nds.add_coord(\n    \"time\",\n    [\"time\"],\n    data=time_data,\n    attrs={\"units\": \"days since 2000-01-01\"},\n    encoding={\"dtype\": \"int32\"}\n)\n\n# Add variable with chunking and compression\ntemp_data = np.random.rand(12, 64, 128) * 20 + 273.15\nds.add_variable(\n    \"tas\",\n    [\"time\", \"lat\", \"lon\"],\n    data=temp_data,\n    attrs={\"long_name\": \"Near-Surface Air Temperature\", \"units\": \"K\"},\n    encoding={\n        \"dtype\": \"float32\",\n        \"chunks\": (6, 32, 64),\n        \"compressor\": None,  # Can use zarr.Blosc() or similar\n    }\n)\n\n# Validate and write to Zarr\nds.validate()\nds.to_zarr(\"output.zarr\")\n</code></pre>"},{"location":"examples/#loading-and-modifying-specifications","title":"Loading and Modifying Specifications","text":"<p>Save and load dataset specifications:</p> <pre><code>from dummyxarray import DummyDataset\n\n# Create and save a specification\nds = DummyDataset()\nds.set_global_attrs(title=\"Template Dataset\")\nds.add_dim(\"time\", 12)\nds.add_variable(\"temperature\", [\"time\"], attrs={\"units\": \"K\"})\nds.save_yaml(\"template.yaml\")\n\n# Load it later\nloaded_ds = DummyDataset.load_yaml(\"template.yaml\")\n\n# Modify and use\nloaded_ds.set_global_attrs(experiment=\"run_001\")\nprint(loaded_ds.to_yaml())\n</code></pre>"},{"location":"examples/#validation-example","title":"Validation Example","text":"<p>Catch errors early with validation:</p> <pre><code>from dummyxarray import DummyDataset\nimport numpy as np\n\nds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.add_dim(\"lat\", 5)\n\n# Add variable with correct shape\ndata = np.random.rand(10, 5)\nds.add_variable(\"test\", dims=[\"time\", \"lat\"], data=data)\n\n# Validate - should pass\ntry:\n    ds.validate()\n    print(\"\u2713 Validation passed!\")\nexcept ValueError as e:\n    print(f\"\u2717 Validation failed: {e}\")\n\n# Try adding a variable with wrong dimension\nds2 = DummyDataset()\nds2.add_variable(\"bad_var\", dims=[\"unknown_dim\"])\n\ntry:\n    ds2.validate()\nexcept ValueError as e:\n    print(f\"\u2717 Caught error: {e}\")\n</code></pre>"},{"location":"examples/#complete-workflow","title":"Complete Workflow","text":"<p>A complete example from specification to xarray:</p> <pre><code>import numpy as np\nfrom dummyxarray import DummyDataset\n\n# 1. Create specification\nds = DummyDataset()\nds.set_global_attrs(\n    title=\"Complete Example\",\n    institution=\"Research Center\",\n    Conventions=\"CF-1.8\"\n)\n\n# 2. Define structure\nds.add_dim(\"time\", 365)\nds.add_dim(\"lat\", 90)\nds.add_dim(\"lon\", 180)\n\n# 3. Add coordinates with data\ntime_data = np.arange(365)\nlat_data = np.linspace(-90, 90, 90)\nlon_data = np.linspace(-180, 180, 180)\n\nds.add_coord(\"time\", [\"time\"], data=time_data, \n             attrs={\"units\": \"days since 2020-01-01\"})\nds.add_coord(\"lat\", [\"lat\"], data=lat_data,\n             attrs={\"units\": \"degrees_north\", \"standard_name\": \"latitude\"})\nds.add_coord(\"lon\", [\"lon\"], data=lon_data,\n             attrs={\"units\": \"degrees_east\", \"standard_name\": \"longitude\"})\n\n# 4. Add variables with encoding\ntemp_data = np.random.rand(365, 90, 180) * 20 + 273.15\nds.add_variable(\n    \"temperature\",\n    [\"time\", \"lat\", \"lon\"],\n    data=temp_data,\n    attrs={\n        \"long_name\": \"Near-Surface Air Temperature\",\n        \"units\": \"K\",\n        \"standard_name\": \"air_temperature\"\n    },\n    encoding={\n        \"dtype\": \"float32\",\n        \"chunks\": (30, 45, 90)\n    }\n)\n\n# 5. Validate\nds.validate()\n\n# 6. Convert to xarray\nxr_ds = ds.to_xarray()\nprint(xr_ds)\n\n# 7. Write to Zarr\nds.to_zarr(\"complete_example.zarr\")\n</code></pre>"},{"location":"examples/#extract-metadata-from-existing-xarray-dataset","title":"Extract Metadata from Existing xarray Dataset","text":"<p>Create a template from an existing dataset:</p> <pre><code>import xarray as xr\nfrom dummyxarray import DummyDataset\n\n# Load an existing xarray dataset\nexisting_ds = xr.open_dataset(\"my_data.nc\")\n\n# Extract metadata only (no data)\ndummy_ds = DummyDataset.from_xarray(existing_ds, include_data=False)\n\n# Save as a reusable template\ndummy_ds.save_yaml(\"template.yaml\")\n\n# Or extract with data included\ndummy_with_data = DummyDataset.from_xarray(existing_ds, include_data=True)\n</code></pre>"},{"location":"examples/#populate-with-random-data-for-testing","title":"Populate with Random Data for Testing","text":"<p>Generate meaningful random data based on metadata:</p> <pre><code>from dummyxarray import DummyDataset\n\n# Create structure without data\nds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.add_dim(\"lat\", 5)\nds.add_dim(\"lon\", 8)\n\nds.add_coord(\"time\", [\"time\"], attrs={\"units\": \"days\"})\nds.add_coord(\"lat\", [\"lat\"], attrs={\"units\": \"degrees_north\"})\nds.add_coord(\"lon\", [\"lon\"], attrs={\"units\": \"degrees_east\"})\n\nds.add_variable(\"temperature\", [\"time\", \"lat\", \"lon\"], \n                attrs={\"units\": \"K\", \"standard_name\": \"air_temperature\"})\nds.add_variable(\"precipitation\", [\"time\", \"lat\", \"lon\"],\n                attrs={\"units\": \"kg m-2 s-1\"})\n\n# Populate all coordinates and variables with meaningful random data\nds.populate_with_random_data(seed=42)  # seed for reproducibility\n\n# Check the generated data\nprint(f\"Temperature range: {ds.variables['temperature'].data.min():.1f} - \"\n      f\"{ds.variables['temperature'].data.max():.1f} K\")\nprint(f\"Latitude range: {ds.coords['lat'].data.min():.1f} - \"\n      f\"{ds.coords['lat'].data.max():.1f}\u00b0\")\n\n# Convert to xarray and use for testing\nxr_ds = ds.to_xarray()\n</code></pre>"},{"location":"examples/#xarray-style-attribute-access","title":"xarray-style Attribute Access","text":"<p>Access coordinates and variables using dot notation:</p> <pre><code>from dummyxarray import DummyDataset\nimport numpy as np\n\nds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.add_coord(\"time\", [\"time\"], attrs={\"units\": \"days\"})\nds.add_variable(\"temperature\", [\"time\"], attrs={\"units\": \"K\"})\n\n# Access using attribute notation (like xarray)\nprint(ds.time)                  # Access coordinate\nprint(ds.temperature)           # Access variable\n\n# Modify via attribute access\nds.time.data = np.arange(10)\n\n# Use assign_attrs for xarray-compatible API\nds.time.assign_attrs(standard_name=\"time\", calendar=\"gregorian\")\n\n# Rich repr shows structure clearly\nprint(ds)\n# Output shows:\n# &lt;dummyxarray.DummyDataset&gt;\n# Dimensions:\n#   time: 10\n# Coordinates:\n#   \u2713 time                 (time)               int64\n# Data variables:\n#   \u2717 temperature          (time)               ?\n\n# Inspect individual arrays\nprint(ds.time)\n# Output:\n# &lt;dummyxarray.DummyArray&gt;\n# Dimensions: (time)\n# Shape: (10,)\n# dtype: int64\n# Data: [0 1 2 3 4 5 6 7 8 9]\n# Attributes:\n#     units: days\n#     standard_name: time\n#     calendar: gregorian\n</code></pre>"},{"location":"examples/#multi-file-dataset-support","title":"Multi-File Dataset Support","text":"<p>Work with multiple NetCDF files as a single dataset:</p> <pre><code>from dummyxarray import DummyDataset\n\n# Open multiple files as one dataset\nds = DummyDataset.open_mfdataset(\"data/*.nc\", concat_dim=\"time\")\n\n# Frequency is automatically inferred\nprint(ds.coords['time'].attrs['frequency'])  # e.g., \"1H\" for hourly\n\n# Group by time periods\ndecades = ds.groupby_time('10Y')\n\n# Each decade is a separate DummyDataset\nfor i, decade in enumerate(decades):\n    print(f\"Decade {i}:\")\n    print(f\"  Time steps: {decade.dims['time']}\")\n    print(f\"  Time units: {decade.coords['time'].attrs['units']}\")\n    print(f\"  Source files: {len(decade.get_source_files())}\")\n\n# Query which files contain specific time ranges\nfiles = ds.get_source_files(time=slice(0, 100))\nprint(f\"Files for first 100 timesteps: {files}\")\n</code></pre>"},{"location":"examples/#intake-catalog-round-trip","title":"Intake Catalog Round-Trip","text":"<p>Complete round-trip workflow with Intake catalogs:</p> <pre><code>from dummyxarray import DummyDataset\nimport tempfile\nimport yaml\n\n# 1. Create original dataset\nds = DummyDataset()\nds.assign_attrs(\n    title=\"Climate Model Output\",\n    institution=\"Example Climate Center\",\n    Conventions=\"CF-1.8\"\n)\nds.add_dim(\"time\", 12)\nds.add_dim(\"lat\", 180)\nds.add_dim(\"lon\", 360)\nds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\nds.add_variable(\n    \"temperature\",\n    dims=[\"time\", \"lat\", \"lon\"],\n    attrs={\"units\": \"K\", \"standard_name\": \"air_temperature\"},\n    encoding={\"dtype\": \"float32\", \"chunks\": [6, 32, 64]}\n)\n\n# 2. Export to Intake catalog\ncatalog_yaml = ds.to_intake_catalog(\n    name=\"climate_data\",\n    description=\"Climate model output with temperature\",\n    driver=\"zarr\",\n    data_path=\"data/climate_model_output.zarr\"\n)\n\nprint(\"Generated Intake catalog:\")\nprint(catalog_yaml)\n\n# 3. Save to file\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:\n    catalog_path = f.name\n    ds.save_intake_catalog(catalog_path, name=\"climate_data\")\n\n# 4. Load from catalog (round-trip)\nrestored_ds = DummyDataset.from_intake_catalog(catalog_path, \"climate_data\")\n\n# 5. Verify integrity\nprint(f\"Original dims: {ds.dims}\")\nprint(f\"Restored dims: {restored_ds.dims}\")\nprint(f\"Dims match: {ds.dims == restored_ds.dims}\")\nprint(f\"Variables match: {set(ds.variables.keys()) == set(restored_ds.variables.keys())}\")\n\n# 6. Load from dictionary\ncatalog_dict = yaml.safe_load(catalog_yaml)\nloaded_from_dict = DummyDataset.from_intake_catalog(catalog_dict, \"climate_data\")\nprint(f\"Dict loading works: {loaded_from_dict.dims == ds.dims}\")\n</code></pre>"},{"location":"examples/#more-examples","title":"More Examples","text":"<p>For more examples, check out the example files in the repository:</p> <ul> <li><code>example.py</code> - Basic usage and core features</li> <li><code>example_from_xarray.py</code> - Extracting metadata from xarray datasets</li> <li><code>example_populate.py</code> - Data population with random data</li> <li><code>example_mfdataset.py</code> - Multi-file dataset support (old version)</li> <li><code>example_groupby_time.py</code> - Time-based grouping with 5 comprehensive examples</li> <li><code>intake_catalog_example.py</code> - Complete Intake catalog round-trip demonstration</li> <li>Basic usage examples</li> <li>Automatic dimension inference</li> <li>Encoding specifications</li> <li>Validation demonstrations</li> <li>xarray conversion</li> <li>Zarr writing</li> <li>YAML save/load</li> <li>Intake catalog export and import</li> </ul>"},{"location":"testing/","title":"Test Structure","text":"<p>This directory contains all tests for dummyxarray, organized by test type and module.</p>"},{"location":"testing/#directory-structure","title":"Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py                    # Shared fixtures and pytest configuration\n\u251c\u2500\u2500 unit/                          # Unit tests (per module)\n\u2502   \u251c\u2500\u2500 test_core.py              # DummyArray, DummyDataset core functionality\n\u2502   \u251c\u2500\u2500 test_history.py           # HistoryMixin tests\n\u2502   \u251c\u2500\u2500 test_provenance.py        # ProvenanceMixin tests\n\u2502   \u251c\u2500\u2500 test_cf_compliance.py     # CFComplianceMixin tests\n\u2502   \u251c\u2500\u2500 test_io.py                # IOMixin tests\n\u2502   \u251c\u2500\u2500 test_validation.py        # ValidationMixin tests\n\u2502   \u251c\u2500\u2500 test_data_generation.py   # DataGenerationMixin tests\n\u2502   \u251c\u2500\u2500 test_mfdataset.py         # Multi-file dataset support tests\n\u2502   \u2514\u2500\u2500 test_ncdump_parser.py     # NetCDF metadata parser tests\n\u251c\u2500\u2500 integration/                   # Integration tests (workflows)\n\u2502   \u2514\u2500\u2500 test_workflows.py         # End-to-end workflow tests\n\u2514\u2500\u2500 fixtures/                      # Test data and fixtures\n    \u2514\u2500\u2500 __init__.py\n</code></pre>"},{"location":"testing/#test-categories","title":"Test Categories","text":""},{"location":"testing/#unit-tests-testsunit","title":"Unit Tests (<code>tests/unit/</code>)","text":"<p>Test individual modules and mixins in isolation:</p> <ul> <li>test_core.py: Core classes (<code>DummyArray</code>, <code>DummyDataset</code>)</li> <li>Initialization, repr, basic operations</li> <li>Dimension management</li> <li>Coordinate and variable management</li> <li> <p>Renaming operations</p> </li> <li> <p>test_history.py: History tracking functionality</p> </li> <li>Operation recording</li> <li>History export/replay</li> <li>History visualization (text, dot, mermaid)</li> <li> <p>Reset history</p> </li> <li> <p>test_provenance.py: Provenance tracking</p> </li> <li>Change tracking (added/removed/modified)</li> <li>Provenance visualization</li> <li> <p>Rename tracking</p> </li> <li> <p>test_cf_compliance.py: CF convention support</p> </li> <li>Axis inference (X/Y/Z/T)</li> <li>Axis attribute setting</li> <li>CF validation</li> <li> <p>Standard name handling</p> </li> <li> <p>test_io.py: I/O operations</p> </li> <li>Dictionary/JSON/YAML export</li> <li>File save/load</li> <li>xarray conversion (to/from)</li> <li> <p>Zarr export</p> </li> <li> <p>test_validation.py: Dataset validation</p> </li> <li>Structure validation</li> <li>Dimension inference</li> <li> <p>Conflict detection</p> </li> <li> <p>test_data_generation.py: Random data generation</p> </li> <li>Coordinate data generation</li> <li>Variable data generation</li> <li> <p>Metadata-based generation</p> </li> <li> <p>test_mfdataset.py: Multi-file dataset support</p> </li> <li>Opening multiple NetCDF files</li> <li>Frequency inference</li> <li>Time-based grouping</li> <li> <p>File tracking and queries</p> </li> <li> <p>test_ncdump_parser.py: NetCDF metadata parser</p> </li> <li>Parsing ncdump output</li> <li>Dimension extraction</li> <li>Variable and attribute parsing</li> </ul>"},{"location":"testing/#integration-tests-testsintegration","title":"Integration Tests (<code>tests/integration/</code>)","text":"<p>Test complete workflows and interactions between modules:</p> <ul> <li>test_workflows.py: End-to-end workflows</li> <li>CF compliance workflow</li> <li>History and provenance workflow</li> <li>Data generation and export workflow</li> <li>Rename workflow</li> <li>Validation and fix workflow</li> <li>Import/modify/export workflow</li> </ul>"},{"location":"testing/#running-tests","title":"Running Tests","text":""},{"location":"testing/#run-all-tests","title":"Run all tests","text":"<pre><code>pixi run test\n</code></pre>"},{"location":"testing/#run-specific-test-file","title":"Run specific test file","text":"<pre><code>pixi run test tests/unit/test_core.py\n</code></pre>"},{"location":"testing/#run-specific-test-class","title":"Run specific test class","text":"<pre><code>pixi run test tests/unit/test_core.py::TestDummyArray\n</code></pre>"},{"location":"testing/#run-specific-test","title":"Run specific test","text":"<pre><code>pixi run test tests/unit/test_core.py::TestDummyArray::test_init_empty\n</code></pre>"},{"location":"testing/#run-only-unit-tests","title":"Run only unit tests","text":"<pre><code>pixi run test tests/unit/\n</code></pre>"},{"location":"testing/#run-only-integration-tests","title":"Run only integration tests","text":"<pre><code>pixi run test tests/integration/\n</code></pre>"},{"location":"testing/#run-with-markers","title":"Run with markers","text":"<pre><code># Run only unit tests (if marked)\npytest -m unit\n\n# Run only integration tests (if marked)\npytest -m integration\n\n# Skip slow tests\npytest -m \"not slow\"\n</code></pre>"},{"location":"testing/#run-with-coverage","title":"Run with coverage","text":"<pre><code>pytest --cov=dummyxarray --cov-report=html\n</code></pre>"},{"location":"testing/#shared-fixtures-conftestpy","title":"Shared Fixtures (<code>conftest.py</code>)","text":"<p>Common fixtures available to all tests:</p>"},{"location":"testing/#dataset-fixtures","title":"Dataset Fixtures","text":"<ul> <li><code>empty_dataset</code>: Empty DummyDataset</li> <li><code>simple_dataset</code>: Dataset with dimensions only</li> <li><code>dataset_with_coords</code>: Dataset with coordinates</li> <li><code>dataset_with_data</code>: Dataset with data arrays</li> <li><code>cf_compliant_dataset</code>: CF-compliant dataset</li> </ul>"},{"location":"testing/#array-fixtures","title":"Array Fixtures","text":"<ul> <li><code>simple_array</code>: Simple DummyArray</li> <li><code>array_with_data</code>: DummyArray with data</li> </ul>"},{"location":"testing/#parametrized-fixtures","title":"Parametrized Fixtures","text":"<ul> <li><code>with_history</code>: Test with/without history tracking</li> <li><code>viz_format</code>: Test all visualization formats</li> </ul>"},{"location":"testing/#temporary-file-fixtures","title":"Temporary File Fixtures","text":"<ul> <li><code>temp_yaml_file</code>: Temporary YAML file path</li> <li><code>temp_zarr_store</code>: Temporary Zarr store path</li> </ul>"},{"location":"testing/#mock-data-fixtures","title":"Mock Data Fixtures","text":"<ul> <li><code>sample_history</code>: Sample operation history</li> <li><code>sample_provenance</code>: Sample provenance data</li> </ul>"},{"location":"testing/#writing-new-tests","title":"Writing New Tests","text":""},{"location":"testing/#unit-test-template","title":"Unit Test Template","text":"<pre><code>\"\"\"Tests for [module] functionality.\"\"\"\n\nimport pytest\nfrom dummyxarray import DummyDataset\n\n\nclass TestFeature:\n    \"\"\"Test [feature] functionality.\"\"\"\n\n    def test_basic_case(self, simple_dataset):\n        \"\"\"Test basic [feature] usage.\"\"\"\n        # Arrange\n        ds = simple_dataset\n\n        # Act\n        result = ds.some_method()\n\n        # Assert\n        assert result is not None\n</code></pre>"},{"location":"testing/#integration-test-template","title":"Integration Test Template","text":"<pre><code>\"\"\"Integration tests for [workflow].\"\"\"\n\nimport pytest\nfrom dummyxarray import DummyDataset\n\n\nclass TestWorkflow:\n    \"\"\"Test [workflow] end-to-end.\"\"\"\n\n    def test_complete_workflow(self, tmp_path):\n        \"\"\"Test complete [workflow] from start to finish.\"\"\"\n        # Create dataset\n        ds = DummyDataset()\n\n        # Perform workflow steps\n        ds.add_dim(\"time\", 10)\n        # ... more steps\n\n        # Verify final state\n        assert ds.dims[\"time\"] == 10\n</code></pre>"},{"location":"testing/#best-practices","title":"Best Practices","text":"<ol> <li>Use fixtures: Leverage shared fixtures from <code>conftest.py</code></li> <li>Test one thing: Each test should verify one specific behavior</li> <li>Descriptive names: Test names should describe what they test</li> <li>Arrange-Act-Assert: Follow AAA pattern for clarity</li> <li>Parametrize: Use <code>@pytest.mark.parametrize</code> for similar tests</li> <li>Mock external dependencies: Don't rely on external services</li> <li>Clean up: Use fixtures with cleanup or <code>tmp_path</code> for files</li> </ol>"},{"location":"testing/#test-coverage","title":"Test Coverage","text":"<p>Current coverage: 188 tests across all modules</p> <ul> <li>Unit tests: ~160 tests</li> <li>Integration tests: ~28 tests</li> </ul> <p>Target: &gt;90% code coverage</p>"},{"location":"api/array/","title":"DummyArray","text":"<p>Represents a single array (variable or coordinate) with metadata.</p>"},{"location":"api/array/#overview","title":"Overview","text":"<p><code>DummyArray</code> is used for both coordinates and variables in a <code>DummyDataset</code>. Each array contains:</p> <ul> <li>Dimensions - List of dimension names</li> <li>Attributes - Metadata dictionary</li> <li>Data - Optional numpy array</li> <li>Encoding - Encoding parameters (dtype, chunks, compression)</li> </ul>"},{"location":"api/array/#key-features","title":"Key Features","text":"<ul> <li>Automatic dimension inference from data shape</li> <li>xarray-compatible attribute assignment</li> <li>History tracking (if enabled)</li> <li>Rich repr for interactive exploration</li> </ul>"},{"location":"api/array/#usage","title":"Usage","text":"<pre><code>from dummyxarray import DummyArray\nimport numpy as np\n\n# Create array with dimensions\narr = DummyArray(dims=[\"time\", \"lat\", \"lon\"])\n\n# Set attributes\narr.assign_attrs(\n    standard_name=\"air_temperature\",\n    units=\"K\",\n    long_name=\"Air Temperature\"\n)\n\n# Add data\narr.data = np.random.rand(10, 64, 128)\n\n# Set encoding\narr.encoding = {\n    \"dtype\": \"float32\",\n    \"chunks\": (5, 32, 64),\n    \"compressor\": \"zstd\"\n}\n</code></pre>"},{"location":"api/array/#api-reference","title":"API Reference","text":"<p>Represents a single array (variable or coordinate) with metadata.</p> Source code in <code>src/dummyxarray/core.py</code> <pre><code>class DummyArray:\n    \"\"\"Represents a single array (variable or coordinate) with metadata.\"\"\"\n\n    def __init__(self, dims=None, attrs=None, data=None, encoding=None, _record_history=True):\n        \"\"\"\n        Initialize a DummyArray.\n\n        Parameters\n        ----------\n        dims : list of str, optional\n            List of dimension names\n        attrs : dict, optional\n            Metadata attributes\n        data : array-like, optional\n            Data array (numpy array or list)\n        encoding : dict, optional\n            Encoding parameters (dtype, chunks, compressor, fill_value, etc.)\n        _record_history : bool, optional\n            Whether to record operation history (default: True)\n        \"\"\"\n        self.dims = dims\n        self.attrs = attrs or {}\n        self.data = data\n        self.encoding = encoding or {}\n\n        # Operation history tracking\n        self._history = [] if _record_history else None\n        if _record_history:\n            # Record initialization\n            init_args = {}\n            if dims is not None:\n                init_args[\"dims\"] = dims\n            if attrs:\n                init_args[\"attrs\"] = attrs\n            if data is not None:\n                init_args[\"data\"] = \"&lt;data&gt;\"\n            if encoding:\n                init_args[\"encoding\"] = encoding\n            self._record_operation(\"__init__\", init_args)\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the DummyArray.\"\"\"\n        lines = [\"&lt;dummyxarray.DummyArray&gt;\"]\n\n        # Dimensions\n        if self.dims:\n            lines.append(f\"Dimensions: ({', '.join(self.dims)})\")\n        else:\n            lines.append(\"Dimensions: ()\")\n\n        # Data info\n        if self.data is not None:\n            data_array = np.asarray(self.data)\n            lines.append(f\"Shape: {data_array.shape}\")\n            lines.append(f\"dtype: {data_array.dtype}\")\n\n            # Show a preview of the data\n            if data_array.size &lt;= 10:\n                lines.append(f\"Data: {data_array}\")\n            else:\n                flat = data_array.flatten()\n                preview = f\"[{flat[0]}, {flat[1]}, ..., {flat[-1]}]\"\n                lines.append(f\"Data: {preview}\")\n        else:\n            lines.append(\"Data: None\")\n\n        # Attributes\n        if self.attrs:\n            lines.append(\"Attributes:\")\n            for key, value in self.attrs.items():\n                value_str = str(value)\n                if len(value_str) &gt; 50:\n                    value_str = value_str[:47] + \"...\"\n                lines.append(f\"    {key}: {value_str}\")\n\n        # Encoding\n        if self.encoding:\n            lines.append(\"Encoding:\")\n            for key, value in self.encoding.items():\n                lines.append(f\"    {key}: {value}\")\n\n        return \"\\n\".join(lines)\n\n    def infer_dims_from_data(self):\n        \"\"\"\n        Infer dimension names and sizes from data shape.\n\n        Returns\n        -------\n        dict\n            Dictionary mapping dimension names to sizes\n        \"\"\"\n        if self.data is not None:\n            shape = np.asarray(self.data).shape\n            if self.dims is None:\n                self.dims = [f\"dim_{i}\" for i in range(len(shape))]\n            return dict(zip(self.dims, shape, strict=True))\n        return {}\n\n    def _record_operation(self, func_name, args):\n        \"\"\"\n        Record an operation in the history.\n\n        Parameters\n        ----------\n        func_name : str\n            Name of the function/method called\n        args : dict\n            Arguments passed to the function\n        \"\"\"\n        if self._history is not None:\n            self._history.append({\"func\": func_name, \"args\": args})\n\n    def get_history(self):\n        \"\"\"\n        Get the operation history for this array.\n\n        Returns\n        -------\n        list of dict\n            List of operations\n\n        Examples\n        --------\n        &gt;&gt;&gt; arr = DummyArray(dims=[\"time\"], attrs={\"units\": \"days\"})\n        &gt;&gt;&gt; arr.get_history()\n        [{'func': '__init__', 'args': {'dims': ['time'], 'attrs': {'units': 'days'}}}]\n        \"\"\"\n        return self._history.copy() if self._history is not None else []\n\n    def replay_history(self, history=None):\n        \"\"\"\n        Replay a sequence of operations to recreate an array.\n\n        Parameters\n        ----------\n        history : list of dict, optional\n            History to replay. If None, uses self.get_history()\n\n        Returns\n        -------\n        DummyArray\n            New array with operations replayed\n        \"\"\"\n        if history is None:\n            history = self.get_history()\n\n        # Get __init__ args\n        init_op = history[0] if history and history[0][\"func\"] == \"__init__\" else {}\n        init_args = init_op.get(\"args\", {})\n\n        # Create new array\n        arr = DummyArray(**init_args, _record_history=False)\n\n        # Replay other operations\n        for op in history[1:]:\n            func = getattr(arr, op[\"func\"], None)\n            if func and callable(func):\n                func(**op[\"args\"])\n\n        return arr\n\n    def assign_attrs(self, **kwargs):\n        \"\"\"\n        Assign new attributes to this array (xarray-compatible API).\n\n        Parameters\n        ----------\n        **kwargs\n            Attributes to assign\n\n        Returns\n        -------\n        self\n            Returns self for method chaining\n\n        Examples\n        --------\n        &gt;&gt;&gt; arr = DummyArray(dims=[\"time\"])\n        &gt;&gt;&gt; arr.assign_attrs(units=\"days\", calendar=\"gregorian\")\n        \"\"\"\n        self._record_operation(\"assign_attrs\", kwargs)\n        self.attrs.update(kwargs)\n        return self\n\n    def to_dict(self):\n        \"\"\"\n        Export structure (without data) for serialization.\n\n        Returns\n        -------\n        dict\n            Dictionary representation\n        \"\"\"\n        return {\n            \"dims\": self.dims,\n            \"attrs\": self.attrs,\n            \"encoding\": self.encoding,\n            \"has_data\": self.data is not None,\n        }\n</code></pre>"},{"location":"api/array/#dummyxarray.DummyArray.__init__","title":"__init__","text":"<pre><code>__init__(\n    dims=None,\n    attrs=None,\n    data=None,\n    encoding=None,\n    _record_history=True,\n)\n</code></pre> <p>Initialize a DummyArray.</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>list of str</code> <p>List of dimension names</p> <code>None</code> <code>attrs</code> <code>dict</code> <p>Metadata attributes</p> <code>None</code> <code>data</code> <code>array - like</code> <p>Data array (numpy array or list)</p> <code>None</code> <code>encoding</code> <code>dict</code> <p>Encoding parameters (dtype, chunks, compressor, fill_value, etc.)</p> <code>None</code> <code>_record_history</code> <code>bool</code> <p>Whether to record operation history (default: True)</p> <code>True</code> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def __init__(self, dims=None, attrs=None, data=None, encoding=None, _record_history=True):\n    \"\"\"\n    Initialize a DummyArray.\n\n    Parameters\n    ----------\n    dims : list of str, optional\n        List of dimension names\n    attrs : dict, optional\n        Metadata attributes\n    data : array-like, optional\n        Data array (numpy array or list)\n    encoding : dict, optional\n        Encoding parameters (dtype, chunks, compressor, fill_value, etc.)\n    _record_history : bool, optional\n        Whether to record operation history (default: True)\n    \"\"\"\n    self.dims = dims\n    self.attrs = attrs or {}\n    self.data = data\n    self.encoding = encoding or {}\n\n    # Operation history tracking\n    self._history = [] if _record_history else None\n    if _record_history:\n        # Record initialization\n        init_args = {}\n        if dims is not None:\n            init_args[\"dims\"] = dims\n        if attrs:\n            init_args[\"attrs\"] = attrs\n        if data is not None:\n            init_args[\"data\"] = \"&lt;data&gt;\"\n        if encoding:\n            init_args[\"encoding\"] = encoding\n        self._record_operation(\"__init__\", init_args)\n</code></pre>"},{"location":"api/array/#dummyxarray.DummyArray.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Return a string representation of the DummyArray.</p> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a string representation of the DummyArray.\"\"\"\n    lines = [\"&lt;dummyxarray.DummyArray&gt;\"]\n\n    # Dimensions\n    if self.dims:\n        lines.append(f\"Dimensions: ({', '.join(self.dims)})\")\n    else:\n        lines.append(\"Dimensions: ()\")\n\n    # Data info\n    if self.data is not None:\n        data_array = np.asarray(self.data)\n        lines.append(f\"Shape: {data_array.shape}\")\n        lines.append(f\"dtype: {data_array.dtype}\")\n\n        # Show a preview of the data\n        if data_array.size &lt;= 10:\n            lines.append(f\"Data: {data_array}\")\n        else:\n            flat = data_array.flatten()\n            preview = f\"[{flat[0]}, {flat[1]}, ..., {flat[-1]}]\"\n            lines.append(f\"Data: {preview}\")\n    else:\n        lines.append(\"Data: None\")\n\n    # Attributes\n    if self.attrs:\n        lines.append(\"Attributes:\")\n        for key, value in self.attrs.items():\n            value_str = str(value)\n            if len(value_str) &gt; 50:\n                value_str = value_str[:47] + \"...\"\n            lines.append(f\"    {key}: {value_str}\")\n\n    # Encoding\n    if self.encoding:\n        lines.append(\"Encoding:\")\n        for key, value in self.encoding.items():\n            lines.append(f\"    {key}: {value}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/array/#dummyxarray.DummyArray.infer_dims_from_data","title":"infer_dims_from_data","text":"<pre><code>infer_dims_from_data()\n</code></pre> <p>Infer dimension names and sizes from data shape.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary mapping dimension names to sizes</p> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def infer_dims_from_data(self):\n    \"\"\"\n    Infer dimension names and sizes from data shape.\n\n    Returns\n    -------\n    dict\n        Dictionary mapping dimension names to sizes\n    \"\"\"\n    if self.data is not None:\n        shape = np.asarray(self.data).shape\n        if self.dims is None:\n            self.dims = [f\"dim_{i}\" for i in range(len(shape))]\n        return dict(zip(self.dims, shape, strict=True))\n    return {}\n</code></pre>"},{"location":"api/array/#dummyxarray.DummyArray.get_history","title":"get_history","text":"<pre><code>get_history()\n</code></pre> <p>Get the operation history for this array.</p> <p>Returns:</p> Type Description <code>list of dict</code> <p>List of operations</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = DummyArray(dims=[\"time\"], attrs={\"units\": \"days\"})\n&gt;&gt;&gt; arr.get_history()\n[{'func': '__init__', 'args': {'dims': ['time'], 'attrs': {'units': 'days'}}}]\n</code></pre> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def get_history(self):\n    \"\"\"\n    Get the operation history for this array.\n\n    Returns\n    -------\n    list of dict\n        List of operations\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = DummyArray(dims=[\"time\"], attrs={\"units\": \"days\"})\n    &gt;&gt;&gt; arr.get_history()\n    [{'func': '__init__', 'args': {'dims': ['time'], 'attrs': {'units': 'days'}}}]\n    \"\"\"\n    return self._history.copy() if self._history is not None else []\n</code></pre>"},{"location":"api/array/#dummyxarray.DummyArray.replay_history","title":"replay_history","text":"<pre><code>replay_history(history=None)\n</code></pre> <p>Replay a sequence of operations to recreate an array.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>list of dict</code> <p>History to replay. If None, uses self.get_history()</p> <code>None</code> <p>Returns:</p> Type Description <code>DummyArray</code> <p>New array with operations replayed</p> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def replay_history(self, history=None):\n    \"\"\"\n    Replay a sequence of operations to recreate an array.\n\n    Parameters\n    ----------\n    history : list of dict, optional\n        History to replay. If None, uses self.get_history()\n\n    Returns\n    -------\n    DummyArray\n        New array with operations replayed\n    \"\"\"\n    if history is None:\n        history = self.get_history()\n\n    # Get __init__ args\n    init_op = history[0] if history and history[0][\"func\"] == \"__init__\" else {}\n    init_args = init_op.get(\"args\", {})\n\n    # Create new array\n    arr = DummyArray(**init_args, _record_history=False)\n\n    # Replay other operations\n    for op in history[1:]:\n        func = getattr(arr, op[\"func\"], None)\n        if func and callable(func):\n            func(**op[\"args\"])\n\n    return arr\n</code></pre>"},{"location":"api/array/#dummyxarray.DummyArray.assign_attrs","title":"assign_attrs","text":"<pre><code>assign_attrs(**kwargs)\n</code></pre> <p>Assign new attributes to this array (xarray-compatible API).</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Attributes to assign</p> <code>{}</code> <p>Returns:</p> Type Description <code>self</code> <p>Returns self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = DummyArray(dims=[\"time\"])\n&gt;&gt;&gt; arr.assign_attrs(units=\"days\", calendar=\"gregorian\")\n</code></pre> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def assign_attrs(self, **kwargs):\n    \"\"\"\n    Assign new attributes to this array (xarray-compatible API).\n\n    Parameters\n    ----------\n    **kwargs\n        Attributes to assign\n\n    Returns\n    -------\n    self\n        Returns self for method chaining\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = DummyArray(dims=[\"time\"])\n    &gt;&gt;&gt; arr.assign_attrs(units=\"days\", calendar=\"gregorian\")\n    \"\"\"\n    self._record_operation(\"assign_attrs\", kwargs)\n    self.attrs.update(kwargs)\n    return self\n</code></pre>"},{"location":"api/array/#dummyxarray.DummyArray.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Export structure (without data) for serialization.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary representation</p> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def to_dict(self):\n    \"\"\"\n    Export structure (without data) for serialization.\n\n    Returns\n    -------\n    dict\n        Dictionary representation\n    \"\"\"\n    return {\n        \"dims\": self.dims,\n        \"attrs\": self.attrs,\n        \"encoding\": self.encoding,\n        \"has_data\": self.data is not None,\n    }\n</code></pre>"},{"location":"api/dataset/","title":"DummyDataset","text":"<p>The main class for creating and managing dataset metadata specifications.</p> <p><code>DummyDataset</code> is composed of multiple mixins that provide different functionality:</p> <ul> <li>Core - Basic dataset operations (dimensions, coordinates, variables)</li> <li>HistoryMixin - Operation tracking and replay</li> <li>ProvenanceMixin - Track what changed in operations</li> <li>CFComplianceMixin - CF convention support</li> <li>CFStandardsMixin - CF standard names and vocabulary</li> <li>IOMixin - Serialization and format conversion</li> <li>ValidationMixin - Dataset structure validation</li> <li>DataGenerationMixin - Generate realistic random data</li> <li>FileTrackerMixin - Track source files in multi-file datasets</li> </ul>"},{"location":"api/dataset/#class-reference","title":"Class Reference","text":"<p>               Bases: <code>HistoryMixin</code>, <code>ProvenanceMixin</code>, <code>CFComplianceMixin</code>, <code>CFStandardsMixin</code>, <code>IOMixin</code>, <code>ValidationMixin</code>, <code>DataGenerationMixin</code>, <code>FileTrackerMixin</code></p> <p>A dummy xarray-like dataset for building metadata specifications.</p> <p>This class allows you to define the structure of a dataset including dimensions, coordinates, variables, and global attributes before creating the actual xarray.Dataset with real data.</p> Source code in <code>src/dummyxarray/core.py</code> <pre><code>class DummyDataset(\n    HistoryMixin,\n    ProvenanceMixin,\n    CFComplianceMixin,\n    CFStandardsMixin,\n    IOMixin,\n    ValidationMixin,\n    DataGenerationMixin,\n    FileTrackerMixin,\n):\n    \"\"\"\n    A dummy xarray-like dataset for building metadata specifications.\n\n    This class allows you to define the structure of a dataset including\n    dimensions, coordinates, variables, and global attributes before\n    creating the actual xarray.Dataset with real data.\n    \"\"\"\n\n    def __init__(self, _record_history=True):\n        \"\"\"\n        Initialize an empty DummyDataset.\n\n        Parameters\n        ----------\n        _record_history : bool, optional\n            Whether to record operation history (default: True)\n        \"\"\"\n        self.dims = {}  # dim_name \u2192 size\n        self.coords = {}  # coord_name \u2192 DummyArray\n        self.variables = {}  # var_name  \u2192 DummyArray\n        self.attrs = {}  # global attributes\n\n        # Operation history tracking\n        self._history = [] if _record_history else None\n        if _record_history:\n            self._record_operation(\"__init__\", {})\n\n    def __repr__(self):\n        \"\"\"Return a string representation similar to xarray.Dataset.\"\"\"\n        lines = [\"&lt;dummyxarray.DummyDataset&gt;\"]\n\n        # Dimensions\n        if self.dims:\n            lines.append(\"Dimensions:\")\n            dim_strs = [f\"  {name}: {size}\" for name, size in self.dims.items()]\n            lines.extend(dim_strs)\n        else:\n            lines.append(\"Dimensions: ()\")\n\n        # Coordinates\n        if self.coords:\n            lines.append(\"Coordinates:\")\n            for name, arr in self.coords.items():\n                dims_str = f\"({', '.join(arr.dims)})\" if arr.dims else \"()\"\n                has_data = \"\u2713\" if arr.data is not None else \"\u2717\"\n                dtype_str = f\"{arr.data.dtype}\" if arr.data is not None else \"?\"\n                lines.append(f\"  {has_data} {name:20s} {dims_str:20s} {dtype_str}\")\n\n        # Data variables\n        if self.variables:\n            lines.append(\"Data variables:\")\n            for name, arr in self.variables.items():\n                dims_str = f\"({', '.join(arr.dims)})\" if arr.dims else \"()\"\n                has_data = \"\u2713\" if arr.data is not None else \"\u2717\"\n                dtype_str = f\"{arr.data.dtype}\" if arr.data is not None else \"?\"\n                lines.append(f\"  {has_data} {name:20s} {dims_str:20s} {dtype_str}\")\n\n        # Global attributes\n        if self.attrs:\n            lines.append(\"Attributes:\")\n            for key, value in self.attrs.items():\n                value_str = str(value)\n                if len(value_str) &gt; 50:\n                    value_str = value_str[:47] + \"...\"\n                lines.append(f\"    {key}: {value_str}\")\n\n        return \"\\n\".join(lines)\n\n    def __getattr__(self, name):\n        \"\"\"\n        Allow attribute-style access to coordinates and variables.\n\n        This enables xarray-style access like `ds.time` instead of `ds.coords['time']`.\n        Coordinates take precedence over variables if both exist with the same name.\n\n        Parameters\n        ----------\n        name : str\n            Name of the coordinate or variable to access\n\n        Returns\n        -------\n        DummyArray\n            The coordinate or variable array\n\n        Raises\n        ------\n        AttributeError\n            If the name is not found in coords or variables\n        \"\"\"\n        # Check coordinates first (like xarray does)\n        if name in self.coords:\n            return self.coords[name]\n        # Then check variables\n        if name in self.variables:\n            return self.variables[name]\n        # If not found, raise AttributeError\n        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n\n    def __setattr__(self, name, value):\n        \"\"\"\n        Handle attribute assignment.\n\n        Special handling for internal attributes (dims, coords, variables, attrs).\n        For other names, this could be extended to allow setting coords/variables.\n        \"\"\"\n        # Internal attributes that should be set normally\n        # Allow private attributes (starting with _) for mixins\n        if name in (\"dims\", \"coords\", \"variables\", \"attrs\", \"_history\") or name.startswith(\"_\"):\n            object.__setattr__(self, name, value)\n        else:\n            # For now, raise an error to avoid confusion\n            # Could be extended to allow ds.time = DummyArray(...) in the future\n            raise AttributeError(\n                f\"Cannot set attribute '{name}' directly. \"\n                f\"Use ds.coords['{name}'] or ds.variables['{name}'] instead.\"\n            )\n\n    def __dir__(self):\n        \"\"\"\n        Customize dir() output to include coordinates and variables.\n\n        This makes tab-completion work in IPython/Jupyter.\n        \"\"\"\n        # Get default attributes\n        default_attrs = set(object.__dir__(self))\n        # Add coordinate and variable names\n        return sorted(default_attrs | set(self.coords.keys()) | set(self.variables.keys()))\n\n    # ------------------------------------------------------------\n    # Core API\n    # ------------------------------------------------------------\n\n    def set_global_attrs(self, **kwargs):\n        \"\"\"\n        Set or update global dataset attributes.\n\n        Parameters\n        ----------\n        **kwargs\n            Attributes to set\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.set_global_attrs(title=\"My Dataset\", institution=\"DKRZ\")\n        \"\"\"\n        self.attrs.update(kwargs)\n\n    def assign_attrs(self, **kwargs):\n        \"\"\"\n        Assign new global attributes to this dataset (xarray-compatible API).\n\n        Parameters\n        ----------\n        **kwargs\n            Attributes to assign\n\n        Returns\n        -------\n        self\n            Returns self for method chaining\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.assign_attrs(title=\"My Dataset\", institution=\"DKRZ\")\n        \"\"\"\n        # Capture provenance\n        provenance = {\"modified\": {}}\n        for key, value in kwargs.items():\n            old_value = self.attrs.get(key)\n            provenance[\"modified\"][key] = {\"before\": old_value, \"after\": value}\n\n        self._record_operation(\"assign_attrs\", kwargs, provenance)\n        self.attrs.update(kwargs)\n        return self\n\n    def add_dim(self, name, size):\n        \"\"\"\n        Add a dimension with a specific size.\n\n        Parameters\n        ----------\n        name : str\n            Dimension name\n        size : int\n            Dimension size\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n        &gt;&gt;&gt; ds.add_dim(\"lat\", 64)\n        \"\"\"\n        # Capture provenance\n        if name in self.dims:\n            provenance = {\"modified\": {name: {\"before\": self.dims[name], \"after\": size}}}\n        else:\n            provenance = {\"added\": [name]}\n\n        self._record_operation(\"add_dim\", {\"name\": name, \"size\": size}, provenance)\n        self.dims[name] = size\n\n    def add_coord(self, name, dims=None, attrs=None, data=None, encoding=None):\n        \"\"\"\n        Add a coordinate variable.\n\n        Parameters\n        ----------\n        name : str\n            Coordinate name\n        dims : list of str, optional\n            Dimension names\n        attrs : dict, optional\n            Metadata attributes\n        data : array-like, optional\n            Coordinate data\n        encoding : dict, optional\n            Encoding parameters\n        \"\"\"\n        # Record operation (don't store actual data)\n        args = {\"name\": name}\n        if dims is not None:\n            args[\"dims\"] = dims\n        if attrs:\n            args[\"attrs\"] = attrs\n        if data is not None:\n            args[\"data\"] = \"&lt;data&gt;\"\n        if encoding:\n            args[\"encoding\"] = encoding\n\n        # Capture provenance\n        provenance = {}\n        if name in self.coords:\n            # Coordinate already exists - track what changed\n            old_coord = self.coords[name]\n            changes = {}\n            if dims != old_coord.dims:\n                changes[\"dims\"] = {\"before\": old_coord.dims, \"after\": dims}\n            if attrs and attrs != old_coord.attrs:\n                changes[\"attrs\"] = {\"before\": old_coord.attrs.copy(), \"after\": attrs}\n            if changes:\n                provenance[\"modified\"] = {name: changes}\n        else:\n            provenance[\"added\"] = [name]\n\n        self._record_operation(\"add_coord\", args, provenance)\n\n        arr = DummyArray(dims, attrs, data, encoding, _record_history=False)\n        self._infer_and_register_dims(arr)\n        self.coords[name] = arr\n\n    def add_variable(self, name, dims=None, attrs=None, data=None, encoding=None):\n        \"\"\"\n        Add a data variable.\n\n        Parameters\n        ----------\n        name : str\n            Variable name\n        dims : list of str, optional\n            Dimension names\n        attrs : dict, optional\n            Metadata attributes\n        data : array-like, optional\n            Variable data\n        encoding : dict, optional\n            Encoding parameters\n        \"\"\"\n        # Record operation (don't store actual data)\n        args = {\"name\": name}\n        if dims is not None:\n            args[\"dims\"] = dims\n        if attrs:\n            args[\"attrs\"] = attrs\n        if data is not None:\n            args[\"data\"] = \"&lt;data&gt;\"\n        if encoding:\n            args[\"encoding\"] = encoding\n\n        # Capture provenance\n        provenance = {}\n        if name in self.variables:\n            # Variable already exists - track what changed\n            old_var = self.variables[name]\n            changes = {}\n            if dims != old_var.dims:\n                changes[\"dims\"] = {\"before\": old_var.dims, \"after\": dims}\n            if attrs and attrs != old_var.attrs:\n                changes[\"attrs\"] = {\"before\": old_var.attrs.copy(), \"after\": attrs}\n            if changes:\n                provenance[\"modified\"] = {name: changes}\n        else:\n            provenance[\"added\"] = [name]\n\n        self._record_operation(\"add_variable\", args, provenance)\n\n        arr = DummyArray(dims, attrs, data, encoding, _record_history=False)\n        self._infer_and_register_dims(arr)\n        self.variables[name] = arr\n\n    def rename_dims(self, dims_dict=None, **dims):\n        \"\"\"\n        Rename dimensions (xarray-compatible API).\n\n        Parameters\n        ----------\n        dims_dict : dict-like, optional\n            Dictionary whose keys are current dimension names and whose\n            values are the desired names.\n        **dims : optional\n            Keyword form of dims_dict.\n            One of dims_dict or dims must be provided.\n\n        Returns\n        -------\n        DummyDataset\n            Returns self for method chaining\n\n        Raises\n        ------\n        KeyError\n            If a dimension doesn't exist\n        ValueError\n            If a new name already exists\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n        &gt;&gt;&gt; ds.add_dim(\"lat\", 64)\n        &gt;&gt;&gt; ds.rename_dims({\"time\": \"t\", \"lat\": \"latitude\"})\n        &gt;&gt;&gt; # Or using keyword arguments:\n        &gt;&gt;&gt; ds.rename_dims(time=\"t\", lat=\"latitude\")\n        \"\"\"\n        # Merge dims_dict and **dims\n        name_dict = {}\n        if dims_dict is not None:\n            name_dict.update(dims_dict)\n        name_dict.update(dims)\n\n        if not name_dict:\n            raise ValueError(\"Either dims_dict or keyword arguments must be provided\")\n\n        # Validate all renames first\n        for old_name, new_name in name_dict.items():\n            if old_name not in self.dims:\n                raise KeyError(f\"Dimension '{old_name}' does not exist\")\n            if new_name in self.dims and new_name != old_name:\n                raise ValueError(f\"Dimension '{new_name}' already exists\")\n\n        # Capture provenance\n        provenance = {\n            \"renamed\": name_dict.copy(),\n            \"removed\": list(name_dict.keys()),\n            \"added\": list(name_dict.values()),\n        }\n\n        self._record_operation(\"rename_dims\", {\"dims_dict\": name_dict}, provenance)\n\n        # Perform all renames\n        for old_name, new_name in name_dict.items():\n            if old_name != new_name:\n                self.dims[new_name] = self.dims.pop(old_name)\n\n                # Update dimension references in coords and variables\n                for coord in self.coords.values():\n                    if coord.dims:\n                        coord.dims = [new_name if d == old_name else d for d in coord.dims]\n                for var in self.variables.values():\n                    if var.dims:\n                        var.dims = [new_name if d == old_name else d for d in var.dims]\n\n        return self\n\n    def rename_vars(self, name_dict=None, **names):\n        \"\"\"\n        Rename variables (xarray-compatible API).\n\n        Parameters\n        ----------\n        name_dict : dict-like, optional\n            Dictionary whose keys are current variable names and whose\n            values are the desired names.\n        **names : optional\n            Keyword form of name_dict.\n            One of name_dict or names must be provided.\n\n        Returns\n        -------\n        DummyDataset\n            Returns self for method chaining\n\n        Raises\n        ------\n        KeyError\n            If a variable doesn't exist\n        ValueError\n            If a new name already exists\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n        &gt;&gt;&gt; ds.add_variable(\"temperature\", dims=[\"time\"])\n        &gt;&gt;&gt; ds.rename_vars({\"temperature\": \"temp\"})\n        &gt;&gt;&gt; # Or using keyword arguments:\n        &gt;&gt;&gt; ds.rename_vars(temperature=\"temp\")\n        \"\"\"\n        # Merge name_dict and **names\n        rename_dict = {}\n        if name_dict is not None:\n            rename_dict.update(name_dict)\n        rename_dict.update(names)\n\n        if not rename_dict:\n            raise ValueError(\"Either name_dict or keyword arguments must be provided\")\n\n        # Validate all renames first\n        for old_name, new_name in rename_dict.items():\n            if old_name not in self.variables:\n                raise KeyError(f\"Variable '{old_name}' does not exist\")\n            if new_name in self.variables and new_name != old_name:\n                raise ValueError(f\"Variable '{new_name}' already exists\")\n\n        # Capture provenance\n        provenance = {\n            \"renamed\": rename_dict.copy(),\n            \"removed\": list(rename_dict.keys()),\n            \"added\": list(rename_dict.values()),\n        }\n\n        self._record_operation(\"rename_vars\", {\"name_dict\": rename_dict}, provenance)\n\n        # Perform all renames\n        for old_name, new_name in rename_dict.items():\n            if old_name != new_name:\n                self.variables[new_name] = self.variables.pop(old_name)\n\n        return self\n\n    def rename(self, name_dict=None, **names):\n        \"\"\"\n        Rename variables, coordinates, and dimensions (xarray-compatible API).\n\n        This method can rename any combination of variables, coordinates, and dimensions.\n\n        Parameters\n        ----------\n        name_dict : dict-like, optional\n            Dictionary whose keys are current names (variables, coordinates, or dimensions)\n            and whose values are the desired names.\n        **names : optional\n            Keyword form of name_dict.\n            One of name_dict or names must be provided.\n\n        Returns\n        -------\n        DummyDataset\n            Returns self for method chaining\n\n        Raises\n        ------\n        KeyError\n            If a name doesn't exist\n        ValueError\n            If a new name already exists\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n        &gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"])\n        &gt;&gt;&gt; ds.add_variable(\"temperature\", dims=[\"time\"])\n        &gt;&gt;&gt; # Rename multiple items at once\n        &gt;&gt;&gt; ds.rename({\"time\": \"t\", \"temperature\": \"temp\"})\n        &gt;&gt;&gt; # Or using keyword arguments:\n        &gt;&gt;&gt; ds.rename(time=\"t\", temperature=\"temp\")\n        \"\"\"\n        # Merge name_dict and **names\n        rename_dict = {}\n        if name_dict is not None:\n            rename_dict.update(name_dict)\n        rename_dict.update(names)\n\n        if not rename_dict:\n            raise ValueError(\"Either name_dict or keyword arguments must be provided\")\n\n        # Categorize renames\n        dim_renames = {}\n        coord_renames = {}\n        var_renames = {}\n\n        for old_name, new_name in rename_dict.items():\n            if old_name in self.dims:\n                dim_renames[old_name] = new_name\n            if old_name in self.coords:\n                coord_renames[old_name] = new_name\n            if old_name in self.variables:\n                var_renames[old_name] = new_name\n\n            # Check if name exists anywhere\n            if (\n                old_name not in self.dims\n                and old_name not in self.coords\n                and old_name not in self.variables\n            ):\n                raise KeyError(\n                    f\"'{old_name}' does not exist in dimensions, coordinates, or variables\"\n                )\n\n        # Capture provenance\n        provenance = {\n            \"renamed\": rename_dict.copy(),\n            \"removed\": list(rename_dict.keys()),\n            \"added\": list(rename_dict.values()),\n        }\n\n        self._record_operation(\"rename\", {\"name_dict\": rename_dict}, provenance)\n\n        # Perform renames in order: dimensions first (affects coords/vars), then coords, then vars\n        if dim_renames:\n            for old_name, new_name in dim_renames.items():\n                if old_name != new_name and old_name in self.dims:\n                    self.dims[new_name] = self.dims.pop(old_name)\n                    # Update dimension references\n                    for coord in self.coords.values():\n                        if coord.dims:\n                            coord.dims = [new_name if d == old_name else d for d in coord.dims]\n                    for var in self.variables.values():\n                        if var.dims:\n                            var.dims = [new_name if d == old_name else d for d in var.dims]\n\n        if coord_renames:\n            for old_name, new_name in coord_renames.items():\n                if old_name != new_name and old_name in self.coords:\n                    self.coords[new_name] = self.coords.pop(old_name)\n\n        if var_renames:\n            for old_name, new_name in var_renames.items():\n                if old_name != new_name and old_name in self.variables:\n                    self.variables[new_name] = self.variables.pop(old_name)\n\n        return self\n\n    @classmethod\n    def open_mfdataset(cls, paths, concat_dim=\"time\", combine=\"nested\", **kwargs):\n        \"\"\"Open multiple files as a single DummyDataset with file tracking.\n\n        This class method reads metadata from multiple NetCDF files and combines them\n        into a single DummyDataset, tracking which files contribute to which\n        coordinate ranges along the concatenation dimension.\n\n        Parameters\n        ----------\n        paths : str or list of str\n            Either a glob pattern (e.g., \"data/*.nc\") or a list of file paths\n        concat_dim : str, optional\n            The dimension along which to concatenate files (default: \"time\")\n        combine : str, optional\n            How to combine datasets. Currently supports \"nested\" (default)\n        **kwargs : optional\n            Additional keyword arguments (reserved for future use)\n\n        Returns\n        -------\n        DummyDataset\n            A DummyDataset with metadata from all files and file tracking enabled\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset.open_mfdataset(\"data/*.nc\", concat_dim=\"time\")\n        &gt;&gt;&gt; files = ds.get_source_files(time=slice(0, 10))\n        &gt;&gt;&gt; print(files)\n        ['data/file1.nc', 'data/file2.nc']\n\n        See Also\n        --------\n        enable_file_tracking : Enable file tracking on an existing dataset\n        get_source_files : Query which files contain specific coordinate ranges\n        \"\"\"\n        from .mfdataset import open_mfdataset\n\n        return open_mfdataset(paths, concat_dim=concat_dim, combine=combine, **kwargs)\n\n    def groupby_time(\n        self,\n        freq: str,\n        dim: str = \"time\",\n        normalize_units: bool = True,\n    ) -&gt; List[\"DummyDataset\"]:\n        \"\"\"Group dataset by time frequency using metadata only.\n\n        This method splits a multi-file dataset into time-based groups without loading\n        any data arrays. Each group is a new DummyDataset with adjusted metadata.\n\n        Parameters\n        ----------\n        freq : str\n            Grouping frequency using pandas-style strings:\n            - Years: '1Y', '5Y', '10Y'\n            - Months: '1M', '3M', '6M'\n            - Days: '1D', '7D', '30D'\n            - Hours: '1H', '6H', '12H'\n        dim : str, default \"time\"\n            Time dimension to group by\n        normalize_units : bool, default True\n            Update time units to reference each group's start datetime\n\n        Returns\n        -------\n        list of DummyDataset\n            One dataset per time group, each with:\n            - Updated time:units attribute (if normalize_units=True)\n            - Filtered file sources for that time period\n            - Adjusted dimension sizes\n            - Preserved frequency attribute\n\n        Raises\n        ------\n        ValueError\n            If time coordinate has no frequency attribute (open with open_mfdataset)\n        ValueError\n            If time coordinate has no units attribute\n        ValueError\n            If dimension does not exist\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Open 100 years of hourly data\n        &gt;&gt;&gt; ds = DummyDataset.open_mfdataset(\"hourly_*.nc\", concat_dim=\"time\")\n        &gt;&gt;&gt; print(ds.coords['time'].attrs['frequency'])\n        '1H'\n        &gt;&gt;&gt; print(ds.dims['time'])\n        876000\n\n        &gt;&gt;&gt; # Group into decades\n        &gt;&gt;&gt; decades = ds.groupby_time('10Y')\n        &gt;&gt;&gt; print(len(decades))\n        10\n\n        &gt;&gt;&gt; # Each decade has normalized units\n        &gt;&gt;&gt; decade_0 = decades[0]\n        &gt;&gt;&gt; print(decade_0.coords['time'].attrs['units'])\n        'hours since 2000-01-01 00:00:00'\n        &gt;&gt;&gt; print(decade_0.dims['time'])\n        87600\n\n        &gt;&gt;&gt; # Query files for specific decade\n        &gt;&gt;&gt; files = decade_0.get_source_files()\n        &gt;&gt;&gt; print(files)\n        ['hourly_2000.nc', 'hourly_2001.nc', ..., 'hourly_2009.nc']\n\n        See Also\n        --------\n        open_mfdataset : Open multiple files with automatic frequency inference\n        get_source_files : Query which files contain specific coordinate ranges\n        \"\"\"\n        from .mfdataset import groupby_time_impl\n\n        return groupby_time_impl(self, freq, dim, normalize_units)\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset-functions","title":"Functions","text":""},{"location":"api/dataset/#dummyxarray.DummyDataset.__init__","title":"__init__","text":"<pre><code>__init__(_record_history=True)\n</code></pre> <p>Initialize an empty DummyDataset.</p> <p>Parameters:</p> Name Type Description Default <code>_record_history</code> <code>bool</code> <p>Whether to record operation history (default: True)</p> <code>True</code> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def __init__(self, _record_history=True):\n    \"\"\"\n    Initialize an empty DummyDataset.\n\n    Parameters\n    ----------\n    _record_history : bool, optional\n        Whether to record operation history (default: True)\n    \"\"\"\n    self.dims = {}  # dim_name \u2192 size\n    self.coords = {}  # coord_name \u2192 DummyArray\n    self.variables = {}  # var_name  \u2192 DummyArray\n    self.attrs = {}  # global attributes\n\n    # Operation history tracking\n    self._history = [] if _record_history else None\n    if _record_history:\n        self._record_operation(\"__init__\", {})\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Return a string representation similar to xarray.Dataset.</p> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a string representation similar to xarray.Dataset.\"\"\"\n    lines = [\"&lt;dummyxarray.DummyDataset&gt;\"]\n\n    # Dimensions\n    if self.dims:\n        lines.append(\"Dimensions:\")\n        dim_strs = [f\"  {name}: {size}\" for name, size in self.dims.items()]\n        lines.extend(dim_strs)\n    else:\n        lines.append(\"Dimensions: ()\")\n\n    # Coordinates\n    if self.coords:\n        lines.append(\"Coordinates:\")\n        for name, arr in self.coords.items():\n            dims_str = f\"({', '.join(arr.dims)})\" if arr.dims else \"()\"\n            has_data = \"\u2713\" if arr.data is not None else \"\u2717\"\n            dtype_str = f\"{arr.data.dtype}\" if arr.data is not None else \"?\"\n            lines.append(f\"  {has_data} {name:20s} {dims_str:20s} {dtype_str}\")\n\n    # Data variables\n    if self.variables:\n        lines.append(\"Data variables:\")\n        for name, arr in self.variables.items():\n            dims_str = f\"({', '.join(arr.dims)})\" if arr.dims else \"()\"\n            has_data = \"\u2713\" if arr.data is not None else \"\u2717\"\n            dtype_str = f\"{arr.data.dtype}\" if arr.data is not None else \"?\"\n            lines.append(f\"  {has_data} {name:20s} {dims_str:20s} {dtype_str}\")\n\n    # Global attributes\n    if self.attrs:\n        lines.append(\"Attributes:\")\n        for key, value in self.attrs.items():\n            value_str = str(value)\n            if len(value_str) &gt; 50:\n                value_str = value_str[:47] + \"...\"\n            lines.append(f\"    {key}: {value_str}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>Allow attribute-style access to coordinates and variables.</p> <p>This enables xarray-style access like <code>ds.time</code> instead of <code>ds.coords['time']</code>. Coordinates take precedence over variables if both exist with the same name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the coordinate or variable to access</p> required <p>Returns:</p> Type Description <code>DummyArray</code> <p>The coordinate or variable array</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the name is not found in coords or variables</p> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"\n    Allow attribute-style access to coordinates and variables.\n\n    This enables xarray-style access like `ds.time` instead of `ds.coords['time']`.\n    Coordinates take precedence over variables if both exist with the same name.\n\n    Parameters\n    ----------\n    name : str\n        Name of the coordinate or variable to access\n\n    Returns\n    -------\n    DummyArray\n        The coordinate or variable array\n\n    Raises\n    ------\n    AttributeError\n        If the name is not found in coords or variables\n    \"\"\"\n    # Check coordinates first (like xarray does)\n    if name in self.coords:\n        return self.coords[name]\n    # Then check variables\n    if name in self.variables:\n        return self.variables[name]\n    # If not found, raise AttributeError\n    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.__setattr__","title":"__setattr__","text":"<pre><code>__setattr__(name, value)\n</code></pre> <p>Handle attribute assignment.</p> <p>Special handling for internal attributes (dims, coords, variables, attrs). For other names, this could be extended to allow setting coords/variables.</p> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def __setattr__(self, name, value):\n    \"\"\"\n    Handle attribute assignment.\n\n    Special handling for internal attributes (dims, coords, variables, attrs).\n    For other names, this could be extended to allow setting coords/variables.\n    \"\"\"\n    # Internal attributes that should be set normally\n    # Allow private attributes (starting with _) for mixins\n    if name in (\"dims\", \"coords\", \"variables\", \"attrs\", \"_history\") or name.startswith(\"_\"):\n        object.__setattr__(self, name, value)\n    else:\n        # For now, raise an error to avoid confusion\n        # Could be extended to allow ds.time = DummyArray(...) in the future\n        raise AttributeError(\n            f\"Cannot set attribute '{name}' directly. \"\n            f\"Use ds.coords['{name}'] or ds.variables['{name}'] instead.\"\n        )\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre> <p>Customize dir() output to include coordinates and variables.</p> <p>This makes tab-completion work in IPython/Jupyter.</p> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def __dir__(self):\n    \"\"\"\n    Customize dir() output to include coordinates and variables.\n\n    This makes tab-completion work in IPython/Jupyter.\n    \"\"\"\n    # Get default attributes\n    default_attrs = set(object.__dir__(self))\n    # Add coordinate and variable names\n    return sorted(default_attrs | set(self.coords.keys()) | set(self.variables.keys()))\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.set_global_attrs","title":"set_global_attrs","text":"<pre><code>set_global_attrs(**kwargs)\n</code></pre> <p>Set or update global dataset attributes.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Attributes to set</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.set_global_attrs(title=\"My Dataset\", institution=\"DKRZ\")\n</code></pre> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def set_global_attrs(self, **kwargs):\n    \"\"\"\n    Set or update global dataset attributes.\n\n    Parameters\n    ----------\n    **kwargs\n        Attributes to set\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.set_global_attrs(title=\"My Dataset\", institution=\"DKRZ\")\n    \"\"\"\n    self.attrs.update(kwargs)\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.assign_attrs","title":"assign_attrs","text":"<pre><code>assign_attrs(**kwargs)\n</code></pre> <p>Assign new global attributes to this dataset (xarray-compatible API).</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Attributes to assign</p> <code>{}</code> <p>Returns:</p> Type Description <code>self</code> <p>Returns self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.assign_attrs(title=\"My Dataset\", institution=\"DKRZ\")\n</code></pre> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def assign_attrs(self, **kwargs):\n    \"\"\"\n    Assign new global attributes to this dataset (xarray-compatible API).\n\n    Parameters\n    ----------\n    **kwargs\n        Attributes to assign\n\n    Returns\n    -------\n    self\n        Returns self for method chaining\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.assign_attrs(title=\"My Dataset\", institution=\"DKRZ\")\n    \"\"\"\n    # Capture provenance\n    provenance = {\"modified\": {}}\n    for key, value in kwargs.items():\n        old_value = self.attrs.get(key)\n        provenance[\"modified\"][key] = {\"before\": old_value, \"after\": value}\n\n    self._record_operation(\"assign_attrs\", kwargs, provenance)\n    self.attrs.update(kwargs)\n    return self\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.add_dim","title":"add_dim","text":"<pre><code>add_dim(name, size)\n</code></pre> <p>Add a dimension with a specific size.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dimension name</p> required <code>size</code> <code>int</code> <p>Dimension size</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 10)\n&gt;&gt;&gt; ds.add_dim(\"lat\", 64)\n</code></pre> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def add_dim(self, name, size):\n    \"\"\"\n    Add a dimension with a specific size.\n\n    Parameters\n    ----------\n    name : str\n        Dimension name\n    size : int\n        Dimension size\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n    &gt;&gt;&gt; ds.add_dim(\"lat\", 64)\n    \"\"\"\n    # Capture provenance\n    if name in self.dims:\n        provenance = {\"modified\": {name: {\"before\": self.dims[name], \"after\": size}}}\n    else:\n        provenance = {\"added\": [name]}\n\n    self._record_operation(\"add_dim\", {\"name\": name, \"size\": size}, provenance)\n    self.dims[name] = size\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.add_coord","title":"add_coord","text":"<pre><code>add_coord(\n    name, dims=None, attrs=None, data=None, encoding=None\n)\n</code></pre> <p>Add a coordinate variable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Coordinate name</p> required <code>dims</code> <code>list of str</code> <p>Dimension names</p> <code>None</code> <code>attrs</code> <code>dict</code> <p>Metadata attributes</p> <code>None</code> <code>data</code> <code>array - like</code> <p>Coordinate data</p> <code>None</code> <code>encoding</code> <code>dict</code> <p>Encoding parameters</p> <code>None</code> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def add_coord(self, name, dims=None, attrs=None, data=None, encoding=None):\n    \"\"\"\n    Add a coordinate variable.\n\n    Parameters\n    ----------\n    name : str\n        Coordinate name\n    dims : list of str, optional\n        Dimension names\n    attrs : dict, optional\n        Metadata attributes\n    data : array-like, optional\n        Coordinate data\n    encoding : dict, optional\n        Encoding parameters\n    \"\"\"\n    # Record operation (don't store actual data)\n    args = {\"name\": name}\n    if dims is not None:\n        args[\"dims\"] = dims\n    if attrs:\n        args[\"attrs\"] = attrs\n    if data is not None:\n        args[\"data\"] = \"&lt;data&gt;\"\n    if encoding:\n        args[\"encoding\"] = encoding\n\n    # Capture provenance\n    provenance = {}\n    if name in self.coords:\n        # Coordinate already exists - track what changed\n        old_coord = self.coords[name]\n        changes = {}\n        if dims != old_coord.dims:\n            changes[\"dims\"] = {\"before\": old_coord.dims, \"after\": dims}\n        if attrs and attrs != old_coord.attrs:\n            changes[\"attrs\"] = {\"before\": old_coord.attrs.copy(), \"after\": attrs}\n        if changes:\n            provenance[\"modified\"] = {name: changes}\n    else:\n        provenance[\"added\"] = [name]\n\n    self._record_operation(\"add_coord\", args, provenance)\n\n    arr = DummyArray(dims, attrs, data, encoding, _record_history=False)\n    self._infer_and_register_dims(arr)\n    self.coords[name] = arr\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.add_variable","title":"add_variable","text":"<pre><code>add_variable(\n    name, dims=None, attrs=None, data=None, encoding=None\n)\n</code></pre> <p>Add a data variable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Variable name</p> required <code>dims</code> <code>list of str</code> <p>Dimension names</p> <code>None</code> <code>attrs</code> <code>dict</code> <p>Metadata attributes</p> <code>None</code> <code>data</code> <code>array - like</code> <p>Variable data</p> <code>None</code> <code>encoding</code> <code>dict</code> <p>Encoding parameters</p> <code>None</code> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def add_variable(self, name, dims=None, attrs=None, data=None, encoding=None):\n    \"\"\"\n    Add a data variable.\n\n    Parameters\n    ----------\n    name : str\n        Variable name\n    dims : list of str, optional\n        Dimension names\n    attrs : dict, optional\n        Metadata attributes\n    data : array-like, optional\n        Variable data\n    encoding : dict, optional\n        Encoding parameters\n    \"\"\"\n    # Record operation (don't store actual data)\n    args = {\"name\": name}\n    if dims is not None:\n        args[\"dims\"] = dims\n    if attrs:\n        args[\"attrs\"] = attrs\n    if data is not None:\n        args[\"data\"] = \"&lt;data&gt;\"\n    if encoding:\n        args[\"encoding\"] = encoding\n\n    # Capture provenance\n    provenance = {}\n    if name in self.variables:\n        # Variable already exists - track what changed\n        old_var = self.variables[name]\n        changes = {}\n        if dims != old_var.dims:\n            changes[\"dims\"] = {\"before\": old_var.dims, \"after\": dims}\n        if attrs and attrs != old_var.attrs:\n            changes[\"attrs\"] = {\"before\": old_var.attrs.copy(), \"after\": attrs}\n        if changes:\n            provenance[\"modified\"] = {name: changes}\n    else:\n        provenance[\"added\"] = [name]\n\n    self._record_operation(\"add_variable\", args, provenance)\n\n    arr = DummyArray(dims, attrs, data, encoding, _record_history=False)\n    self._infer_and_register_dims(arr)\n    self.variables[name] = arr\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.rename_dims","title":"rename_dims","text":"<pre><code>rename_dims(dims_dict=None, **dims)\n</code></pre> <p>Rename dimensions (xarray-compatible API).</p> <p>Parameters:</p> Name Type Description Default <code>dims_dict</code> <code>dict - like</code> <p>Dictionary whose keys are current dimension names and whose values are the desired names.</p> <code>None</code> <code>**dims</code> <code>optional</code> <p>Keyword form of dims_dict. One of dims_dict or dims must be provided.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DummyDataset</code> <p>Returns self for method chaining</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a dimension doesn't exist</p> <code>ValueError</code> <p>If a new name already exists</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 10)\n&gt;&gt;&gt; ds.add_dim(\"lat\", 64)\n&gt;&gt;&gt; ds.rename_dims({\"time\": \"t\", \"lat\": \"latitude\"})\n&gt;&gt;&gt; # Or using keyword arguments:\n&gt;&gt;&gt; ds.rename_dims(time=\"t\", lat=\"latitude\")\n</code></pre> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def rename_dims(self, dims_dict=None, **dims):\n    \"\"\"\n    Rename dimensions (xarray-compatible API).\n\n    Parameters\n    ----------\n    dims_dict : dict-like, optional\n        Dictionary whose keys are current dimension names and whose\n        values are the desired names.\n    **dims : optional\n        Keyword form of dims_dict.\n        One of dims_dict or dims must be provided.\n\n    Returns\n    -------\n    DummyDataset\n        Returns self for method chaining\n\n    Raises\n    ------\n    KeyError\n        If a dimension doesn't exist\n    ValueError\n        If a new name already exists\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n    &gt;&gt;&gt; ds.add_dim(\"lat\", 64)\n    &gt;&gt;&gt; ds.rename_dims({\"time\": \"t\", \"lat\": \"latitude\"})\n    &gt;&gt;&gt; # Or using keyword arguments:\n    &gt;&gt;&gt; ds.rename_dims(time=\"t\", lat=\"latitude\")\n    \"\"\"\n    # Merge dims_dict and **dims\n    name_dict = {}\n    if dims_dict is not None:\n        name_dict.update(dims_dict)\n    name_dict.update(dims)\n\n    if not name_dict:\n        raise ValueError(\"Either dims_dict or keyword arguments must be provided\")\n\n    # Validate all renames first\n    for old_name, new_name in name_dict.items():\n        if old_name not in self.dims:\n            raise KeyError(f\"Dimension '{old_name}' does not exist\")\n        if new_name in self.dims and new_name != old_name:\n            raise ValueError(f\"Dimension '{new_name}' already exists\")\n\n    # Capture provenance\n    provenance = {\n        \"renamed\": name_dict.copy(),\n        \"removed\": list(name_dict.keys()),\n        \"added\": list(name_dict.values()),\n    }\n\n    self._record_operation(\"rename_dims\", {\"dims_dict\": name_dict}, provenance)\n\n    # Perform all renames\n    for old_name, new_name in name_dict.items():\n        if old_name != new_name:\n            self.dims[new_name] = self.dims.pop(old_name)\n\n            # Update dimension references in coords and variables\n            for coord in self.coords.values():\n                if coord.dims:\n                    coord.dims = [new_name if d == old_name else d for d in coord.dims]\n            for var in self.variables.values():\n                if var.dims:\n                    var.dims = [new_name if d == old_name else d for d in var.dims]\n\n    return self\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.rename_vars","title":"rename_vars","text":"<pre><code>rename_vars(name_dict=None, **names)\n</code></pre> <p>Rename variables (xarray-compatible API).</p> <p>Parameters:</p> Name Type Description Default <code>name_dict</code> <code>dict - like</code> <p>Dictionary whose keys are current variable names and whose values are the desired names.</p> <code>None</code> <code>**names</code> <code>optional</code> <p>Keyword form of name_dict. One of name_dict or names must be provided.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DummyDataset</code> <p>Returns self for method chaining</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a variable doesn't exist</p> <code>ValueError</code> <p>If a new name already exists</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 10)\n&gt;&gt;&gt; ds.add_variable(\"temperature\", dims=[\"time\"])\n&gt;&gt;&gt; ds.rename_vars({\"temperature\": \"temp\"})\n&gt;&gt;&gt; # Or using keyword arguments:\n&gt;&gt;&gt; ds.rename_vars(temperature=\"temp\")\n</code></pre> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def rename_vars(self, name_dict=None, **names):\n    \"\"\"\n    Rename variables (xarray-compatible API).\n\n    Parameters\n    ----------\n    name_dict : dict-like, optional\n        Dictionary whose keys are current variable names and whose\n        values are the desired names.\n    **names : optional\n        Keyword form of name_dict.\n        One of name_dict or names must be provided.\n\n    Returns\n    -------\n    DummyDataset\n        Returns self for method chaining\n\n    Raises\n    ------\n    KeyError\n        If a variable doesn't exist\n    ValueError\n        If a new name already exists\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n    &gt;&gt;&gt; ds.add_variable(\"temperature\", dims=[\"time\"])\n    &gt;&gt;&gt; ds.rename_vars({\"temperature\": \"temp\"})\n    &gt;&gt;&gt; # Or using keyword arguments:\n    &gt;&gt;&gt; ds.rename_vars(temperature=\"temp\")\n    \"\"\"\n    # Merge name_dict and **names\n    rename_dict = {}\n    if name_dict is not None:\n        rename_dict.update(name_dict)\n    rename_dict.update(names)\n\n    if not rename_dict:\n        raise ValueError(\"Either name_dict or keyword arguments must be provided\")\n\n    # Validate all renames first\n    for old_name, new_name in rename_dict.items():\n        if old_name not in self.variables:\n            raise KeyError(f\"Variable '{old_name}' does not exist\")\n        if new_name in self.variables and new_name != old_name:\n            raise ValueError(f\"Variable '{new_name}' already exists\")\n\n    # Capture provenance\n    provenance = {\n        \"renamed\": rename_dict.copy(),\n        \"removed\": list(rename_dict.keys()),\n        \"added\": list(rename_dict.values()),\n    }\n\n    self._record_operation(\"rename_vars\", {\"name_dict\": rename_dict}, provenance)\n\n    # Perform all renames\n    for old_name, new_name in rename_dict.items():\n        if old_name != new_name:\n            self.variables[new_name] = self.variables.pop(old_name)\n\n    return self\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.rename","title":"rename","text":"<pre><code>rename(name_dict=None, **names)\n</code></pre> <p>Rename variables, coordinates, and dimensions (xarray-compatible API).</p> <p>This method can rename any combination of variables, coordinates, and dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>name_dict</code> <code>dict - like</code> <p>Dictionary whose keys are current names (variables, coordinates, or dimensions) and whose values are the desired names.</p> <code>None</code> <code>**names</code> <code>optional</code> <p>Keyword form of name_dict. One of name_dict or names must be provided.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DummyDataset</code> <p>Returns self for method chaining</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a name doesn't exist</p> <code>ValueError</code> <p>If a new name already exists</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 10)\n&gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"])\n&gt;&gt;&gt; ds.add_variable(\"temperature\", dims=[\"time\"])\n&gt;&gt;&gt; # Rename multiple items at once\n&gt;&gt;&gt; ds.rename({\"time\": \"t\", \"temperature\": \"temp\"})\n&gt;&gt;&gt; # Or using keyword arguments:\n&gt;&gt;&gt; ds.rename(time=\"t\", temperature=\"temp\")\n</code></pre> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def rename(self, name_dict=None, **names):\n    \"\"\"\n    Rename variables, coordinates, and dimensions (xarray-compatible API).\n\n    This method can rename any combination of variables, coordinates, and dimensions.\n\n    Parameters\n    ----------\n    name_dict : dict-like, optional\n        Dictionary whose keys are current names (variables, coordinates, or dimensions)\n        and whose values are the desired names.\n    **names : optional\n        Keyword form of name_dict.\n        One of name_dict or names must be provided.\n\n    Returns\n    -------\n    DummyDataset\n        Returns self for method chaining\n\n    Raises\n    ------\n    KeyError\n        If a name doesn't exist\n    ValueError\n        If a new name already exists\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n    &gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"])\n    &gt;&gt;&gt; ds.add_variable(\"temperature\", dims=[\"time\"])\n    &gt;&gt;&gt; # Rename multiple items at once\n    &gt;&gt;&gt; ds.rename({\"time\": \"t\", \"temperature\": \"temp\"})\n    &gt;&gt;&gt; # Or using keyword arguments:\n    &gt;&gt;&gt; ds.rename(time=\"t\", temperature=\"temp\")\n    \"\"\"\n    # Merge name_dict and **names\n    rename_dict = {}\n    if name_dict is not None:\n        rename_dict.update(name_dict)\n    rename_dict.update(names)\n\n    if not rename_dict:\n        raise ValueError(\"Either name_dict or keyword arguments must be provided\")\n\n    # Categorize renames\n    dim_renames = {}\n    coord_renames = {}\n    var_renames = {}\n\n    for old_name, new_name in rename_dict.items():\n        if old_name in self.dims:\n            dim_renames[old_name] = new_name\n        if old_name in self.coords:\n            coord_renames[old_name] = new_name\n        if old_name in self.variables:\n            var_renames[old_name] = new_name\n\n        # Check if name exists anywhere\n        if (\n            old_name not in self.dims\n            and old_name not in self.coords\n            and old_name not in self.variables\n        ):\n            raise KeyError(\n                f\"'{old_name}' does not exist in dimensions, coordinates, or variables\"\n            )\n\n    # Capture provenance\n    provenance = {\n        \"renamed\": rename_dict.copy(),\n        \"removed\": list(rename_dict.keys()),\n        \"added\": list(rename_dict.values()),\n    }\n\n    self._record_operation(\"rename\", {\"name_dict\": rename_dict}, provenance)\n\n    # Perform renames in order: dimensions first (affects coords/vars), then coords, then vars\n    if dim_renames:\n        for old_name, new_name in dim_renames.items():\n            if old_name != new_name and old_name in self.dims:\n                self.dims[new_name] = self.dims.pop(old_name)\n                # Update dimension references\n                for coord in self.coords.values():\n                    if coord.dims:\n                        coord.dims = [new_name if d == old_name else d for d in coord.dims]\n                for var in self.variables.values():\n                    if var.dims:\n                        var.dims = [new_name if d == old_name else d for d in var.dims]\n\n    if coord_renames:\n        for old_name, new_name in coord_renames.items():\n            if old_name != new_name and old_name in self.coords:\n                self.coords[new_name] = self.coords.pop(old_name)\n\n    if var_renames:\n        for old_name, new_name in var_renames.items():\n            if old_name != new_name and old_name in self.variables:\n                self.variables[new_name] = self.variables.pop(old_name)\n\n    return self\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.open_mfdataset","title":"open_mfdataset  <code>classmethod</code>","text":"<pre><code>open_mfdataset(\n    paths, concat_dim=\"time\", combine=\"nested\", **kwargs\n)\n</code></pre> <p>Open multiple files as a single DummyDataset with file tracking.</p> <p>This class method reads metadata from multiple NetCDF files and combines them into a single DummyDataset, tracking which files contribute to which coordinate ranges along the concatenation dimension.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>str or list of str</code> <p>Either a glob pattern (e.g., \"data/*.nc\") or a list of file paths</p> required <code>concat_dim</code> <code>str</code> <p>The dimension along which to concatenate files (default: \"time\")</p> <code>'time'</code> <code>combine</code> <code>str</code> <p>How to combine datasets. Currently supports \"nested\" (default)</p> <code>'nested'</code> <code>**kwargs</code> <code>optional</code> <p>Additional keyword arguments (reserved for future use)</p> <code>{}</code> <p>Returns:</p> Type Description <code>DummyDataset</code> <p>A DummyDataset with metadata from all files and file tracking enabled</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset.open_mfdataset(\"data/*.nc\", concat_dim=\"time\")\n&gt;&gt;&gt; files = ds.get_source_files(time=slice(0, 10))\n&gt;&gt;&gt; print(files)\n['data/file1.nc', 'data/file2.nc']\n</code></pre> See Also <p>enable_file_tracking : Enable file tracking on an existing dataset get_source_files : Query which files contain specific coordinate ranges</p> Source code in <code>src/dummyxarray/core.py</code> <pre><code>@classmethod\ndef open_mfdataset(cls, paths, concat_dim=\"time\", combine=\"nested\", **kwargs):\n    \"\"\"Open multiple files as a single DummyDataset with file tracking.\n\n    This class method reads metadata from multiple NetCDF files and combines them\n    into a single DummyDataset, tracking which files contribute to which\n    coordinate ranges along the concatenation dimension.\n\n    Parameters\n    ----------\n    paths : str or list of str\n        Either a glob pattern (e.g., \"data/*.nc\") or a list of file paths\n    concat_dim : str, optional\n        The dimension along which to concatenate files (default: \"time\")\n    combine : str, optional\n        How to combine datasets. Currently supports \"nested\" (default)\n    **kwargs : optional\n        Additional keyword arguments (reserved for future use)\n\n    Returns\n    -------\n    DummyDataset\n        A DummyDataset with metadata from all files and file tracking enabled\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset.open_mfdataset(\"data/*.nc\", concat_dim=\"time\")\n    &gt;&gt;&gt; files = ds.get_source_files(time=slice(0, 10))\n    &gt;&gt;&gt; print(files)\n    ['data/file1.nc', 'data/file2.nc']\n\n    See Also\n    --------\n    enable_file_tracking : Enable file tracking on an existing dataset\n    get_source_files : Query which files contain specific coordinate ranges\n    \"\"\"\n    from .mfdataset import open_mfdataset\n\n    return open_mfdataset(paths, concat_dim=concat_dim, combine=combine, **kwargs)\n</code></pre>"},{"location":"api/dataset/#dummyxarray.DummyDataset.groupby_time","title":"groupby_time","text":"<pre><code>groupby_time(\n    freq: str,\n    dim: str = \"time\",\n    normalize_units: bool = True,\n) -&gt; List[DummyDataset]\n</code></pre> <p>Group dataset by time frequency using metadata only.</p> <p>This method splits a multi-file dataset into time-based groups without loading any data arrays. Each group is a new DummyDataset with adjusted metadata.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>str</code> <p>Grouping frequency using pandas-style strings: - Years: '1Y', '5Y', '10Y' - Months: '1M', '3M', '6M' - Days: '1D', '7D', '30D' - Hours: '1H', '6H', '12H'</p> required <code>dim</code> <code>str</code> <p>Time dimension to group by</p> <code>\"time\"</code> <code>normalize_units</code> <code>bool</code> <p>Update time units to reference each group's start datetime</p> <code>True</code> <p>Returns:</p> Type Description <code>list of DummyDataset</code> <p>One dataset per time group, each with: - Updated time:units attribute (if normalize_units=True) - Filtered file sources for that time period - Adjusted dimension sizes - Preserved frequency attribute</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If time coordinate has no frequency attribute (open with open_mfdataset)</p> <code>ValueError</code> <p>If time coordinate has no units attribute</p> <code>ValueError</code> <p>If dimension does not exist</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Open 100 years of hourly data\n&gt;&gt;&gt; ds = DummyDataset.open_mfdataset(\"hourly_*.nc\", concat_dim=\"time\")\n&gt;&gt;&gt; print(ds.coords['time'].attrs['frequency'])\n'1H'\n&gt;&gt;&gt; print(ds.dims['time'])\n876000\n</code></pre> <pre><code>&gt;&gt;&gt; # Group into decades\n&gt;&gt;&gt; decades = ds.groupby_time('10Y')\n&gt;&gt;&gt; print(len(decades))\n10\n</code></pre> <pre><code>&gt;&gt;&gt; # Each decade has normalized units\n&gt;&gt;&gt; decade_0 = decades[0]\n&gt;&gt;&gt; print(decade_0.coords['time'].attrs['units'])\n'hours since 2000-01-01 00:00:00'\n&gt;&gt;&gt; print(decade_0.dims['time'])\n87600\n</code></pre> <pre><code>&gt;&gt;&gt; # Query files for specific decade\n&gt;&gt;&gt; files = decade_0.get_source_files()\n&gt;&gt;&gt; print(files)\n['hourly_2000.nc', 'hourly_2001.nc', ..., 'hourly_2009.nc']\n</code></pre> See Also <p>open_mfdataset : Open multiple files with automatic frequency inference get_source_files : Query which files contain specific coordinate ranges</p> Source code in <code>src/dummyxarray/core.py</code> <pre><code>def groupby_time(\n    self,\n    freq: str,\n    dim: str = \"time\",\n    normalize_units: bool = True,\n) -&gt; List[\"DummyDataset\"]:\n    \"\"\"Group dataset by time frequency using metadata only.\n\n    This method splits a multi-file dataset into time-based groups without loading\n    any data arrays. Each group is a new DummyDataset with adjusted metadata.\n\n    Parameters\n    ----------\n    freq : str\n        Grouping frequency using pandas-style strings:\n        - Years: '1Y', '5Y', '10Y'\n        - Months: '1M', '3M', '6M'\n        - Days: '1D', '7D', '30D'\n        - Hours: '1H', '6H', '12H'\n    dim : str, default \"time\"\n        Time dimension to group by\n    normalize_units : bool, default True\n        Update time units to reference each group's start datetime\n\n    Returns\n    -------\n    list of DummyDataset\n        One dataset per time group, each with:\n        - Updated time:units attribute (if normalize_units=True)\n        - Filtered file sources for that time period\n        - Adjusted dimension sizes\n        - Preserved frequency attribute\n\n    Raises\n    ------\n    ValueError\n        If time coordinate has no frequency attribute (open with open_mfdataset)\n    ValueError\n        If time coordinate has no units attribute\n    ValueError\n        If dimension does not exist\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Open 100 years of hourly data\n    &gt;&gt;&gt; ds = DummyDataset.open_mfdataset(\"hourly_*.nc\", concat_dim=\"time\")\n    &gt;&gt;&gt; print(ds.coords['time'].attrs['frequency'])\n    '1H'\n    &gt;&gt;&gt; print(ds.dims['time'])\n    876000\n\n    &gt;&gt;&gt; # Group into decades\n    &gt;&gt;&gt; decades = ds.groupby_time('10Y')\n    &gt;&gt;&gt; print(len(decades))\n    10\n\n    &gt;&gt;&gt; # Each decade has normalized units\n    &gt;&gt;&gt; decade_0 = decades[0]\n    &gt;&gt;&gt; print(decade_0.coords['time'].attrs['units'])\n    'hours since 2000-01-01 00:00:00'\n    &gt;&gt;&gt; print(decade_0.dims['time'])\n    87600\n\n    &gt;&gt;&gt; # Query files for specific decade\n    &gt;&gt;&gt; files = decade_0.get_source_files()\n    &gt;&gt;&gt; print(files)\n    ['hourly_2000.nc', 'hourly_2001.nc', ..., 'hourly_2009.nc']\n\n    See Also\n    --------\n    open_mfdataset : Open multiple files with automatic frequency inference\n    get_source_files : Query which files contain specific coordinate ranges\n    \"\"\"\n    from .mfdataset import groupby_time_impl\n\n    return groupby_time_impl(self, freq, dim, normalize_units)\n</code></pre>"},{"location":"api/mixins/cf-compliance/","title":"CFComplianceMixin","text":"<p>Provides CF (Climate and Forecast) convention support.</p>"},{"location":"api/mixins/cf-compliance/#overview","title":"Overview","text":"<p>The <code>CFComplianceMixin</code> helps create CF-compliant datasets by:</p> <ul> <li>Detecting axis types - Automatically infer X, Y, Z, T axes</li> <li>Validating metadata - Check for CF compliance issues</li> <li>Setting attributes - Apply CF-standard axis attributes</li> <li>Querying coordinates - Find coordinates by axis type</li> </ul>"},{"location":"api/mixins/cf-compliance/#key-methods","title":"Key Methods","text":"<ul> <li><code>infer_axis()</code> - Detect axis types for all coordinates</li> <li><code>set_axis_attributes()</code> - Set axis attributes on coordinates</li> <li><code>get_axis_coordinates(axis)</code> - Get coordinates for specific axis</li> <li><code>validate_cf(strict=False)</code> - Validate CF compliance</li> </ul>"},{"location":"api/mixins/cf-compliance/#axis-detection","title":"Axis Detection","text":"<p>Axes are detected based on:</p> <ul> <li>Coordinate names - time, lat, lon, lev, etc.</li> <li>Units - degrees_north, days since, hPa, etc.</li> <li>Standard names - latitude, longitude, time, etc.</li> </ul>"},{"location":"api/mixins/cf-compliance/#usage","title":"Usage","text":"<p>See the CF Compliance Guide for detailed examples.</p>"},{"location":"api/mixins/cf-compliance/#api-reference","title":"API Reference","text":"<p>Mixin providing CF compliance and axis detection capabilities.</p> Source code in <code>src/dummyxarray/cf_compliance.py</code> <pre><code>class CFComplianceMixin:\n    \"\"\"Mixin providing CF compliance and axis detection capabilities.\"\"\"\n\n    def infer_axis(self, coord_name=None):\n        \"\"\"\n        Infer axis attribute (X/Y/Z/T) for coordinates based on CF conventions.\n\n        Uses coordinate names, standard_name attributes, units, and dimension\n        patterns to automatically detect axis types.\n\n        Parameters\n        ----------\n        coord_name : str, optional\n            Specific coordinate to infer axis for. If None, infers for all coordinates.\n\n        Returns\n        -------\n        dict\n            Dictionary mapping coordinate names to inferred axis values ('X', 'Y', 'Z', 'T')\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n        &gt;&gt;&gt; ds.add_dim(\"lat\", 64)\n        &gt;&gt;&gt; ds.add_dim(\"lon\", 128)\n        &gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\n        &gt;&gt;&gt; ds.add_coord(\"lat\", dims=[\"lat\"], attrs={\"units\": \"degrees_north\"})\n        &gt;&gt;&gt; ds.add_coord(\"lon\", dims=[\"lon\"], attrs={\"units\": \"degrees_east\"})\n        &gt;&gt;&gt; axes = ds.infer_axis()\n        &gt;&gt;&gt; # Returns: {'time': 'T', 'lat': 'Y', 'lon': 'X'}\n        \"\"\"\n        axes = {}\n        coords_to_check = [coord_name] if coord_name else list(self.coords.keys())\n\n        for name in coords_to_check:\n            if name not in self.coords:\n                continue\n\n            coord = self.coords[name]\n            axis = self._detect_axis_type(name, coord)\n            if axis:\n                axes[name] = axis\n\n        return axes\n\n    def _detect_axis_type(self, name, coord):\n        \"\"\"\n        Detect axis type for a coordinate based on CF conventions.\n\n        Parameters\n        ----------\n        name : str\n            Coordinate name\n        coord : DummyArray\n            Coordinate object\n\n        Returns\n        -------\n        str or None\n            Axis type ('X', 'Y', 'Z', 'T') or None if cannot be determined\n        \"\"\"\n        # Check if axis already set\n        if coord.attrs.get(\"axis\"):\n            return coord.attrs[\"axis\"]\n\n        # Check standard_name (CF convention)\n        standard_name = coord.attrs.get(\"standard_name\", \"\").lower()\n        standard_name_map = {\n            \"longitude\": \"X\",\n            \"projection_x_coordinate\": \"X\",\n            \"grid_longitude\": \"X\",\n            \"latitude\": \"Y\",\n            \"projection_y_coordinate\": \"Y\",\n            \"grid_latitude\": \"Y\",\n            \"altitude\": \"Z\",\n            \"height\": \"Z\",\n            \"depth\": \"Z\",\n            \"air_pressure\": \"Z\",\n            \"model_level_number\": \"Z\",\n            \"time\": \"T\",\n        }\n        if standard_name in standard_name_map:\n            return standard_name_map[standard_name]\n\n        # Check units (CF convention)\n        units = coord.attrs.get(\"units\", \"\").lower()\n\n        # Time axis patterns\n        time_patterns = [\"since\", \"days\", \"hours\", \"minutes\", \"seconds\"]\n        if any(pattern in units for pattern in time_patterns):\n            return \"T\"\n\n        # Longitude patterns\n        if units in [\"degrees_east\", \"degree_east\", \"degreee\", \"degreese\"]:\n            return \"X\"\n\n        # Latitude patterns\n        if units in [\"degrees_north\", \"degree_north\", \"degreen\", \"degreesn\"]:\n            return \"Y\"\n\n        # Vertical coordinate patterns\n        vertical_units = [\"pa\", \"hpa\", \"mbar\", \"bar\", \"m\", \"km\", \"level\", \"sigma\", \"eta\"]\n        if any(units.startswith(u) for u in vertical_units):\n            return \"Z\"\n\n        # Check coordinate name patterns (common conventions)\n        name_lower = name.lower()\n\n        # X-axis patterns\n        x_patterns = [\"lon\", \"longitude\", \"x\", \"i\", \"ni\", \"xc\"]\n        if any(name_lower.startswith(p) or name_lower == p for p in x_patterns):\n            return \"X\"\n\n        # Y-axis patterns\n        y_patterns = [\"lat\", \"latitude\", \"y\", \"j\", \"nj\", \"yc\"]\n        if any(name_lower.startswith(p) or name_lower == p for p in y_patterns):\n            return \"Y\"\n\n        # Z-axis patterns\n        z_patterns = [\"lev\", \"level\", \"plev\", \"height\", \"depth\", \"alt\", \"z\", \"k\", \"nk\"]\n        if any(name_lower.startswith(p) or name_lower == p for p in z_patterns):\n            return \"Z\"\n\n        # T-axis patterns\n        t_patterns = [\"time\", \"t\", \"date\"]\n        if any(name_lower.startswith(p) or name_lower == p for p in t_patterns):\n            return \"T\"\n\n        return None\n\n    def set_axis_attributes(self, inferred_only=False):\n        \"\"\"\n        Set axis attributes on coordinates based on inferred axis types.\n\n        This modifies coordinate attributes in-place to add 'axis' attributes\n        following CF conventions.\n\n        Parameters\n        ----------\n        inferred_only : bool, default False\n            If True, only set axis for coordinates that don't already have one.\n            If False, overwrite existing axis attributes with inferred values.\n\n        Returns\n        -------\n        dict\n            Dictionary of coordinate names and their assigned axis values\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n        &gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\n        &gt;&gt;&gt; ds.set_axis_attributes()\n        &gt;&gt;&gt; print(ds.coords[\"time\"].attrs[\"axis\"])\n        T\n        \"\"\"\n        assigned = {}\n\n        for coord_name, coord in self.coords.items():\n            # Check if we should skip this coordinate\n            if inferred_only and \"axis\" in coord.attrs:\n                continue\n\n            # Temporarily remove axis attribute to force re-inference\n            existing_axis = coord.attrs.pop(\"axis\", None)\n\n            # Infer axis for this coordinate\n            axis = self._detect_axis_type(coord_name, coord)\n\n            if axis:\n                # Set the inferred axis attribute\n                coord.attrs[\"axis\"] = axis\n                assigned[coord_name] = axis\n            elif existing_axis:\n                # Restore existing axis if we couldn't infer a new one\n                coord.attrs[\"axis\"] = existing_axis\n\n        return assigned\n\n    def get_axis_coordinates(self, axis):\n        \"\"\"\n        Get all coordinates with a specific axis attribute.\n\n        Parameters\n        ----------\n        axis : str\n            Axis type to search for ('X', 'Y', 'Z', 'T')\n\n        Returns\n        -------\n        list\n            List of coordinate names with the specified axis\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_coord(\"lon\", dims=[\"lon\"], attrs={\"axis\": \"X\"})\n        &gt;&gt;&gt; ds.add_coord(\"lat\", dims=[\"lat\"], attrs={\"axis\": \"Y\"})\n        &gt;&gt;&gt; x_coords = ds.get_axis_coordinates(\"X\")\n        &gt;&gt;&gt; # Returns: ['lon']\n        \"\"\"\n        coords = []\n        for name, coord in self.coords.items():\n            if coord.attrs.get(\"axis\") == axis:\n                coords.append(name)\n        return coords\n\n    def validate_cf(self, strict=False):\n        \"\"\"\n        Validate dataset against CF conventions.\n\n        Checks for common CF compliance issues like missing axis attributes,\n        invalid units, missing standard_name, etc.\n\n        Parameters\n        ----------\n        strict : bool, default False\n            If True, raise ValueError on any CF violation.\n            If False, return a list of warnings/errors.\n\n        Returns\n        -------\n        dict\n            Dictionary with 'errors' and 'warnings' lists\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n        &gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"])\n        &gt;&gt;&gt; result = ds.validate_cf()\n        &gt;&gt;&gt; print(result['warnings'])\n        ['time: Missing axis attribute', 'time: Missing units attribute']\n        \"\"\"\n        errors = []\n        warnings = []\n\n        # Check coordinates for axis attributes\n        for name, coord in self.coords.items():\n            # Check for axis attribute\n            if \"axis\" not in coord.attrs:\n                inferred = self.infer_axis(name)\n                if name in inferred:\n                    warnings.append(\n                        f\"{name}: Missing 'axis' attribute (can be inferred as '{inferred[name]}')\"\n                    )\n                else:\n                    warnings.append(f\"{name}: Missing 'axis' attribute (cannot infer)\")\n\n            # Check for units\n            if \"units\" not in coord.attrs:\n                warnings.append(f\"{name}: Missing 'units' attribute\")\n\n            # Check for standard_name on coordinates\n            if \"standard_name\" not in coord.attrs:\n                warnings.append(f\"{name}: Missing 'standard_name' attribute\")\n\n        # Check variables for required attributes\n        for name, var in self.variables.items():\n            # Check for units\n            if \"units\" not in var.attrs:\n                warnings.append(f\"{name}: Variable missing 'units' attribute\")\n\n            # Check for long_name or standard_name\n            if \"long_name\" not in var.attrs and \"standard_name\" not in var.attrs:\n                warnings.append(f\"{name}: Variable missing both 'long_name' and 'standard_name'\")\n\n        # Check global attributes\n        required_global = [\"Conventions\"]\n        for attr in required_global:\n            if attr not in self.attrs:\n                warnings.append(f\"Missing required global attribute: '{attr}'\")\n\n        # Check for CF Conventions version\n        if \"Conventions\" in self.attrs:\n            conventions = self.attrs[\"Conventions\"]\n            if not any(cf in conventions for cf in [\"CF-\", \"cf-\"]):\n                warnings.append(f\"Conventions attribute '{conventions}' does not reference CF\")\n\n        # Check dimension ordering (CF recommends T, Z, Y, X)\n        for name, var in self.variables.items():\n            if var.dims and len(var.dims) &gt; 1:\n                # Get axis types for dimensions\n                dim_axes = []\n                for dim in var.dims:\n                    if dim in self.coords:\n                        axis = self.coords[dim].attrs.get(\"axis\")\n                        if axis:\n                            dim_axes.append(axis)\n\n                # Check if order is T, Z, Y, X\n                expected_order = [\"T\", \"Z\", \"Y\", \"X\"]\n                actual_order = [a for a in dim_axes if a in expected_order]\n                sorted_order = sorted(actual_order, key=lambda x: expected_order.index(x))\n\n                if actual_order != sorted_order:\n                    warnings.append(\n                        f\"{name}: Dimension order {actual_order} does not follow \"\n                        f\"CF recommendation (T, Z, Y, X)\"\n                    )\n\n        result = {\"errors\": errors, \"warnings\": warnings}\n\n        if strict and (errors or warnings):\n            all_issues = errors + warnings\n            raise ValueError(\"CF validation failed:\\n\" + \"\\n\".join(all_issues))\n\n        return result\n</code></pre>"},{"location":"api/mixins/cf-compliance/#dummyxarray.cf_compliance.CFComplianceMixin.infer_axis","title":"infer_axis","text":"<pre><code>infer_axis(coord_name=None)\n</code></pre> <p>Infer axis attribute (X/Y/Z/T) for coordinates based on CF conventions.</p> <p>Uses coordinate names, standard_name attributes, units, and dimension patterns to automatically detect axis types.</p> <p>Parameters:</p> Name Type Description Default <code>coord_name</code> <code>str</code> <p>Specific coordinate to infer axis for. If None, infers for all coordinates.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary mapping coordinate names to inferred axis values ('X', 'Y', 'Z', 'T')</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 10)\n&gt;&gt;&gt; ds.add_dim(\"lat\", 64)\n&gt;&gt;&gt; ds.add_dim(\"lon\", 128)\n&gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\n&gt;&gt;&gt; ds.add_coord(\"lat\", dims=[\"lat\"], attrs={\"units\": \"degrees_north\"})\n&gt;&gt;&gt; ds.add_coord(\"lon\", dims=[\"lon\"], attrs={\"units\": \"degrees_east\"})\n&gt;&gt;&gt; axes = ds.infer_axis()\n&gt;&gt;&gt; # Returns: {'time': 'T', 'lat': 'Y', 'lon': 'X'}\n</code></pre> Source code in <code>src/dummyxarray/cf_compliance.py</code> <pre><code>def infer_axis(self, coord_name=None):\n    \"\"\"\n    Infer axis attribute (X/Y/Z/T) for coordinates based on CF conventions.\n\n    Uses coordinate names, standard_name attributes, units, and dimension\n    patterns to automatically detect axis types.\n\n    Parameters\n    ----------\n    coord_name : str, optional\n        Specific coordinate to infer axis for. If None, infers for all coordinates.\n\n    Returns\n    -------\n    dict\n        Dictionary mapping coordinate names to inferred axis values ('X', 'Y', 'Z', 'T')\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n    &gt;&gt;&gt; ds.add_dim(\"lat\", 64)\n    &gt;&gt;&gt; ds.add_dim(\"lon\", 128)\n    &gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\n    &gt;&gt;&gt; ds.add_coord(\"lat\", dims=[\"lat\"], attrs={\"units\": \"degrees_north\"})\n    &gt;&gt;&gt; ds.add_coord(\"lon\", dims=[\"lon\"], attrs={\"units\": \"degrees_east\"})\n    &gt;&gt;&gt; axes = ds.infer_axis()\n    &gt;&gt;&gt; # Returns: {'time': 'T', 'lat': 'Y', 'lon': 'X'}\n    \"\"\"\n    axes = {}\n    coords_to_check = [coord_name] if coord_name else list(self.coords.keys())\n\n    for name in coords_to_check:\n        if name not in self.coords:\n            continue\n\n        coord = self.coords[name]\n        axis = self._detect_axis_type(name, coord)\n        if axis:\n            axes[name] = axis\n\n    return axes\n</code></pre>"},{"location":"api/mixins/cf-compliance/#dummyxarray.cf_compliance.CFComplianceMixin.set_axis_attributes","title":"set_axis_attributes","text":"<pre><code>set_axis_attributes(inferred_only=False)\n</code></pre> <p>Set axis attributes on coordinates based on inferred axis types.</p> <p>This modifies coordinate attributes in-place to add 'axis' attributes following CF conventions.</p> <p>Parameters:</p> Name Type Description Default <code>inferred_only</code> <code>bool</code> <p>If True, only set axis for coordinates that don't already have one. If False, overwrite existing axis attributes with inferred values.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of coordinate names and their assigned axis values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 10)\n&gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\n&gt;&gt;&gt; ds.set_axis_attributes()\n&gt;&gt;&gt; print(ds.coords[\"time\"].attrs[\"axis\"])\nT\n</code></pre> Source code in <code>src/dummyxarray/cf_compliance.py</code> <pre><code>def set_axis_attributes(self, inferred_only=False):\n    \"\"\"\n    Set axis attributes on coordinates based on inferred axis types.\n\n    This modifies coordinate attributes in-place to add 'axis' attributes\n    following CF conventions.\n\n    Parameters\n    ----------\n    inferred_only : bool, default False\n        If True, only set axis for coordinates that don't already have one.\n        If False, overwrite existing axis attributes with inferred values.\n\n    Returns\n    -------\n    dict\n        Dictionary of coordinate names and their assigned axis values\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n    &gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\n    &gt;&gt;&gt; ds.set_axis_attributes()\n    &gt;&gt;&gt; print(ds.coords[\"time\"].attrs[\"axis\"])\n    T\n    \"\"\"\n    assigned = {}\n\n    for coord_name, coord in self.coords.items():\n        # Check if we should skip this coordinate\n        if inferred_only and \"axis\" in coord.attrs:\n            continue\n\n        # Temporarily remove axis attribute to force re-inference\n        existing_axis = coord.attrs.pop(\"axis\", None)\n\n        # Infer axis for this coordinate\n        axis = self._detect_axis_type(coord_name, coord)\n\n        if axis:\n            # Set the inferred axis attribute\n            coord.attrs[\"axis\"] = axis\n            assigned[coord_name] = axis\n        elif existing_axis:\n            # Restore existing axis if we couldn't infer a new one\n            coord.attrs[\"axis\"] = existing_axis\n\n    return assigned\n</code></pre>"},{"location":"api/mixins/cf-compliance/#dummyxarray.cf_compliance.CFComplianceMixin.get_axis_coordinates","title":"get_axis_coordinates","text":"<pre><code>get_axis_coordinates(axis)\n</code></pre> <p>Get all coordinates with a specific axis attribute.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>str</code> <p>Axis type to search for ('X', 'Y', 'Z', 'T')</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of coordinate names with the specified axis</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_coord(\"lon\", dims=[\"lon\"], attrs={\"axis\": \"X\"})\n&gt;&gt;&gt; ds.add_coord(\"lat\", dims=[\"lat\"], attrs={\"axis\": \"Y\"})\n&gt;&gt;&gt; x_coords = ds.get_axis_coordinates(\"X\")\n&gt;&gt;&gt; # Returns: ['lon']\n</code></pre> Source code in <code>src/dummyxarray/cf_compliance.py</code> <pre><code>def get_axis_coordinates(self, axis):\n    \"\"\"\n    Get all coordinates with a specific axis attribute.\n\n    Parameters\n    ----------\n    axis : str\n        Axis type to search for ('X', 'Y', 'Z', 'T')\n\n    Returns\n    -------\n    list\n        List of coordinate names with the specified axis\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_coord(\"lon\", dims=[\"lon\"], attrs={\"axis\": \"X\"})\n    &gt;&gt;&gt; ds.add_coord(\"lat\", dims=[\"lat\"], attrs={\"axis\": \"Y\"})\n    &gt;&gt;&gt; x_coords = ds.get_axis_coordinates(\"X\")\n    &gt;&gt;&gt; # Returns: ['lon']\n    \"\"\"\n    coords = []\n    for name, coord in self.coords.items():\n        if coord.attrs.get(\"axis\") == axis:\n            coords.append(name)\n    return coords\n</code></pre>"},{"location":"api/mixins/cf-compliance/#dummyxarray.cf_compliance.CFComplianceMixin.validate_cf","title":"validate_cf","text":"<pre><code>validate_cf(strict=False)\n</code></pre> <p>Validate dataset against CF conventions.</p> <p>Checks for common CF compliance issues like missing axis attributes, invalid units, missing standard_name, etc.</p> <p>Parameters:</p> Name Type Description Default <code>strict</code> <code>bool</code> <p>If True, raise ValueError on any CF violation. If False, return a list of warnings/errors.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with 'errors' and 'warnings' lists</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 10)\n&gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"])\n&gt;&gt;&gt; result = ds.validate_cf()\n&gt;&gt;&gt; print(result['warnings'])\n['time: Missing axis attribute', 'time: Missing units attribute']\n</code></pre> Source code in <code>src/dummyxarray/cf_compliance.py</code> <pre><code>def validate_cf(self, strict=False):\n    \"\"\"\n    Validate dataset against CF conventions.\n\n    Checks for common CF compliance issues like missing axis attributes,\n    invalid units, missing standard_name, etc.\n\n    Parameters\n    ----------\n    strict : bool, default False\n        If True, raise ValueError on any CF violation.\n        If False, return a list of warnings/errors.\n\n    Returns\n    -------\n    dict\n        Dictionary with 'errors' and 'warnings' lists\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n    &gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"])\n    &gt;&gt;&gt; result = ds.validate_cf()\n    &gt;&gt;&gt; print(result['warnings'])\n    ['time: Missing axis attribute', 'time: Missing units attribute']\n    \"\"\"\n    errors = []\n    warnings = []\n\n    # Check coordinates for axis attributes\n    for name, coord in self.coords.items():\n        # Check for axis attribute\n        if \"axis\" not in coord.attrs:\n            inferred = self.infer_axis(name)\n            if name in inferred:\n                warnings.append(\n                    f\"{name}: Missing 'axis' attribute (can be inferred as '{inferred[name]}')\"\n                )\n            else:\n                warnings.append(f\"{name}: Missing 'axis' attribute (cannot infer)\")\n\n        # Check for units\n        if \"units\" not in coord.attrs:\n            warnings.append(f\"{name}: Missing 'units' attribute\")\n\n        # Check for standard_name on coordinates\n        if \"standard_name\" not in coord.attrs:\n            warnings.append(f\"{name}: Missing 'standard_name' attribute\")\n\n    # Check variables for required attributes\n    for name, var in self.variables.items():\n        # Check for units\n        if \"units\" not in var.attrs:\n            warnings.append(f\"{name}: Variable missing 'units' attribute\")\n\n        # Check for long_name or standard_name\n        if \"long_name\" not in var.attrs and \"standard_name\" not in var.attrs:\n            warnings.append(f\"{name}: Variable missing both 'long_name' and 'standard_name'\")\n\n    # Check global attributes\n    required_global = [\"Conventions\"]\n    for attr in required_global:\n        if attr not in self.attrs:\n            warnings.append(f\"Missing required global attribute: '{attr}'\")\n\n    # Check for CF Conventions version\n    if \"Conventions\" in self.attrs:\n        conventions = self.attrs[\"Conventions\"]\n        if not any(cf in conventions for cf in [\"CF-\", \"cf-\"]):\n            warnings.append(f\"Conventions attribute '{conventions}' does not reference CF\")\n\n    # Check dimension ordering (CF recommends T, Z, Y, X)\n    for name, var in self.variables.items():\n        if var.dims and len(var.dims) &gt; 1:\n            # Get axis types for dimensions\n            dim_axes = []\n            for dim in var.dims:\n                if dim in self.coords:\n                    axis = self.coords[dim].attrs.get(\"axis\")\n                    if axis:\n                        dim_axes.append(axis)\n\n            # Check if order is T, Z, Y, X\n            expected_order = [\"T\", \"Z\", \"Y\", \"X\"]\n            actual_order = [a for a in dim_axes if a in expected_order]\n            sorted_order = sorted(actual_order, key=lambda x: expected_order.index(x))\n\n            if actual_order != sorted_order:\n                warnings.append(\n                    f\"{name}: Dimension order {actual_order} does not follow \"\n                    f\"CF recommendation (T, Z, Y, X)\"\n                )\n\n    result = {\"errors\": errors, \"warnings\": warnings}\n\n    if strict and (errors or warnings):\n        all_issues = errors + warnings\n        raise ValueError(\"CF validation failed:\\n\" + \"\\n\".join(all_issues))\n\n    return result\n</code></pre>"},{"location":"api/mixins/data-generation/","title":"DataGenerationMixin","text":"<p>Provides realistic random data generation for testing and prototyping.</p>"},{"location":"api/mixins/data-generation/#overview","title":"Overview","text":"<p>The <code>DataGenerationMixin</code> populates datasets with realistic random data:</p> <ul> <li>Smart generation - Appropriate ranges based on variable type</li> <li>Reproducible - Use seeds for consistent results</li> <li>Type-aware - Different strategies for coordinates vs variables</li> </ul>"},{"location":"api/mixins/data-generation/#key-methods","title":"Key Methods","text":"<ul> <li><code>populate_with_random_data(seed=None)</code> - Fill all arrays with data</li> <li><code>_generate_coordinate_data(coord_name, size)</code> - Generate coordinate data</li> <li><code>_generate_variable_data(var_name, shape, attrs)</code> - Generate variable data</li> </ul>"},{"location":"api/mixins/data-generation/#data-generation-strategies","title":"Data Generation Strategies","text":""},{"location":"api/mixins/data-generation/#coordinates","title":"Coordinates","text":"<ul> <li>time - Sequential integers (0, 1, 2, ...)</li> <li>lat - Uniform distribution from -90 to 90</li> <li>lon - Uniform distribution from -180 to 180</li> <li>lev/plev - Decreasing pressure levels</li> <li>Default - Sequential integers</li> </ul>"},{"location":"api/mixins/data-generation/#variables","title":"Variables","text":"<p>Based on variable name and units:</p> <ul> <li>temperature - Realistic ranges (250-310 K or -20-40\u00b0C)</li> <li>precipitation - Non-negative, skewed distribution</li> <li>wind - Appropriate ranges for wind components</li> <li>humidity - 0-100% range</li> <li>Default - Standard normal distribution</li> </ul>"},{"location":"api/mixins/data-generation/#usage","title":"Usage","text":"<pre><code>ds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.add_dim(\"lat\", 64)\nds.add_coord(\"time\", dims=[\"time\"])\nds.add_coord(\"lat\", dims=[\"lat\"])\nds.add_variable(\"temperature\", dims=[\"time\", \"lat\"])\n\n# Populate with random data\nds.populate_with_random_data(seed=42)\n\n# Now all arrays have data\nprint(ds.coords[\"time\"].data.shape)  # (10,)\nprint(ds.variables[\"temperature\"].data.shape)  # (10, 64)\n</code></pre>"},{"location":"api/mixins/data-generation/#api-reference","title":"API Reference","text":"<p>Mixin providing data generation capabilities.</p> Source code in <code>src/dummyxarray/data_generation.py</code> <pre><code>class DataGenerationMixin:\n    \"\"\"Mixin providing data generation capabilities.\"\"\"\n\n    def populate_with_random_data(self, seed=None):\n        \"\"\"\n        Populate all variables and coordinates with random but meaningful data.\n\n        This method generates random data based on variable metadata (units,\n        standard_name, etc.) to create realistic-looking test datasets.\n\n        Parameters\n        ----------\n        seed : int, optional\n            Random seed for reproducibility\n\n        Returns\n        -------\n        self\n            Returns self for method chaining\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n        &gt;&gt;&gt; ds.add_dim(\"lat\", 5)\n        &gt;&gt;&gt; ds.add_coord(\"time\", [\"time\"], attrs={\"units\": \"days\"})\n        &gt;&gt;&gt; ds.add_variable(\"temperature\", [\"time\", \"lat\"],\n        ...                 attrs={\"units\": \"K\"})\n        &gt;&gt;&gt; ds.populate_with_random_data(seed=42)\n        &gt;&gt;&gt; print(ds.coords[\"time\"].data)\n        [0 1 2 3 4 5 6 7 8 9]\n        \"\"\"\n        if seed is not None:\n            np.random.seed(seed)\n\n        # Populate coordinates\n        for coord_name, coord_array in self.coords.items():\n            if coord_array.data is None:\n                coord_array.data = self._generate_coordinate_data(coord_name, coord_array)\n\n        # Populate variables\n        for var_name, var_array in self.variables.items():\n            if var_array.data is None:\n                var_array.data = self._generate_variable_data(var_name, var_array)\n\n        return self\n\n    def _generate_coordinate_data(self, name, array):\n        \"\"\"Generate meaningful coordinate data based on metadata.\"\"\"\n        shape = tuple(self.dims[d] for d in array.dims)\n        size = shape[0] if shape else 1\n\n        # Check standard_name or units for hints\n        standard_name = array.attrs.get(\"standard_name\", \"\").lower()\n        units = array.attrs.get(\"units\", \"\").lower()\n\n        # Time coordinates\n        if \"time\" in name.lower() or \"time\" in standard_name:\n            return np.arange(size)\n\n        # Latitude coordinates\n        if \"lat\" in name.lower() or \"latitude\" in standard_name:\n            return np.linspace(-90, 90, size)\n\n        # Longitude coordinates\n        if \"lon\" in name.lower() or \"longitude\" in standard_name:\n            return np.linspace(-180, 180, size)\n\n        # Vertical levels (pressure, height, etc.)\n        if any(x in name.lower() for x in [\"lev\", \"level\", \"plev\", \"height\", \"depth\"]):\n            if \"pressure\" in units or \"hpa\" in units or \"pa\" in units:\n                # Pressure levels (high to low)\n                return np.linspace(1000, 100, size)\n            else:\n                # Generic levels\n                return np.arange(size)\n\n        # Default: sequential integers\n        return np.arange(size)\n\n    def _generate_variable_data(self, name, array):\n        \"\"\"Generate meaningful variable data based on metadata.\"\"\"\n        shape = tuple(self.dims[d] for d in array.dims)\n\n        # Get metadata hints\n        standard_name = array.attrs.get(\"standard_name\", \"\").lower()\n        units = array.attrs.get(\"units\", \"\").lower()\n        long_name = array.attrs.get(\"long_name\", \"\").lower()\n\n        # Temperature variables\n        if any(\n            x in standard_name or x in name.lower() or x in long_name\n            for x in [\"temperature\", \"temp\", \"tas\", \"ts\"]\n        ):\n            if \"k\" == units or \"kelvin\" in units:\n                # Temperature in Kelvin (250-310K range)\n                return np.random.uniform(250, 310, shape)\n            elif \"c\" == units or \"celsius\" in units or \"degc\" in units:\n                # Temperature in Celsius (-30 to 40C range)\n                return np.random.uniform(-30, 40, shape)\n            else:\n                return np.random.uniform(250, 310, shape)\n\n        # Pressure variables (check before precipitation to avoid \"pr\" conflict)\n        if any(\n            x in standard_name or x in name.lower() or x in long_name\n            for x in [\"pressure\", \"pres\", \"psl\"]\n        ):\n            if \"sea_level\" in standard_name or \"msl\" in name.lower() or \"psl\" in name.lower():\n                # Sea level pressure (980-1040 hPa)\n                return np.random.uniform(98000, 104000, shape)\n            else:\n                # Generic pressure\n                return np.random.uniform(50000, 105000, shape)\n\n        # Precipitation variables\n        if (\n            any(\n                x in standard_name or x in name.lower() or x in long_name\n                for x in [\"precipitation\", \"precip\", \"rain\"]\n            )\n            or name.lower() == \"pr\"\n        ):\n            # Precipitation (always positive, skewed distribution)\n            return np.random.exponential(0.001, shape)\n\n        # Wind variables\n        if any(\n            x in standard_name or x in name.lower() or x in long_name for x in [\"wind\", \"velocity\"]\n        ):\n            # Wind speed (0-30 m/s, can be negative for components)\n            if any(x in name.lower() for x in [\"u\", \"zonal\", \"eastward\"]):\n                return np.random.uniform(-20, 20, shape)\n            elif any(x in name.lower() for x in [\"v\", \"meridional\", \"northward\"]):\n                return np.random.uniform(-20, 20, shape)\n            else:\n                return np.random.uniform(0, 30, shape)\n\n        # Humidity variables\n        if any(\n            x in standard_name or x in name.lower() or x in long_name\n            for x in [\"humidity\", \"moisture\", \"rh\"]\n        ):\n            if \"relative\" in standard_name or \"relative\" in long_name:\n                # Relative humidity (0-100%)\n                return np.random.uniform(20, 100, shape)\n            else:\n                # Specific humidity (small positive values)\n                return np.random.uniform(0, 0.02, shape)\n\n        # Radiation variables\n        if any(\n            x in standard_name or x in name.lower() or x in long_name for x in [\"radiation\", \"flux\"]\n        ):\n            # Radiation (positive values, 0-1000 W/m\u00b2)\n            return np.random.uniform(0, 1000, shape)\n\n        # Default: standard normal distribution\n        return np.random.randn(*shape)\n</code></pre>"},{"location":"api/mixins/data-generation/#dummyxarray.data_generation.DataGenerationMixin.populate_with_random_data","title":"populate_with_random_data","text":"<pre><code>populate_with_random_data(seed=None)\n</code></pre> <p>Populate all variables and coordinates with random but meaningful data.</p> <p>This method generates random data based on variable metadata (units, standard_name, etc.) to create realistic-looking test datasets.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed for reproducibility</p> <code>None</code> <p>Returns:</p> Type Description <code>self</code> <p>Returns self for method chaining</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 10)\n&gt;&gt;&gt; ds.add_dim(\"lat\", 5)\n&gt;&gt;&gt; ds.add_coord(\"time\", [\"time\"], attrs={\"units\": \"days\"})\n&gt;&gt;&gt; ds.add_variable(\"temperature\", [\"time\", \"lat\"],\n...                 attrs={\"units\": \"K\"})\n&gt;&gt;&gt; ds.populate_with_random_data(seed=42)\n&gt;&gt;&gt; print(ds.coords[\"time\"].data)\n[0 1 2 3 4 5 6 7 8 9]\n</code></pre> Source code in <code>src/dummyxarray/data_generation.py</code> <pre><code>def populate_with_random_data(self, seed=None):\n    \"\"\"\n    Populate all variables and coordinates with random but meaningful data.\n\n    This method generates random data based on variable metadata (units,\n    standard_name, etc.) to create realistic-looking test datasets.\n\n    Parameters\n    ----------\n    seed : int, optional\n        Random seed for reproducibility\n\n    Returns\n    -------\n    self\n        Returns self for method chaining\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n    &gt;&gt;&gt; ds.add_dim(\"lat\", 5)\n    &gt;&gt;&gt; ds.add_coord(\"time\", [\"time\"], attrs={\"units\": \"days\"})\n    &gt;&gt;&gt; ds.add_variable(\"temperature\", [\"time\", \"lat\"],\n    ...                 attrs={\"units\": \"K\"})\n    &gt;&gt;&gt; ds.populate_with_random_data(seed=42)\n    &gt;&gt;&gt; print(ds.coords[\"time\"].data)\n    [0 1 2 3 4 5 6 7 8 9]\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Populate coordinates\n    for coord_name, coord_array in self.coords.items():\n        if coord_array.data is None:\n            coord_array.data = self._generate_coordinate_data(coord_name, coord_array)\n\n    # Populate variables\n    for var_name, var_array in self.variables.items():\n        if var_array.data is None:\n            var_array.data = self._generate_variable_data(var_name, var_array)\n\n    return self\n</code></pre>"},{"location":"api/mixins/history/","title":"HistoryMixin","text":"<p>Provides operation tracking and replay functionality.</p>"},{"location":"api/mixins/history/#overview","title":"Overview","text":"<p>The <code>HistoryMixin</code> automatically records all operations performed on a dataset, enabling:</p> <ul> <li>Reproducibility - Replay operations to recreate datasets</li> <li>Documentation - Export history as Python code, JSON, or YAML</li> <li>Visualization - View history as text, DOT graphs, or Mermaid diagrams</li> <li>Debugging - Understand how a dataset was constructed</li> </ul>"},{"location":"api/mixins/history/#key-methods","title":"Key Methods","text":"<ul> <li><code>get_history()</code> - Get list of recorded operations</li> <li><code>export_history(format)</code> - Export as 'python', 'json', or 'yaml'</li> <li><code>visualize_history(format)</code> - Visualize as 'text', 'dot', or 'mermaid'</li> <li><code>replay_history(history)</code> - Recreate dataset from history</li> <li><code>reset_history()</code> - Clear history and start fresh</li> </ul>"},{"location":"api/mixins/history/#usage","title":"Usage","text":"<p>See the History Tracking Guide for detailed examples.</p>"},{"location":"api/mixins/history/#api-reference","title":"API Reference","text":"<p>Mixin providing history tracking and visualization capabilities.</p> Source code in <code>src/dummyxarray/history.py</code> <pre><code>class HistoryMixin:\n    \"\"\"Mixin providing history tracking and visualization capabilities.\"\"\"\n\n    def _record_operation(self, func_name, args, provenance=None):\n        \"\"\"\n        Record an operation in the history with provenance information.\n\n        Parameters\n        ----------\n        func_name : str\n            Name of the function/method called\n        args : dict\n            Arguments passed to the function\n        provenance : dict, optional\n            Provenance information capturing state changes:\n            - 'changes': dict of what changed (before -&gt; after)\n            - 'added': list of items added\n            - 'removed': list of items removed\n            - 'modified': dict of items modified with before/after values\n        \"\"\"\n        if self._history is not None:\n            entry = {\"func\": func_name, \"args\": args}\n            if provenance:\n                entry[\"provenance\"] = provenance\n            self._history.append(entry)\n\n    def get_history(self, include_provenance=True):\n        \"\"\"\n        Get the operation history for this dataset.\n\n        Parameters\n        ----------\n        include_provenance : bool, optional\n            Whether to include provenance information (default: True)\n\n        Returns\n        -------\n        list of dict\n            List of operations, each with 'func', 'args', and optionally 'provenance' keys\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n        &gt;&gt;&gt; ds.assign_attrs(title=\"Test\")\n        &gt;&gt;&gt; ds.get_history()\n        [{'func': '__init__', 'args': {}},\n         {'func': 'add_dim', 'args': {'name': 'time', 'size': 10},\n          'provenance': {'added': ['time']}},\n         {'func': 'assign_attrs', 'args': {'title': 'Test'},\n          'provenance': {'modified': {'title': {'before': None, 'after': 'Test'}}}}]\n        \"\"\"\n        if self._history is None:\n            return []\n\n        if include_provenance:\n            return self._history.copy()\n        else:\n            # Return history without provenance information\n            return [{\"func\": op[\"func\"], \"args\": op[\"args\"]} for op in self._history]\n\n    def export_history(self, format=\"json\"):\n        \"\"\"\n        Export the operation history in a serializable format.\n\n        Parameters\n        ----------\n        format : str, optional\n            Export format: 'json', 'yaml', or 'python' (default: 'json')\n\n        Returns\n        -------\n        str\n            Serialized history\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n        &gt;&gt;&gt; print(ds.export_history('python'))\n        ds = DummyDataset()\n        ds.add_dim(name='time', size=10)\n        \"\"\"\n        history = self.get_history()\n\n        if format == \"json\":\n            return json.dumps(history, indent=2)\n        elif format == \"yaml\":\n            return yaml.dump(history, default_flow_style=False)\n        elif format == \"python\":\n            lines = []\n            for op in history:\n                if op[\"func\"] == \"__init__\":\n                    lines.append(\"ds = DummyDataset()\")\n                else:\n                    args_str = \", \".join(f\"{k}={repr(v)}\" for k, v in op[\"args\"].items())\n                    lines.append(f\"ds.{op['func']}({args_str})\")\n            return \"\\n\".join(lines)\n        else:\n            raise ValueError(f\"Unknown format: {format}. Use 'json', 'yaml', or 'python'\")\n\n    @classmethod\n    def replay_history(cls, history):\n        \"\"\"\n        Replay a sequence of operations to recreate a dataset.\n\n        Parameters\n        ----------\n        history : list of dict or str\n            History to replay. Can be a list of operations or a JSON/YAML string.\n\n        Returns\n        -------\n        DummyDataset\n            New dataset with operations replayed\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n        &gt;&gt;&gt; history = ds.get_history()\n        &gt;&gt;&gt; new_ds = DummyDataset.replay_history(history)\n        \"\"\"\n        # Parse history if it's a string\n        if isinstance(history, str):\n            try:\n                history = json.loads(history)\n            except json.JSONDecodeError:\n                history = yaml.safe_load(history)\n\n        # Create new dataset without recording\n        ds = cls(_record_history=False)\n\n        # Replay operations (skip __init__)\n        for op in history:\n            if op[\"func\"] == \"__init__\":\n                continue\n\n            func = getattr(ds, op[\"func\"], None)\n            if func and callable(func):\n                func(**op[\"args\"])\n\n        return ds\n\n    def reset_history(self):\n        \"\"\"\n        Reset the operation history and provenance tracking.\n\n        This is useful after capturing an initial dataset state (e.g., from xarray)\n        when you want to track only subsequent modifications without the initial\n        construction operations.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Capture initial state from xarray\n        &gt;&gt;&gt; ds = DummyDataset.from_xarray(xr_dataset)\n        &gt;&gt;&gt; # Reset to start tracking only new changes\n        &gt;&gt;&gt; ds.reset_history()\n        &gt;&gt;&gt; # Now modifications are tracked from this point\n        &gt;&gt;&gt; ds.assign_attrs(institution=\"DKRZ\")\n        &gt;&gt;&gt; # History only shows changes after reset\n        &gt;&gt;&gt; print(ds.visualize_history())\n        \"\"\"\n        self._history = []\n        self._record_operation(\"__init__\", {})\n\n    def visualize_history(self, format=\"text\", **kwargs):\n        \"\"\"\n        Visualize the operation history.\n\n        Parameters\n        ----------\n        format : str, optional\n            Visualization format: 'text', 'dot', 'mermaid' (default: 'text')\n        **kwargs\n            Additional arguments for specific formats:\n            - show_args : bool - Show operation arguments (default: True)\n            - compact : bool - Use compact representation (default: False)\n\n        Returns\n        -------\n        str\n            Formatted visualization string\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n        &gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"])\n        &gt;&gt;&gt; print(ds.visualize_history())\n        Dataset Construction History\n        ============================\n        1. __init__()\n        2. add_dim(name='time', size=10)\n        3. add_coord(name='time', dims=['time'])\n\n        &gt;&gt;&gt; print(ds.visualize_history(format='dot'))\n        digraph dataset_history { ... }\n        \"\"\"\n        history = self.get_history()\n        show_args = kwargs.get(\"show_args\", True)\n        compact = kwargs.get(\"compact\", False)\n\n        if format == \"text\":\n            return self._visualize_text(history, show_args, compact)\n        elif format == \"dot\":\n            return self._visualize_dot(history, show_args)\n        elif format == \"mermaid\":\n            return self._visualize_mermaid(history, show_args)\n        else:\n            raise ValueError(f\"Unknown format: {format}. Use 'text', 'dot', or 'mermaid'\")\n\n    def _visualize_text(self, history, show_args=True, compact=False):\n        \"\"\"Create a text-based visualization of the history.\"\"\"\n        if not history:\n            return \"No operations recorded\"\n\n        if compact:\n            lines = []\n            for i, op in enumerate(history, 1):\n                if show_args and op[\"args\"]:\n                    args_str = \", \".join(f\"{k}={repr(v)}\" for k, v in op[\"args\"].items())\n                    lines.append(f\"{i}. {op['func']}({args_str})\")\n                else:\n                    lines.append(f\"{i}. {op['func']}()\")\n            return \"\\n\".join(lines)\n        else:\n            lines = [\"Dataset Construction History\", \"=\" * 28, \"\"]\n\n            for i, op in enumerate(history, 1):\n                if show_args and op[\"args\"]:\n                    args_str = \", \".join(f\"{k}={repr(v)}\" for k, v in op[\"args\"].items())\n                    lines.append(f\"{i}. {op['func']}({args_str})\")\n                else:\n                    lines.append(f\"{i}. {op['func']}()\")\n\n            # Add summary\n            lines.append(\"\")\n            lines.append(\"Summary:\")\n            lines.append(f\"  Total operations: {len(history)}\")\n\n            # Count operation types\n            op_counts = {}\n            for op in history:\n                op_counts[op[\"func\"]] = op_counts.get(op[\"func\"], 0) + 1\n\n            lines.append(\"  Operation breakdown:\")\n            for func, count in sorted(op_counts.items()):\n                lines.append(f\"    {func}: {count}\")\n\n            return \"\\n\".join(lines)\n\n    def _visualize_dot(self, history, show_args=True):\n        \"\"\"Create a Graphviz DOT format visualization.\"\"\"\n        lines = [\n            \"digraph dataset_history {\",\n            \"  rankdir=TB;\",\n            \"  node [shape=box, style=rounded];\",\n            \"\",\n        ]\n\n        # Create nodes for each operation\n        for i, op in enumerate(history):\n            label = op[\"func\"]\n            if show_args and op[\"args\"]:\n                args_str = \"\\\\n\".join(f\"{k}={repr(v)}\" for k, v in list(op[\"args\"].items())[:3])\n                if len(op[\"args\"]) &gt; 3:\n                    args_str += \"\\\\n...\"\n                label = f\"{label}\\\\n{args_str}\"\n\n            # Color code by operation type\n            color = self._get_operation_color(op[\"func\"])\n            lines.append(f'  op{i} [label=\"{label}\", fillcolor=\"{color}\", style=filled];')\n\n        # Create edges\n        for i in range(len(history) - 1):\n            lines.append(f\"  op{i} -&gt; op{i+1};\")\n\n        lines.append(\"}\")\n        return \"\\n\".join(lines)\n\n    def _visualize_mermaid(self, history, show_args=True):\n        \"\"\"Create a Mermaid diagram visualization.\"\"\"\n        lines = [\"graph TD\"]\n\n        # Create nodes for each operation\n        for i, op in enumerate(history):\n            label = op[\"func\"]\n            if show_args and op[\"args\"]:\n                args_str = \"&lt;br/&gt;\".join(f\"{k}={repr(v)}\" for k, v in list(op[\"args\"].items())[:2])\n                if len(op[\"args\"]) &gt; 2:\n                    args_str += \"&lt;br/&gt;...\"\n                label = f\"{label}&lt;br/&gt;{args_str}\"\n\n            # Use different shapes for different operations\n            if op[\"func\"] == \"__init__\":\n                lines.append(f'  op{i}[\"{label}\"]')\n            elif op[\"func\"].startswith(\"add_\"):\n                lines.append(f'  op{i}(\"{label}\")')\n            else:\n                lines.append(f'  op{i}[\"{label}\"]')\n\n        # Create edges\n        for i in range(len(history) - 1):\n            lines.append(f\"  op{i} --&gt; op{i+1}\")\n\n        return \"\\n\".join(lines)\n\n    def _get_operation_color(self, func_name):\n        \"\"\"Get color for operation type.\"\"\"\n        color_map = {\n            \"__init__\": \"lightblue\",\n            \"add_dim\": \"lightgreen\",\n            \"add_coord\": \"lightyellow\",\n            \"add_variable\": \"lightcoral\",\n            \"assign_attrs\": \"lavender\",\n            \"populate_with_random_data\": \"lightpink\",\n        }\n        return color_map.get(func_name, \"lightgray\")\n</code></pre>"},{"location":"api/mixins/history/#dummyxarray.history.HistoryMixin.get_history","title":"get_history","text":"<pre><code>get_history(include_provenance=True)\n</code></pre> <p>Get the operation history for this dataset.</p> <p>Parameters:</p> Name Type Description Default <code>include_provenance</code> <code>bool</code> <p>Whether to include provenance information (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <code>list of dict</code> <p>List of operations, each with 'func', 'args', and optionally 'provenance' keys</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 10)\n&gt;&gt;&gt; ds.assign_attrs(title=\"Test\")\n&gt;&gt;&gt; ds.get_history()\n[{'func': '__init__', 'args': {}},\n {'func': 'add_dim', 'args': {'name': 'time', 'size': 10},\n  'provenance': {'added': ['time']}},\n {'func': 'assign_attrs', 'args': {'title': 'Test'},\n  'provenance': {'modified': {'title': {'before': None, 'after': 'Test'}}}}]\n</code></pre> Source code in <code>src/dummyxarray/history.py</code> <pre><code>def get_history(self, include_provenance=True):\n    \"\"\"\n    Get the operation history for this dataset.\n\n    Parameters\n    ----------\n    include_provenance : bool, optional\n        Whether to include provenance information (default: True)\n\n    Returns\n    -------\n    list of dict\n        List of operations, each with 'func', 'args', and optionally 'provenance' keys\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n    &gt;&gt;&gt; ds.assign_attrs(title=\"Test\")\n    &gt;&gt;&gt; ds.get_history()\n    [{'func': '__init__', 'args': {}},\n     {'func': 'add_dim', 'args': {'name': 'time', 'size': 10},\n      'provenance': {'added': ['time']}},\n     {'func': 'assign_attrs', 'args': {'title': 'Test'},\n      'provenance': {'modified': {'title': {'before': None, 'after': 'Test'}}}}]\n    \"\"\"\n    if self._history is None:\n        return []\n\n    if include_provenance:\n        return self._history.copy()\n    else:\n        # Return history without provenance information\n        return [{\"func\": op[\"func\"], \"args\": op[\"args\"]} for op in self._history]\n</code></pre>"},{"location":"api/mixins/history/#dummyxarray.history.HistoryMixin.export_history","title":"export_history","text":"<pre><code>export_history(format='json')\n</code></pre> <p>Export the operation history in a serializable format.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>str</code> <p>Export format: 'json', 'yaml', or 'python' (default: 'json')</p> <code>'json'</code> <p>Returns:</p> Type Description <code>str</code> <p>Serialized history</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 10)\n&gt;&gt;&gt; print(ds.export_history('python'))\nds = DummyDataset()\nds.add_dim(name='time', size=10)\n</code></pre> Source code in <code>src/dummyxarray/history.py</code> <pre><code>def export_history(self, format=\"json\"):\n    \"\"\"\n    Export the operation history in a serializable format.\n\n    Parameters\n    ----------\n    format : str, optional\n        Export format: 'json', 'yaml', or 'python' (default: 'json')\n\n    Returns\n    -------\n    str\n        Serialized history\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n    &gt;&gt;&gt; print(ds.export_history('python'))\n    ds = DummyDataset()\n    ds.add_dim(name='time', size=10)\n    \"\"\"\n    history = self.get_history()\n\n    if format == \"json\":\n        return json.dumps(history, indent=2)\n    elif format == \"yaml\":\n        return yaml.dump(history, default_flow_style=False)\n    elif format == \"python\":\n        lines = []\n        for op in history:\n            if op[\"func\"] == \"__init__\":\n                lines.append(\"ds = DummyDataset()\")\n            else:\n                args_str = \", \".join(f\"{k}={repr(v)}\" for k, v in op[\"args\"].items())\n                lines.append(f\"ds.{op['func']}({args_str})\")\n        return \"\\n\".join(lines)\n    else:\n        raise ValueError(f\"Unknown format: {format}. Use 'json', 'yaml', or 'python'\")\n</code></pre>"},{"location":"api/mixins/history/#dummyxarray.history.HistoryMixin.replay_history","title":"replay_history  <code>classmethod</code>","text":"<pre><code>replay_history(history)\n</code></pre> <p>Replay a sequence of operations to recreate a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>list of dict or str</code> <p>History to replay. Can be a list of operations or a JSON/YAML string.</p> required <p>Returns:</p> Type Description <code>DummyDataset</code> <p>New dataset with operations replayed</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 10)\n&gt;&gt;&gt; history = ds.get_history()\n&gt;&gt;&gt; new_ds = DummyDataset.replay_history(history)\n</code></pre> Source code in <code>src/dummyxarray/history.py</code> <pre><code>@classmethod\ndef replay_history(cls, history):\n    \"\"\"\n    Replay a sequence of operations to recreate a dataset.\n\n    Parameters\n    ----------\n    history : list of dict or str\n        History to replay. Can be a list of operations or a JSON/YAML string.\n\n    Returns\n    -------\n    DummyDataset\n        New dataset with operations replayed\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n    &gt;&gt;&gt; history = ds.get_history()\n    &gt;&gt;&gt; new_ds = DummyDataset.replay_history(history)\n    \"\"\"\n    # Parse history if it's a string\n    if isinstance(history, str):\n        try:\n            history = json.loads(history)\n        except json.JSONDecodeError:\n            history = yaml.safe_load(history)\n\n    # Create new dataset without recording\n    ds = cls(_record_history=False)\n\n    # Replay operations (skip __init__)\n    for op in history:\n        if op[\"func\"] == \"__init__\":\n            continue\n\n        func = getattr(ds, op[\"func\"], None)\n        if func and callable(func):\n            func(**op[\"args\"])\n\n    return ds\n</code></pre>"},{"location":"api/mixins/history/#dummyxarray.history.HistoryMixin.reset_history","title":"reset_history","text":"<pre><code>reset_history()\n</code></pre> <p>Reset the operation history and provenance tracking.</p> <p>This is useful after capturing an initial dataset state (e.g., from xarray) when you want to track only subsequent modifications without the initial construction operations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Capture initial state from xarray\n&gt;&gt;&gt; ds = DummyDataset.from_xarray(xr_dataset)\n&gt;&gt;&gt; # Reset to start tracking only new changes\n&gt;&gt;&gt; ds.reset_history()\n&gt;&gt;&gt; # Now modifications are tracked from this point\n&gt;&gt;&gt; ds.assign_attrs(institution=\"DKRZ\")\n&gt;&gt;&gt; # History only shows changes after reset\n&gt;&gt;&gt; print(ds.visualize_history())\n</code></pre> Source code in <code>src/dummyxarray/history.py</code> <pre><code>def reset_history(self):\n    \"\"\"\n    Reset the operation history and provenance tracking.\n\n    This is useful after capturing an initial dataset state (e.g., from xarray)\n    when you want to track only subsequent modifications without the initial\n    construction operations.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Capture initial state from xarray\n    &gt;&gt;&gt; ds = DummyDataset.from_xarray(xr_dataset)\n    &gt;&gt;&gt; # Reset to start tracking only new changes\n    &gt;&gt;&gt; ds.reset_history()\n    &gt;&gt;&gt; # Now modifications are tracked from this point\n    &gt;&gt;&gt; ds.assign_attrs(institution=\"DKRZ\")\n    &gt;&gt;&gt; # History only shows changes after reset\n    &gt;&gt;&gt; print(ds.visualize_history())\n    \"\"\"\n    self._history = []\n    self._record_operation(\"__init__\", {})\n</code></pre>"},{"location":"api/mixins/history/#dummyxarray.history.HistoryMixin.visualize_history","title":"visualize_history","text":"<pre><code>visualize_history(format='text', **kwargs)\n</code></pre> <p>Visualize the operation history.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>str</code> <p>Visualization format: 'text', 'dot', 'mermaid' (default: 'text')</p> <code>'text'</code> <code>**kwargs</code> <p>Additional arguments for specific formats: - show_args : bool - Show operation arguments (default: True) - compact : bool - Use compact representation (default: False)</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted visualization string</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 10)\n&gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"])\n&gt;&gt;&gt; print(ds.visualize_history())\nDataset Construction History\n============================\n1. __init__()\n2. add_dim(name='time', size=10)\n3. add_coord(name='time', dims=['time'])\n</code></pre> <pre><code>&gt;&gt;&gt; print(ds.visualize_history(format='dot'))\ndigraph dataset_history { ... }\n</code></pre> Source code in <code>src/dummyxarray/history.py</code> <pre><code>def visualize_history(self, format=\"text\", **kwargs):\n    \"\"\"\n    Visualize the operation history.\n\n    Parameters\n    ----------\n    format : str, optional\n        Visualization format: 'text', 'dot', 'mermaid' (default: 'text')\n    **kwargs\n        Additional arguments for specific formats:\n        - show_args : bool - Show operation arguments (default: True)\n        - compact : bool - Use compact representation (default: False)\n\n    Returns\n    -------\n    str\n        Formatted visualization string\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 10)\n    &gt;&gt;&gt; ds.add_coord(\"time\", dims=[\"time\"])\n    &gt;&gt;&gt; print(ds.visualize_history())\n    Dataset Construction History\n    ============================\n    1. __init__()\n    2. add_dim(name='time', size=10)\n    3. add_coord(name='time', dims=['time'])\n\n    &gt;&gt;&gt; print(ds.visualize_history(format='dot'))\n    digraph dataset_history { ... }\n    \"\"\"\n    history = self.get_history()\n    show_args = kwargs.get(\"show_args\", True)\n    compact = kwargs.get(\"compact\", False)\n\n    if format == \"text\":\n        return self._visualize_text(history, show_args, compact)\n    elif format == \"dot\":\n        return self._visualize_dot(history, show_args)\n    elif format == \"mermaid\":\n        return self._visualize_mermaid(history, show_args)\n    else:\n        raise ValueError(f\"Unknown format: {format}. Use 'text', 'dot', or 'mermaid'\")\n</code></pre>"},{"location":"api/mixins/io/","title":"IOMixin","text":"<p>Provides serialization and format conversion functionality.</p>"},{"location":"api/mixins/io/#overview","title":"Overview","text":"<p>The <code>IOMixin</code> enables exporting and importing datasets in multiple formats:</p> <ul> <li>YAML - Human-readable configuration format</li> <li>JSON - Structured data format</li> <li>xarray - Convert to/from xarray.Dataset</li> <li>Zarr - Write directly to Zarr storage</li> <li>NetCDF - Via xarray conversion</li> <li>Intake Catalogs - Export and import Intake catalog YAML files</li> </ul>"},{"location":"api/mixins/io/#key-methods","title":"Key Methods","text":""},{"location":"api/mixins/io/#export-methods","title":"Export Methods","text":"<ul> <li><code>to_dict()</code> - Export as Python dictionary</li> <li><code>to_json(indent=2, **kwargs)</code> - Export as JSON string</li> <li><code>to_yaml()</code> - Export as YAML string</li> <li><code>save_yaml(filepath)</code> - Save to YAML file</li> <li><code>to_xarray()</code> - Convert to xarray.Dataset</li> <li><code>to_zarr(store, **kwargs)</code> - Write to Zarr store</li> <li><code>to_intake_catalog(name, description, driver, data_path, **kwargs)</code> - Export as Intake catalog YAML</li> <li><code>save_intake_catalog(path, name, description, driver, data_path, **kwargs)</code> - Save Intake catalog to file</li> </ul>"},{"location":"api/mixins/io/#import-methods","title":"Import Methods","text":"<ul> <li><code>from_xarray(xr_dataset)</code> - Create from xarray.Dataset (class method)</li> <li><code>load_yaml(filepath)</code> - Load from YAML file (class method)</li> <li><code>from_intake_catalog(catalog_source, source_name)</code> - Load from Intake catalog (class method)</li> <li><code>load_intake_catalog(path, source_name)</code> - Load from Intake catalog file (class method)</li> </ul>"},{"location":"api/mixins/io/#usage","title":"Usage","text":"<pre><code># Export to YAML\nds.save_yaml(\"template.yaml\")\n\n# Load from YAML\nds = DummyDataset.load_yaml(\"template.yaml\")\n\n# Convert to xarray\nxr_ds = ds.to_xarray()\n\n# Import from xarray\nds = DummyDataset.from_xarray(xr_ds)\n\n# Write to Zarr\nds.to_zarr(\"output.zarr\")\n\n# Export to Intake catalog\ncatalog_yaml = ds.to_intake_catalog(\n    name=\"my_data\", \n    description=\"My dataset\",\n    driver=\"zarr\"\n)\nds.save_intake_catalog(\"catalog.yaml\", name=\"my_data\")\n\n# Import from Intake catalog\nloaded_ds = DummyDataset.from_intake_catalog(\"catalog.yaml\", \"my_data\")\nloaded_ds = DummyDataset.load_intake_catalog(\"catalog.yaml\", \"my_data\")\n</code></pre>"},{"location":"api/mixins/io/#api-reference","title":"API Reference","text":"<p>Mixin providing I/O capabilities.</p> Source code in <code>src/dummyxarray/io.py</code> <pre><code>class IOMixin:\n    \"\"\"Mixin providing I/O capabilities.\"\"\"\n\n    def to_dict(self):\n        \"\"\"\n        Export dataset structure to a dictionary.\n\n        Returns\n        -------\n        dict\n            Dictionary representation of the dataset\n        \"\"\"\n        return {\n            \"dimensions\": self.dims,\n            \"coordinates\": {k: v.to_dict() for k, v in self.coords.items()},\n            \"variables\": {k: v.to_dict() for k, v in self.variables.items()},\n            \"attrs\": self.attrs,\n        }\n\n    def to_json(self, **kwargs):\n        \"\"\"\n        Export dataset structure to JSON string.\n\n        Parameters\n        ----------\n        **kwargs\n            Additional arguments passed to json.dumps\n\n        Returns\n        -------\n        str\n            JSON representation\n        \"\"\"\n        # Set default indent if not provided\n        if \"indent\" not in kwargs:\n            kwargs[\"indent\"] = 2\n        return json.dumps(self.to_dict(), **kwargs)\n\n    def to_yaml(self):\n        \"\"\"\n        Export dataset structure to YAML string.\n\n        Returns\n        -------\n        str\n            YAML representation\n        \"\"\"\n        return yaml.dump(self.to_dict(), sort_keys=False)\n\n    def save_yaml(self, path):\n        \"\"\"\n        Save dataset specification to a YAML file.\n\n        Parameters\n        ----------\n        path : str\n            Output file path\n        \"\"\"\n        with open(path, \"w\") as f:\n            f.write(self.to_yaml())\n\n    @classmethod\n    def load_yaml(cls, path):\n        \"\"\"\n        Load dataset specification from a YAML file.\n\n        Parameters\n        ----------\n        path : str\n            Input file path\n\n        Returns\n        -------\n        DummyDataset\n            Loaded dataset (without data arrays)\n        \"\"\"\n        # Import here to avoid circular dependency\n        from .core import DummyArray\n\n        with open(path) as f:\n            spec = yaml.safe_load(f)\n\n        ds = cls()\n\n        ds.dims.update(spec.get(\"dimensions\", {}))\n\n        for name, info in spec.get(\"coordinates\", {}).items():\n            ds.coords[name] = DummyArray(\n                dims=info[\"dims\"], attrs=info[\"attrs\"], data=None, encoding=info.get(\"encoding\", {})\n            )\n\n        for name, info in spec.get(\"variables\", {}).items():\n            ds.variables[name] = DummyArray(\n                dims=info[\"dims\"], attrs=info[\"attrs\"], data=None, encoding=info.get(\"encoding\", {})\n            )\n\n        ds.attrs.update(spec.get(\"attrs\", {}))\n\n        return ds\n\n    @classmethod\n    def from_xarray(cls, xr_dataset, include_data=False):\n        \"\"\"\n        Create a DummyDataset from an existing xarray.Dataset.\n\n        This captures all metadata (dimensions, coordinates, variables, attributes,\n        and encoding) from an xarray.Dataset without the actual data arrays\n        (unless include_data=True).\n\n        Parameters\n        ----------\n        xr_dataset : xarray.Dataset\n            The xarray Dataset to extract metadata from\n        include_data : bool, default False\n            If True, include the actual data arrays. If False, only capture\n            metadata structure.\n\n        Returns\n        -------\n        DummyDataset\n            A new DummyDataset with the structure and metadata from xr_dataset\n\n        Examples\n        --------\n        &gt;&gt;&gt; import xarray as xr\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; xr_ds = xr.Dataset({\n        ...     \"temperature\": ([\"time\", \"lat\"], np.random.rand(10, 5))\n        ... })\n        &gt;&gt;&gt; dummy_ds = DummyDataset.from_xarray(xr_ds)\n        &gt;&gt;&gt; print(dummy_ds.dims)\n        {'time': 10, 'lat': 5}\n        \"\"\"\n        # Import here to avoid circular dependency\n        from .core import DummyArray\n\n        ds = cls()\n\n        # Copy global attributes\n        ds.attrs.update(dict(xr_dataset.attrs))\n\n        # Extract dimensions\n        for dim_name, dim_size in xr_dataset.sizes.items():\n            ds.dims[dim_name] = dim_size\n\n        # Extract coordinates\n        for coord_name, coord_var in xr_dataset.coords.items():\n            ds.coords[coord_name] = DummyArray(\n                dims=list(coord_var.dims),\n                attrs=dict(coord_var.attrs),\n                data=coord_var.values if include_data else None,\n                encoding=dict(coord_var.encoding) if hasattr(coord_var, \"encoding\") else {},\n            )\n\n        # Extract data variables\n        for var_name, var in xr_dataset.data_vars.items():\n            ds.variables[var_name] = DummyArray(\n                dims=list(var.dims),\n                attrs=dict(var.attrs),\n                data=var.values if include_data else None,\n                encoding=dict(var.encoding) if hasattr(var, \"encoding\") else {},\n            )\n\n        return ds\n\n    def to_xarray(self, validate=True):\n        \"\"\"\n        Convert to a real xarray.Dataset.\n\n        Parameters\n        ----------\n        validate : bool, default True\n            Whether to validate the dataset before conversion\n\n        Returns\n        -------\n        xarray.Dataset\n            The constructed xarray Dataset\n\n        Raises\n        ------\n        ValueError\n            If validation fails or if any variable/coordinate is missing data\n        \"\"\"\n        import xarray as xr\n\n        if validate:\n            self.validate(strict_coords=False)\n\n        coords = {}\n        for name, arr in self.coords.items():\n            if arr.data is None:\n                raise ValueError(f\"Coordinate '{name}' missing data.\")\n            coords[name] = (arr.dims, arr.data, arr.attrs)\n\n        variables = {}\n        for name, arr in self.variables.items():\n            if arr.data is None:\n                raise ValueError(f\"Variable '{name}' missing data.\")\n            variables[name] = (arr.dims, arr.data, arr.attrs)\n\n        ds = xr.Dataset(data_vars=variables, coords=coords, attrs=self.attrs)\n\n        # Apply encodings\n        for name, arr in self.variables.items():\n            if arr.encoding:\n                ds[name].encoding = arr.encoding\n\n        for name, arr in self.coords.items():\n            if arr.encoding:\n                ds[name].encoding = arr.encoding\n\n        return ds\n\n    def to_zarr(self, store_path, mode=\"w\", validate=True):\n        \"\"\"\n        Write dataset to Zarr format.\n\n        Parameters\n        ----------\n        store_path : str\n            Path to Zarr store\n        mode : str, default \"w\"\n            Write mode ('w' for write, 'a' for append)\n        validate : bool, default True\n            Whether to validate before writing\n\n        Returns\n        -------\n        zarr.hierarchy.Group\n            The Zarr group\n        \"\"\"\n        ds = self.to_xarray(validate=validate)\n        return ds.to_zarr(store_path, mode=mode)\n\n    def to_intake_catalog(\n        self,\n        name=\"dataset\",\n        description=\"Dataset generated by dummyxarray\",\n        driver=\"zarr\",\n        data_path=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Convert dataset to Intake catalog format.\n\n        Parameters\n        ----------\n        name : str, default \"dataset\"\n            Name for the data source in the catalog\n        description : str, default \"Dataset generated by dummyxarray\"\n            Description of the data source\n        driver : str, default \"zarr\"\n            Intake driver to use (zarr, netcdf, xarray, etc.)\n        data_path : str, optional\n            Path to the actual data file. If None, uses template path\n        **kwargs\n            Additional arguments to pass to the driver\n\n        Returns\n        -------\n        str\n            YAML string representing the Intake catalog\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.add_dim(\"time\", 12)\n        &gt;&gt;&gt; ds.add_variable(\"temperature\", dims=[\"time\"], attrs={\"units\": \"K\"})\n        &gt;&gt;&gt; catalog_yaml = ds.to_intake_catalog(\n        ...     name=\"my_dataset\",\n        ...     description=\"Temperature data\",\n        ...     data_path=\"data/my_dataset.zarr\"\n        ... )\n        \"\"\"\n        # Build catalog structure\n        catalog = {\n            \"metadata\": {\n                \"version\": 1,\n                \"description\": f\"Intake catalog for {name}\",\n            }\n        }\n\n        # Add dataset-level parameters if any\n        if hasattr(self, \"attrs\") and self.attrs:\n            catalog[\"metadata\"][\"dataset_attrs\"] = dict(self.attrs)\n\n        # Build sources section\n        sources = {}\n\n        # Default data path template if not provided\n        if data_path is None:\n            data_path = \"{{ CATALOG_DIR }}/\" + name + \".zarr\"\n\n        source_entry = {\n            \"description\": description,\n            \"driver\": driver,\n            \"args\": {\"urlpath\": data_path, **kwargs},\n        }\n\n        # Add metadata about the dataset structure\n        source_metadata = {}\n\n        # Add dimension information\n        if hasattr(self, \"dims\") and self.dims:\n            source_metadata[\"dimensions\"] = dict(self.dims)\n\n        # Add coordinate information\n        if hasattr(self, \"coords\") and self.coords:\n            coord_info = {}\n            for coord_name, coord_arr in self.coords.items():\n                coord_info[coord_name] = {\n                    \"dims\": coord_arr.dims,\n                    \"attrs\": dict(coord_arr.attrs) if coord_arr.attrs else {},\n                }\n                if coord_arr.encoding:\n                    encoding = dict(coord_arr.encoding)\n                    # Convert tuples to lists for YAML compatibility\n                    for key, value in encoding.items():\n                        if isinstance(value, tuple):\n                            encoding[key] = list(value)\n                    coord_info[coord_name][\"encoding\"] = encoding\n            source_metadata[\"coordinates\"] = coord_info\n\n        # Add variable information\n        if hasattr(self, \"variables\") and self.variables:\n            var_info = {}\n            for var_name, var_arr in self.variables.items():\n                var_info[var_name] = {\n                    \"dims\": var_arr.dims,\n                    \"attrs\": dict(var_arr.attrs) if var_arr.attrs else {},\n                }\n                if var_arr.encoding:\n                    encoding = dict(var_arr.encoding)\n                    # Convert tuples to lists for YAML compatibility\n                    for key, value in encoding.items():\n                        if isinstance(value, tuple):\n                            encoding[key] = list(value)\n                    var_info[var_name][\"encoding\"] = encoding\n            source_metadata[\"variables\"] = var_info\n\n        if source_metadata:\n            source_entry[\"metadata\"] = source_metadata\n\n        sources[name] = source_entry\n        catalog[\"sources\"] = sources\n\n        return yaml.dump(catalog, sort_keys=False)\n\n    def save_intake_catalog(\n        self,\n        path,\n        name=\"dataset\",\n        description=\"Dataset generated by dummyxarray\",\n        driver=\"zarr\",\n        data_path=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Save Intake catalog to a YAML file.\n\n        Parameters\n        ----------\n        path : str\n            Output file path for the catalog YAML\n        name : str, default \"dataset\"\n            Name for the data source in the catalog\n        description : str, default \"Dataset generated by dummyxarray\"\n            Description of the data source\n        driver : str, default \"zarr\"\n            Intake driver to use (zarr, netcdf, xarray, etc.)\n        data_path : str, optional\n            Path to the actual data file. If None, uses template path\n        **kwargs\n            Additional arguments to pass to the driver\n        \"\"\"\n        catalog_yaml = self.to_intake_catalog(\n            name=name, description=description, driver=driver, data_path=data_path, **kwargs\n        )\n\n        with open(path, \"w\") as f:\n            f.write(catalog_yaml)\n\n    @classmethod\n    def from_intake_catalog(cls, catalog_source, source_name=None):\n        \"\"\"\n        Create a DummyDataset from an Intake catalog.\n\n        Parameters\n        ----------\n        catalog_source : str or dict\n            Either a path to a YAML catalog file or a dictionary containing\n            the catalog structure\n        source_name : str, optional\n            Name of the source to use from the catalog. If None and catalog\n            contains only one source, that source will be used automatically.\n\n        Returns\n        -------\n        DummyDataset\n            A new DummyDataset with the structure from the catalog\n\n        Raises\n        ------\n        ValueError\n            If catalog format is invalid or source_name is not found\n        FileNotFoundError\n            If catalog_source is a file path that doesn't exist\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Load from file\n        &gt;&gt;&gt; ds = DummyDataset.from_intake_catalog(\"catalog.yaml\", \"climate_data\")\n\n        &gt;&gt;&gt; # Load from dictionary\n        &gt;&gt;&gt; catalog_dict = yaml.safe_load(catalog_yaml)\n        &gt;&gt;&gt; ds = DummyDataset.from_intake_catalog(catalog_dict, \"climate_data\")\n        \"\"\"\n        from pathlib import Path\n\n        import yaml\n\n        # Load catalog\n        if isinstance(catalog_source, (str, Path)):\n            # Load from file\n            try:\n                with open(catalog_source) as f:\n                    catalog = yaml.safe_load(f)\n            except FileNotFoundError as err:\n                raise FileNotFoundError(f\"Catalog file not found: {catalog_source}\") from err\n        elif isinstance(catalog_source, dict):\n            # Use provided dictionary\n            catalog = catalog_source\n        else:\n            raise ValueError(\"catalog_source must be a file path or dictionary\")\n\n        # Validate catalog structure\n        if not isinstance(catalog, dict):\n            raise ValueError(\"Catalog must be a dictionary\")\n\n        if \"sources\" not in catalog:\n            raise ValueError(\"Catalog must contain 'sources' section\")\n\n        sources = catalog[\"sources\"]\n        if not sources:\n            raise ValueError(\"Catalog sources section cannot be empty\")\n\n        # Determine which source to use\n        if source_name is None:\n            if len(sources) == 1:\n                source_name = list(sources.keys())[0]\n            else:\n                raise ValueError(\n                    \"Multiple sources found in catalog. \" \"Please specify source_name explicitly.\"\n                )\n\n        if source_name not in sources:\n            available_sources = list(sources.keys())\n            raise ValueError(\n                f\"Source '{source_name}' not found in catalog. \"\n                f\"Available sources: {available_sources}\"\n            )\n\n        source = sources[source_name]\n\n        # Create new DummyDataset\n        ds = cls()\n\n        # Extract dataset attributes from catalog metadata if available\n        if \"metadata\" in catalog:\n            catalog_metadata = catalog[\"metadata\"]\n            if \"dataset_attrs\" in catalog_metadata:\n                ds.attrs.update(catalog_metadata[\"dataset_attrs\"])\n\n        # Extract source metadata if available\n        source_metadata = source.get(\"metadata\", {})\n\n        # Add dimensions\n        if \"dimensions\" in source_metadata:\n            for dim_name, dim_size in source_metadata[\"dimensions\"].items():\n                ds.add_dim(dim_name, dim_size)\n\n        # Add coordinates\n        if \"coordinates\" in source_metadata:\n            for coord_name, coord_info in source_metadata[\"coordinates\"].items():\n                coord_attrs = coord_info.get(\"attrs\", {})\n                coord_encoding = coord_info.get(\"encoding\", {})\n                ds.add_coord(\n                    coord_name, dims=coord_info[\"dims\"], attrs=coord_attrs, encoding=coord_encoding\n                )\n\n        # Add variables\n        if \"variables\" in source_metadata:\n            for var_name, var_info in source_metadata[\"variables\"].items():\n                var_attrs = var_info.get(\"attrs\", {})\n                var_encoding = var_info.get(\"encoding\", {})\n                ds.add_variable(\n                    var_name, dims=var_info[\"dims\"], attrs=var_attrs, encoding=var_encoding\n                )\n\n        # Add catalog-specific attributes\n        ds.attrs.update(\n            {\n                \"intake_catalog_source\": source_name,\n                \"intake_driver\": source.get(\"driver\", \"unknown\"),\n                \"intake_description\": source.get(\"description\", \"\"),\n            }\n        )\n\n        return ds\n\n    @classmethod\n    def load_intake_catalog(cls, path, source_name=None):\n        \"\"\"\n        Load a DummyDataset from an Intake catalog YAML file.\n\n        This is a convenience method that wraps from_intake_catalog() for file loading.\n\n        Parameters\n        ----------\n        path : str\n            Path to the catalog YAML file\n        source_name : str, optional\n            Name of the source to use from the catalog\n\n        Returns\n        -------\n        DummyDataset\n            A new DummyDataset with the structure from the catalog\n        \"\"\"\n        return cls.from_intake_catalog(path, source_name)\n</code></pre>"},{"location":"api/mixins/io/#dummyxarray.io.IOMixin.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Export dataset structure to a dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary representation of the dataset</p> Source code in <code>src/dummyxarray/io.py</code> <pre><code>def to_dict(self):\n    \"\"\"\n    Export dataset structure to a dictionary.\n\n    Returns\n    -------\n    dict\n        Dictionary representation of the dataset\n    \"\"\"\n    return {\n        \"dimensions\": self.dims,\n        \"coordinates\": {k: v.to_dict() for k, v in self.coords.items()},\n        \"variables\": {k: v.to_dict() for k, v in self.variables.items()},\n        \"attrs\": self.attrs,\n    }\n</code></pre>"},{"location":"api/mixins/io/#dummyxarray.io.IOMixin.to_json","title":"to_json","text":"<pre><code>to_json(**kwargs)\n</code></pre> <p>Export dataset structure to JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional arguments passed to json.dumps</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>JSON representation</p> Source code in <code>src/dummyxarray/io.py</code> <pre><code>def to_json(self, **kwargs):\n    \"\"\"\n    Export dataset structure to JSON string.\n\n    Parameters\n    ----------\n    **kwargs\n        Additional arguments passed to json.dumps\n\n    Returns\n    -------\n    str\n        JSON representation\n    \"\"\"\n    # Set default indent if not provided\n    if \"indent\" not in kwargs:\n        kwargs[\"indent\"] = 2\n    return json.dumps(self.to_dict(), **kwargs)\n</code></pre>"},{"location":"api/mixins/io/#dummyxarray.io.IOMixin.to_yaml","title":"to_yaml","text":"<pre><code>to_yaml()\n</code></pre> <p>Export dataset structure to YAML string.</p> <p>Returns:</p> Type Description <code>str</code> <p>YAML representation</p> Source code in <code>src/dummyxarray/io.py</code> <pre><code>def to_yaml(self):\n    \"\"\"\n    Export dataset structure to YAML string.\n\n    Returns\n    -------\n    str\n        YAML representation\n    \"\"\"\n    return yaml.dump(self.to_dict(), sort_keys=False)\n</code></pre>"},{"location":"api/mixins/io/#dummyxarray.io.IOMixin.save_yaml","title":"save_yaml","text":"<pre><code>save_yaml(path)\n</code></pre> <p>Save dataset specification to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output file path</p> required Source code in <code>src/dummyxarray/io.py</code> <pre><code>def save_yaml(self, path):\n    \"\"\"\n    Save dataset specification to a YAML file.\n\n    Parameters\n    ----------\n    path : str\n        Output file path\n    \"\"\"\n    with open(path, \"w\") as f:\n        f.write(self.to_yaml())\n</code></pre>"},{"location":"api/mixins/io/#dummyxarray.io.IOMixin.load_yaml","title":"load_yaml  <code>classmethod</code>","text":"<pre><code>load_yaml(path)\n</code></pre> <p>Load dataset specification from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Input file path</p> required <p>Returns:</p> Type Description <code>DummyDataset</code> <p>Loaded dataset (without data arrays)</p> Source code in <code>src/dummyxarray/io.py</code> <pre><code>@classmethod\ndef load_yaml(cls, path):\n    \"\"\"\n    Load dataset specification from a YAML file.\n\n    Parameters\n    ----------\n    path : str\n        Input file path\n\n    Returns\n    -------\n    DummyDataset\n        Loaded dataset (without data arrays)\n    \"\"\"\n    # Import here to avoid circular dependency\n    from .core import DummyArray\n\n    with open(path) as f:\n        spec = yaml.safe_load(f)\n\n    ds = cls()\n\n    ds.dims.update(spec.get(\"dimensions\", {}))\n\n    for name, info in spec.get(\"coordinates\", {}).items():\n        ds.coords[name] = DummyArray(\n            dims=info[\"dims\"], attrs=info[\"attrs\"], data=None, encoding=info.get(\"encoding\", {})\n        )\n\n    for name, info in spec.get(\"variables\", {}).items():\n        ds.variables[name] = DummyArray(\n            dims=info[\"dims\"], attrs=info[\"attrs\"], data=None, encoding=info.get(\"encoding\", {})\n        )\n\n    ds.attrs.update(spec.get(\"attrs\", {}))\n\n    return ds\n</code></pre>"},{"location":"api/mixins/io/#dummyxarray.io.IOMixin.from_xarray","title":"from_xarray  <code>classmethod</code>","text":"<pre><code>from_xarray(xr_dataset, include_data=False)\n</code></pre> <p>Create a DummyDataset from an existing xarray.Dataset.</p> <p>This captures all metadata (dimensions, coordinates, variables, attributes, and encoding) from an xarray.Dataset without the actual data arrays (unless include_data=True).</p> <p>Parameters:</p> Name Type Description Default <code>xr_dataset</code> <code>Dataset</code> <p>The xarray Dataset to extract metadata from</p> required <code>include_data</code> <code>bool</code> <p>If True, include the actual data arrays. If False, only capture metadata structure.</p> <code>False</code> <p>Returns:</p> Type Description <code>DummyDataset</code> <p>A new DummyDataset with the structure and metadata from xr_dataset</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import xarray as xr\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; xr_ds = xr.Dataset({\n...     \"temperature\": ([\"time\", \"lat\"], np.random.rand(10, 5))\n... })\n&gt;&gt;&gt; dummy_ds = DummyDataset.from_xarray(xr_ds)\n&gt;&gt;&gt; print(dummy_ds.dims)\n{'time': 10, 'lat': 5}\n</code></pre> Source code in <code>src/dummyxarray/io.py</code> <pre><code>@classmethod\ndef from_xarray(cls, xr_dataset, include_data=False):\n    \"\"\"\n    Create a DummyDataset from an existing xarray.Dataset.\n\n    This captures all metadata (dimensions, coordinates, variables, attributes,\n    and encoding) from an xarray.Dataset without the actual data arrays\n    (unless include_data=True).\n\n    Parameters\n    ----------\n    xr_dataset : xarray.Dataset\n        The xarray Dataset to extract metadata from\n    include_data : bool, default False\n        If True, include the actual data arrays. If False, only capture\n        metadata structure.\n\n    Returns\n    -------\n    DummyDataset\n        A new DummyDataset with the structure and metadata from xr_dataset\n\n    Examples\n    --------\n    &gt;&gt;&gt; import xarray as xr\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; xr_ds = xr.Dataset({\n    ...     \"temperature\": ([\"time\", \"lat\"], np.random.rand(10, 5))\n    ... })\n    &gt;&gt;&gt; dummy_ds = DummyDataset.from_xarray(xr_ds)\n    &gt;&gt;&gt; print(dummy_ds.dims)\n    {'time': 10, 'lat': 5}\n    \"\"\"\n    # Import here to avoid circular dependency\n    from .core import DummyArray\n\n    ds = cls()\n\n    # Copy global attributes\n    ds.attrs.update(dict(xr_dataset.attrs))\n\n    # Extract dimensions\n    for dim_name, dim_size in xr_dataset.sizes.items():\n        ds.dims[dim_name] = dim_size\n\n    # Extract coordinates\n    for coord_name, coord_var in xr_dataset.coords.items():\n        ds.coords[coord_name] = DummyArray(\n            dims=list(coord_var.dims),\n            attrs=dict(coord_var.attrs),\n            data=coord_var.values if include_data else None,\n            encoding=dict(coord_var.encoding) if hasattr(coord_var, \"encoding\") else {},\n        )\n\n    # Extract data variables\n    for var_name, var in xr_dataset.data_vars.items():\n        ds.variables[var_name] = DummyArray(\n            dims=list(var.dims),\n            attrs=dict(var.attrs),\n            data=var.values if include_data else None,\n            encoding=dict(var.encoding) if hasattr(var, \"encoding\") else {},\n        )\n\n    return ds\n</code></pre>"},{"location":"api/mixins/io/#dummyxarray.io.IOMixin.to_xarray","title":"to_xarray","text":"<pre><code>to_xarray(validate=True)\n</code></pre> <p>Convert to a real xarray.Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>validate</code> <code>bool</code> <p>Whether to validate the dataset before conversion</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The constructed xarray Dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If validation fails or if any variable/coordinate is missing data</p> Source code in <code>src/dummyxarray/io.py</code> <pre><code>def to_xarray(self, validate=True):\n    \"\"\"\n    Convert to a real xarray.Dataset.\n\n    Parameters\n    ----------\n    validate : bool, default True\n        Whether to validate the dataset before conversion\n\n    Returns\n    -------\n    xarray.Dataset\n        The constructed xarray Dataset\n\n    Raises\n    ------\n    ValueError\n        If validation fails or if any variable/coordinate is missing data\n    \"\"\"\n    import xarray as xr\n\n    if validate:\n        self.validate(strict_coords=False)\n\n    coords = {}\n    for name, arr in self.coords.items():\n        if arr.data is None:\n            raise ValueError(f\"Coordinate '{name}' missing data.\")\n        coords[name] = (arr.dims, arr.data, arr.attrs)\n\n    variables = {}\n    for name, arr in self.variables.items():\n        if arr.data is None:\n            raise ValueError(f\"Variable '{name}' missing data.\")\n        variables[name] = (arr.dims, arr.data, arr.attrs)\n\n    ds = xr.Dataset(data_vars=variables, coords=coords, attrs=self.attrs)\n\n    # Apply encodings\n    for name, arr in self.variables.items():\n        if arr.encoding:\n            ds[name].encoding = arr.encoding\n\n    for name, arr in self.coords.items():\n        if arr.encoding:\n            ds[name].encoding = arr.encoding\n\n    return ds\n</code></pre>"},{"location":"api/mixins/io/#dummyxarray.io.IOMixin.to_zarr","title":"to_zarr","text":"<pre><code>to_zarr(store_path, mode='w', validate=True)\n</code></pre> <p>Write dataset to Zarr format.</p> <p>Parameters:</p> Name Type Description Default <code>store_path</code> <code>str</code> <p>Path to Zarr store</p> required <code>mode</code> <code>str</code> <p>Write mode ('w' for write, 'a' for append)</p> <code>\"w\"</code> <code>validate</code> <code>bool</code> <p>Whether to validate before writing</p> <code>True</code> <p>Returns:</p> Type Description <code>Group</code> <p>The Zarr group</p> Source code in <code>src/dummyxarray/io.py</code> <pre><code>def to_zarr(self, store_path, mode=\"w\", validate=True):\n    \"\"\"\n    Write dataset to Zarr format.\n\n    Parameters\n    ----------\n    store_path : str\n        Path to Zarr store\n    mode : str, default \"w\"\n        Write mode ('w' for write, 'a' for append)\n    validate : bool, default True\n        Whether to validate before writing\n\n    Returns\n    -------\n    zarr.hierarchy.Group\n        The Zarr group\n    \"\"\"\n    ds = self.to_xarray(validate=validate)\n    return ds.to_zarr(store_path, mode=mode)\n</code></pre>"},{"location":"api/mixins/io/#dummyxarray.io.IOMixin.to_intake_catalog","title":"to_intake_catalog","text":"<pre><code>to_intake_catalog(\n    name=\"dataset\",\n    description=\"Dataset generated by dummyxarray\",\n    driver=\"zarr\",\n    data_path=None,\n    **kwargs\n)\n</code></pre> <p>Convert dataset to Intake catalog format.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name for the data source in the catalog</p> <code>\"dataset\"</code> <code>description</code> <code>str</code> <p>Description of the data source</p> <code>\"Dataset generated by dummyxarray\"</code> <code>driver</code> <code>str</code> <p>Intake driver to use (zarr, netcdf, xarray, etc.)</p> <code>\"zarr\"</code> <code>data_path</code> <code>str</code> <p>Path to the actual data file. If None, uses template path</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the driver</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>YAML string representing the Intake catalog</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.add_dim(\"time\", 12)\n&gt;&gt;&gt; ds.add_variable(\"temperature\", dims=[\"time\"], attrs={\"units\": \"K\"})\n&gt;&gt;&gt; catalog_yaml = ds.to_intake_catalog(\n...     name=\"my_dataset\",\n...     description=\"Temperature data\",\n...     data_path=\"data/my_dataset.zarr\"\n... )\n</code></pre> Source code in <code>src/dummyxarray/io.py</code> <pre><code>def to_intake_catalog(\n    self,\n    name=\"dataset\",\n    description=\"Dataset generated by dummyxarray\",\n    driver=\"zarr\",\n    data_path=None,\n    **kwargs,\n):\n    \"\"\"\n    Convert dataset to Intake catalog format.\n\n    Parameters\n    ----------\n    name : str, default \"dataset\"\n        Name for the data source in the catalog\n    description : str, default \"Dataset generated by dummyxarray\"\n        Description of the data source\n    driver : str, default \"zarr\"\n        Intake driver to use (zarr, netcdf, xarray, etc.)\n    data_path : str, optional\n        Path to the actual data file. If None, uses template path\n    **kwargs\n        Additional arguments to pass to the driver\n\n    Returns\n    -------\n    str\n        YAML string representing the Intake catalog\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.add_dim(\"time\", 12)\n    &gt;&gt;&gt; ds.add_variable(\"temperature\", dims=[\"time\"], attrs={\"units\": \"K\"})\n    &gt;&gt;&gt; catalog_yaml = ds.to_intake_catalog(\n    ...     name=\"my_dataset\",\n    ...     description=\"Temperature data\",\n    ...     data_path=\"data/my_dataset.zarr\"\n    ... )\n    \"\"\"\n    # Build catalog structure\n    catalog = {\n        \"metadata\": {\n            \"version\": 1,\n            \"description\": f\"Intake catalog for {name}\",\n        }\n    }\n\n    # Add dataset-level parameters if any\n    if hasattr(self, \"attrs\") and self.attrs:\n        catalog[\"metadata\"][\"dataset_attrs\"] = dict(self.attrs)\n\n    # Build sources section\n    sources = {}\n\n    # Default data path template if not provided\n    if data_path is None:\n        data_path = \"{{ CATALOG_DIR }}/\" + name + \".zarr\"\n\n    source_entry = {\n        \"description\": description,\n        \"driver\": driver,\n        \"args\": {\"urlpath\": data_path, **kwargs},\n    }\n\n    # Add metadata about the dataset structure\n    source_metadata = {}\n\n    # Add dimension information\n    if hasattr(self, \"dims\") and self.dims:\n        source_metadata[\"dimensions\"] = dict(self.dims)\n\n    # Add coordinate information\n    if hasattr(self, \"coords\") and self.coords:\n        coord_info = {}\n        for coord_name, coord_arr in self.coords.items():\n            coord_info[coord_name] = {\n                \"dims\": coord_arr.dims,\n                \"attrs\": dict(coord_arr.attrs) if coord_arr.attrs else {},\n            }\n            if coord_arr.encoding:\n                encoding = dict(coord_arr.encoding)\n                # Convert tuples to lists for YAML compatibility\n                for key, value in encoding.items():\n                    if isinstance(value, tuple):\n                        encoding[key] = list(value)\n                coord_info[coord_name][\"encoding\"] = encoding\n        source_metadata[\"coordinates\"] = coord_info\n\n    # Add variable information\n    if hasattr(self, \"variables\") and self.variables:\n        var_info = {}\n        for var_name, var_arr in self.variables.items():\n            var_info[var_name] = {\n                \"dims\": var_arr.dims,\n                \"attrs\": dict(var_arr.attrs) if var_arr.attrs else {},\n            }\n            if var_arr.encoding:\n                encoding = dict(var_arr.encoding)\n                # Convert tuples to lists for YAML compatibility\n                for key, value in encoding.items():\n                    if isinstance(value, tuple):\n                        encoding[key] = list(value)\n                var_info[var_name][\"encoding\"] = encoding\n        source_metadata[\"variables\"] = var_info\n\n    if source_metadata:\n        source_entry[\"metadata\"] = source_metadata\n\n    sources[name] = source_entry\n    catalog[\"sources\"] = sources\n\n    return yaml.dump(catalog, sort_keys=False)\n</code></pre>"},{"location":"api/mixins/io/#dummyxarray.io.IOMixin.save_intake_catalog","title":"save_intake_catalog","text":"<pre><code>save_intake_catalog(\n    path,\n    name=\"dataset\",\n    description=\"Dataset generated by dummyxarray\",\n    driver=\"zarr\",\n    data_path=None,\n    **kwargs\n)\n</code></pre> <p>Save Intake catalog to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output file path for the catalog YAML</p> required <code>name</code> <code>str</code> <p>Name for the data source in the catalog</p> <code>\"dataset\"</code> <code>description</code> <code>str</code> <p>Description of the data source</p> <code>\"Dataset generated by dummyxarray\"</code> <code>driver</code> <code>str</code> <p>Intake driver to use (zarr, netcdf, xarray, etc.)</p> <code>\"zarr\"</code> <code>data_path</code> <code>str</code> <p>Path to the actual data file. If None, uses template path</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the driver</p> <code>{}</code> Source code in <code>src/dummyxarray/io.py</code> <pre><code>def save_intake_catalog(\n    self,\n    path,\n    name=\"dataset\",\n    description=\"Dataset generated by dummyxarray\",\n    driver=\"zarr\",\n    data_path=None,\n    **kwargs,\n):\n    \"\"\"\n    Save Intake catalog to a YAML file.\n\n    Parameters\n    ----------\n    path : str\n        Output file path for the catalog YAML\n    name : str, default \"dataset\"\n        Name for the data source in the catalog\n    description : str, default \"Dataset generated by dummyxarray\"\n        Description of the data source\n    driver : str, default \"zarr\"\n        Intake driver to use (zarr, netcdf, xarray, etc.)\n    data_path : str, optional\n        Path to the actual data file. If None, uses template path\n    **kwargs\n        Additional arguments to pass to the driver\n    \"\"\"\n    catalog_yaml = self.to_intake_catalog(\n        name=name, description=description, driver=driver, data_path=data_path, **kwargs\n    )\n\n    with open(path, \"w\") as f:\n        f.write(catalog_yaml)\n</code></pre>"},{"location":"api/mixins/io/#dummyxarray.io.IOMixin.from_intake_catalog","title":"from_intake_catalog  <code>classmethod</code>","text":"<pre><code>from_intake_catalog(catalog_source, source_name=None)\n</code></pre> <p>Create a DummyDataset from an Intake catalog.</p> <p>Parameters:</p> Name Type Description Default <code>catalog_source</code> <code>str or dict</code> <p>Either a path to a YAML catalog file or a dictionary containing the catalog structure</p> required <code>source_name</code> <code>str</code> <p>Name of the source to use from the catalog. If None and catalog contains only one source, that source will be used automatically.</p> <code>None</code> <p>Returns:</p> Type Description <code>DummyDataset</code> <p>A new DummyDataset with the structure from the catalog</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If catalog format is invalid or source_name is not found</p> <code>FileNotFoundError</code> <p>If catalog_source is a file path that doesn't exist</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load from file\n&gt;&gt;&gt; ds = DummyDataset.from_intake_catalog(\"catalog.yaml\", \"climate_data\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Load from dictionary\n&gt;&gt;&gt; catalog_dict = yaml.safe_load(catalog_yaml)\n&gt;&gt;&gt; ds = DummyDataset.from_intake_catalog(catalog_dict, \"climate_data\")\n</code></pre> Source code in <code>src/dummyxarray/io.py</code> <pre><code>@classmethod\ndef from_intake_catalog(cls, catalog_source, source_name=None):\n    \"\"\"\n    Create a DummyDataset from an Intake catalog.\n\n    Parameters\n    ----------\n    catalog_source : str or dict\n        Either a path to a YAML catalog file or a dictionary containing\n        the catalog structure\n    source_name : str, optional\n        Name of the source to use from the catalog. If None and catalog\n        contains only one source, that source will be used automatically.\n\n    Returns\n    -------\n    DummyDataset\n        A new DummyDataset with the structure from the catalog\n\n    Raises\n    ------\n    ValueError\n        If catalog format is invalid or source_name is not found\n    FileNotFoundError\n        If catalog_source is a file path that doesn't exist\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Load from file\n    &gt;&gt;&gt; ds = DummyDataset.from_intake_catalog(\"catalog.yaml\", \"climate_data\")\n\n    &gt;&gt;&gt; # Load from dictionary\n    &gt;&gt;&gt; catalog_dict = yaml.safe_load(catalog_yaml)\n    &gt;&gt;&gt; ds = DummyDataset.from_intake_catalog(catalog_dict, \"climate_data\")\n    \"\"\"\n    from pathlib import Path\n\n    import yaml\n\n    # Load catalog\n    if isinstance(catalog_source, (str, Path)):\n        # Load from file\n        try:\n            with open(catalog_source) as f:\n                catalog = yaml.safe_load(f)\n        except FileNotFoundError as err:\n            raise FileNotFoundError(f\"Catalog file not found: {catalog_source}\") from err\n    elif isinstance(catalog_source, dict):\n        # Use provided dictionary\n        catalog = catalog_source\n    else:\n        raise ValueError(\"catalog_source must be a file path or dictionary\")\n\n    # Validate catalog structure\n    if not isinstance(catalog, dict):\n        raise ValueError(\"Catalog must be a dictionary\")\n\n    if \"sources\" not in catalog:\n        raise ValueError(\"Catalog must contain 'sources' section\")\n\n    sources = catalog[\"sources\"]\n    if not sources:\n        raise ValueError(\"Catalog sources section cannot be empty\")\n\n    # Determine which source to use\n    if source_name is None:\n        if len(sources) == 1:\n            source_name = list(sources.keys())[0]\n        else:\n            raise ValueError(\n                \"Multiple sources found in catalog. \" \"Please specify source_name explicitly.\"\n            )\n\n    if source_name not in sources:\n        available_sources = list(sources.keys())\n        raise ValueError(\n            f\"Source '{source_name}' not found in catalog. \"\n            f\"Available sources: {available_sources}\"\n        )\n\n    source = sources[source_name]\n\n    # Create new DummyDataset\n    ds = cls()\n\n    # Extract dataset attributes from catalog metadata if available\n    if \"metadata\" in catalog:\n        catalog_metadata = catalog[\"metadata\"]\n        if \"dataset_attrs\" in catalog_metadata:\n            ds.attrs.update(catalog_metadata[\"dataset_attrs\"])\n\n    # Extract source metadata if available\n    source_metadata = source.get(\"metadata\", {})\n\n    # Add dimensions\n    if \"dimensions\" in source_metadata:\n        for dim_name, dim_size in source_metadata[\"dimensions\"].items():\n            ds.add_dim(dim_name, dim_size)\n\n    # Add coordinates\n    if \"coordinates\" in source_metadata:\n        for coord_name, coord_info in source_metadata[\"coordinates\"].items():\n            coord_attrs = coord_info.get(\"attrs\", {})\n            coord_encoding = coord_info.get(\"encoding\", {})\n            ds.add_coord(\n                coord_name, dims=coord_info[\"dims\"], attrs=coord_attrs, encoding=coord_encoding\n            )\n\n    # Add variables\n    if \"variables\" in source_metadata:\n        for var_name, var_info in source_metadata[\"variables\"].items():\n            var_attrs = var_info.get(\"attrs\", {})\n            var_encoding = var_info.get(\"encoding\", {})\n            ds.add_variable(\n                var_name, dims=var_info[\"dims\"], attrs=var_attrs, encoding=var_encoding\n            )\n\n    # Add catalog-specific attributes\n    ds.attrs.update(\n        {\n            \"intake_catalog_source\": source_name,\n            \"intake_driver\": source.get(\"driver\", \"unknown\"),\n            \"intake_description\": source.get(\"description\", \"\"),\n        }\n    )\n\n    return ds\n</code></pre>"},{"location":"api/mixins/io/#dummyxarray.io.IOMixin.load_intake_catalog","title":"load_intake_catalog  <code>classmethod</code>","text":"<pre><code>load_intake_catalog(path, source_name=None)\n</code></pre> <p>Load a DummyDataset from an Intake catalog YAML file.</p> <p>This is a convenience method that wraps from_intake_catalog() for file loading.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the catalog YAML file</p> required <code>source_name</code> <code>str</code> <p>Name of the source to use from the catalog</p> <code>None</code> <p>Returns:</p> Type Description <code>DummyDataset</code> <p>A new DummyDataset with the structure from the catalog</p> Source code in <code>src/dummyxarray/io.py</code> <pre><code>@classmethod\ndef load_intake_catalog(cls, path, source_name=None):\n    \"\"\"\n    Load a DummyDataset from an Intake catalog YAML file.\n\n    This is a convenience method that wraps from_intake_catalog() for file loading.\n\n    Parameters\n    ----------\n    path : str\n        Path to the catalog YAML file\n    source_name : str, optional\n        Name of the source to use from the catalog\n\n    Returns\n    -------\n    DummyDataset\n        A new DummyDataset with the structure from the catalog\n    \"\"\"\n    return cls.from_intake_catalog(path, source_name)\n</code></pre>"},{"location":"api/mixins/provenance/","title":"ProvenanceMixin","text":"<p>Tracks what changed in each operation (added, removed, modified, renamed).</p>"},{"location":"api/mixins/provenance/#overview","title":"Overview","text":"<p>The <code>ProvenanceMixin</code> extends history tracking by recording detailed provenance information:</p> <ul> <li>Added items - New dimensions, coordinates, or variables</li> <li>Removed items - Deleted items</li> <li>Modified items - Changed attributes or data</li> <li>Renamed items - Name changes with old \u2192 new mapping</li> </ul>"},{"location":"api/mixins/provenance/#key-methods","title":"Key Methods","text":"<ul> <li><code>get_provenance()</code> - Get provenance information for all operations</li> <li><code>visualize_provenance(format)</code> - Visualize as 'compact' or 'detailed'</li> </ul>"},{"location":"api/mixins/provenance/#usage","title":"Usage","text":"<pre><code>ds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.rename_dims(time=\"t\")\n\n# Get provenance\nprovenance = ds.get_provenance()\nfor op in provenance:\n    print(f\"{op['func']}: {op['provenance']}\")\n</code></pre>"},{"location":"api/mixins/provenance/#api-reference","title":"API Reference","text":"<p>Mixin providing provenance tracking capabilities.</p> Source code in <code>src/dummyxarray/provenance.py</code> <pre><code>class ProvenanceMixin:\n    \"\"\"Mixin providing provenance tracking capabilities.\"\"\"\n\n    def get_provenance(self, operation_index=None):\n        \"\"\"\n        Get provenance information showing what changed in each operation.\n\n        Parameters\n        ----------\n        operation_index : int, optional\n            If provided, return provenance for a specific operation.\n            Otherwise, return provenance for all operations.\n\n        Returns\n        -------\n        dict or list of dict\n            Provenance information showing changes\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.assign_attrs(units='degC')\n        &gt;&gt;&gt; ds.assign_attrs(units='K')  # Overwrites previous value\n        &gt;&gt;&gt; prov = ds.get_provenance()\n        &gt;&gt;&gt; prov[2]['provenance']['modified']['units']\n        {'before': 'degC', 'after': 'K'}\n        \"\"\"\n        history = self.get_history(include_provenance=True)\n\n        if operation_index is not None:\n            if 0 &lt;= operation_index &lt; len(history):\n                return history[operation_index].get(\"provenance\", {})\n            else:\n                raise IndexError(f\"Operation index {operation_index} out of range\")\n\n        # Return all provenance information\n        return [\n            {\n                \"index\": i,\n                \"func\": op[\"func\"],\n                \"provenance\": op.get(\"provenance\", {}),\n            }\n            for i, op in enumerate(history)\n            if \"provenance\" in op\n        ]\n\n    def visualize_provenance(self, compact=False):\n        \"\"\"\n        Visualize provenance information showing what changed.\n\n        Parameters\n        ----------\n        compact : bool, optional\n            Use compact representation (default: False)\n\n        Returns\n        -------\n        str\n            Formatted provenance visualization\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds = DummyDataset()\n        &gt;&gt;&gt; ds.assign_attrs(units='degC', title='Test')\n        &gt;&gt;&gt; ds.assign_attrs(units='K')  # Overwrites units\n        &gt;&gt;&gt; print(ds.visualize_provenance())\n        Provenance: Dataset Changes\n        ============================\n\n        Operation 1: assign_attrs\n          Modified attributes:\n            units: None \u2192 'degC'\n            title: None \u2192 'Test'\n\n        Operation 2: assign_attrs\n          Modified attributes:\n            units: 'degC' \u2192 'K'\n        \"\"\"\n        history = self.get_history(include_provenance=True)\n\n        if compact:\n            lines = []\n            for i, op in enumerate(history):\n                if \"provenance\" not in op:\n                    continue\n                prov = op[\"provenance\"]\n                changes = []\n                if \"renamed\" in prov:\n                    for old, new in prov[\"renamed\"].items():\n                        changes.append(f\"renamed: {old} \u2192 {new}\")\n                if \"added\" in prov:\n                    changes.append(f\"added: {', '.join(prov['added'])}\")\n                if \"removed\" in prov:\n                    changes.append(f\"removed: {', '.join(prov['removed'])}\")\n                if \"modified\" in prov:\n                    for key, change in prov[\"modified\"].items():\n                        if isinstance(change, dict) and \"before\" in change:\n                            changes.append(f\"{key}: {change['before']} \u2192 {change['after']}\")\n                        else:\n                            changes.append(f\"{key}: modified\")\n                if changes:\n                    lines.append(f\"{i}. {op['func']}: {'; '.join(changes)}\")\n            return \"\\n\".join(lines) if lines else \"No changes recorded\"\n        else:\n            lines = [\"Provenance: Dataset Changes\", \"=\" * 28, \"\"]\n\n            has_changes = False\n            for i, op in enumerate(history):\n                if \"provenance\" not in op:\n                    continue\n\n                has_changes = True\n                prov = op[\"provenance\"]\n                lines.append(f\"Operation {i}: {op['func']}\")\n\n                if \"renamed\" in prov:\n                    lines.append(\"  Renamed:\")\n                    for old, new in prov[\"renamed\"].items():\n                        lines.append(f\"    {old} \u2192 {new}\")\n\n                if \"added\" in prov:\n                    lines.append(f\"  Added: {', '.join(prov['added'])}\")\n\n                if \"removed\" in prov:\n                    lines.append(f\"  Removed: {', '.join(prov['removed'])}\")\n\n                if \"modified\" in prov:\n                    lines.append(\"  Modified:\")\n                    for key, change in prov[\"modified\"].items():\n                        if isinstance(change, dict) and \"before\" in change:\n                            before = (\n                                repr(change[\"before\"]) if change[\"before\"] is not None else \"None\"\n                            )\n                            after = repr(change[\"after\"])\n                            lines.append(f\"    {key}: {before} \u2192 {after}\")\n                        else:\n                            # Nested changes (e.g., for coords/variables)\n                            lines.append(f\"    {key}:\")\n                            for subkey, subchange in change.items():\n                                before = repr(subchange[\"before\"])\n                                after = repr(subchange[\"after\"])\n                                lines.append(f\"      {subkey}: {before} \u2192 {after}\")\n\n                lines.append(\"\")\n\n            if not has_changes:\n                return \"No changes recorded\"\n\n            return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/mixins/provenance/#dummyxarray.provenance.ProvenanceMixin.get_provenance","title":"get_provenance","text":"<pre><code>get_provenance(operation_index=None)\n</code></pre> <p>Get provenance information showing what changed in each operation.</p> <p>Parameters:</p> Name Type Description Default <code>operation_index</code> <code>int</code> <p>If provided, return provenance for a specific operation. Otherwise, return provenance for all operations.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict or list of dict</code> <p>Provenance information showing changes</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.assign_attrs(units='degC')\n&gt;&gt;&gt; ds.assign_attrs(units='K')  # Overwrites previous value\n&gt;&gt;&gt; prov = ds.get_provenance()\n&gt;&gt;&gt; prov[2]['provenance']['modified']['units']\n{'before': 'degC', 'after': 'K'}\n</code></pre> Source code in <code>src/dummyxarray/provenance.py</code> <pre><code>def get_provenance(self, operation_index=None):\n    \"\"\"\n    Get provenance information showing what changed in each operation.\n\n    Parameters\n    ----------\n    operation_index : int, optional\n        If provided, return provenance for a specific operation.\n        Otherwise, return provenance for all operations.\n\n    Returns\n    -------\n    dict or list of dict\n        Provenance information showing changes\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.assign_attrs(units='degC')\n    &gt;&gt;&gt; ds.assign_attrs(units='K')  # Overwrites previous value\n    &gt;&gt;&gt; prov = ds.get_provenance()\n    &gt;&gt;&gt; prov[2]['provenance']['modified']['units']\n    {'before': 'degC', 'after': 'K'}\n    \"\"\"\n    history = self.get_history(include_provenance=True)\n\n    if operation_index is not None:\n        if 0 &lt;= operation_index &lt; len(history):\n            return history[operation_index].get(\"provenance\", {})\n        else:\n            raise IndexError(f\"Operation index {operation_index} out of range\")\n\n    # Return all provenance information\n    return [\n        {\n            \"index\": i,\n            \"func\": op[\"func\"],\n            \"provenance\": op.get(\"provenance\", {}),\n        }\n        for i, op in enumerate(history)\n        if \"provenance\" in op\n    ]\n</code></pre>"},{"location":"api/mixins/provenance/#dummyxarray.provenance.ProvenanceMixin.visualize_provenance","title":"visualize_provenance","text":"<pre><code>visualize_provenance(compact=False)\n</code></pre> <p>Visualize provenance information showing what changed.</p> <p>Parameters:</p> Name Type Description Default <code>compact</code> <code>bool</code> <p>Use compact representation (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted provenance visualization</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DummyDataset()\n&gt;&gt;&gt; ds.assign_attrs(units='degC', title='Test')\n&gt;&gt;&gt; ds.assign_attrs(units='K')  # Overwrites units\n&gt;&gt;&gt; print(ds.visualize_provenance())\nProvenance: Dataset Changes\n============================\n</code></pre> <p>Operation 1: assign_attrs   Modified attributes:     units: None \u2192 'degC'     title: None \u2192 'Test'</p> <p>Operation 2: assign_attrs   Modified attributes:     units: 'degC' \u2192 'K'</p> Source code in <code>src/dummyxarray/provenance.py</code> <pre><code>def visualize_provenance(self, compact=False):\n    \"\"\"\n    Visualize provenance information showing what changed.\n\n    Parameters\n    ----------\n    compact : bool, optional\n        Use compact representation (default: False)\n\n    Returns\n    -------\n    str\n        Formatted provenance visualization\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds = DummyDataset()\n    &gt;&gt;&gt; ds.assign_attrs(units='degC', title='Test')\n    &gt;&gt;&gt; ds.assign_attrs(units='K')  # Overwrites units\n    &gt;&gt;&gt; print(ds.visualize_provenance())\n    Provenance: Dataset Changes\n    ============================\n\n    Operation 1: assign_attrs\n      Modified attributes:\n        units: None \u2192 'degC'\n        title: None \u2192 'Test'\n\n    Operation 2: assign_attrs\n      Modified attributes:\n        units: 'degC' \u2192 'K'\n    \"\"\"\n    history = self.get_history(include_provenance=True)\n\n    if compact:\n        lines = []\n        for i, op in enumerate(history):\n            if \"provenance\" not in op:\n                continue\n            prov = op[\"provenance\"]\n            changes = []\n            if \"renamed\" in prov:\n                for old, new in prov[\"renamed\"].items():\n                    changes.append(f\"renamed: {old} \u2192 {new}\")\n            if \"added\" in prov:\n                changes.append(f\"added: {', '.join(prov['added'])}\")\n            if \"removed\" in prov:\n                changes.append(f\"removed: {', '.join(prov['removed'])}\")\n            if \"modified\" in prov:\n                for key, change in prov[\"modified\"].items():\n                    if isinstance(change, dict) and \"before\" in change:\n                        changes.append(f\"{key}: {change['before']} \u2192 {change['after']}\")\n                    else:\n                        changes.append(f\"{key}: modified\")\n            if changes:\n                lines.append(f\"{i}. {op['func']}: {'; '.join(changes)}\")\n        return \"\\n\".join(lines) if lines else \"No changes recorded\"\n    else:\n        lines = [\"Provenance: Dataset Changes\", \"=\" * 28, \"\"]\n\n        has_changes = False\n        for i, op in enumerate(history):\n            if \"provenance\" not in op:\n                continue\n\n            has_changes = True\n            prov = op[\"provenance\"]\n            lines.append(f\"Operation {i}: {op['func']}\")\n\n            if \"renamed\" in prov:\n                lines.append(\"  Renamed:\")\n                for old, new in prov[\"renamed\"].items():\n                    lines.append(f\"    {old} \u2192 {new}\")\n\n            if \"added\" in prov:\n                lines.append(f\"  Added: {', '.join(prov['added'])}\")\n\n            if \"removed\" in prov:\n                lines.append(f\"  Removed: {', '.join(prov['removed'])}\")\n\n            if \"modified\" in prov:\n                lines.append(\"  Modified:\")\n                for key, change in prov[\"modified\"].items():\n                    if isinstance(change, dict) and \"before\" in change:\n                        before = (\n                            repr(change[\"before\"]) if change[\"before\"] is not None else \"None\"\n                        )\n                        after = repr(change[\"after\"])\n                        lines.append(f\"    {key}: {before} \u2192 {after}\")\n                    else:\n                        # Nested changes (e.g., for coords/variables)\n                        lines.append(f\"    {key}:\")\n                        for subkey, subchange in change.items():\n                            before = repr(subchange[\"before\"])\n                            after = repr(subchange[\"after\"])\n                            lines.append(f\"      {subkey}: {before} \u2192 {after}\")\n\n            lines.append(\"\")\n\n        if not has_changes:\n            return \"No changes recorded\"\n\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/mixins/validation/","title":"ValidationMixin","text":"<p>Provides dataset structure validation functionality.</p>"},{"location":"api/mixins/validation/#overview","title":"Overview","text":"<p>The <code>ValidationMixin</code> validates dataset structure to catch errors early:</p> <ul> <li>Dimension checks - Ensure all referenced dimensions exist</li> <li>Shape validation - Verify data shapes match dimension sizes</li> <li>Coordinate checks - Validate coordinate presence (strict mode)</li> </ul>"},{"location":"api/mixins/validation/#key-methods","title":"Key Methods","text":"<ul> <li><code>validate(strict=False)</code> - Validate dataset structure</li> <li><code>_infer_and_register_dims(name, dims, data)</code> - Internal dimension inference</li> </ul>"},{"location":"api/mixins/validation/#validation-checks","title":"Validation Checks","text":""},{"location":"api/mixins/validation/#basic-validation","title":"Basic Validation","text":"<ul> <li>Unknown dimensions referenced by variables/coordinates</li> <li>Shape mismatches between data and declared dimensions</li> </ul>"},{"location":"api/mixins/validation/#strict-mode","title":"Strict Mode","text":"<p>When <code>strict=True</code>:</p> <ul> <li>All dimensions must have corresponding coordinates</li> <li>Raises <code>ValueError</code> on any validation failure</li> </ul>"},{"location":"api/mixins/validation/#usage","title":"Usage","text":"<pre><code>ds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.add_variable(\"temp\", dims=[\"time\", \"lat\"])  # Error: 'lat' not defined\n\n# Validate\ntry:\n    ds.validate()\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n\n# Strict validation\nds.validate(strict=True)  # Raises error if issues found\n</code></pre>"},{"location":"api/mixins/validation/#api-reference","title":"API Reference","text":"<p>Mixin providing dataset validation capabilities.</p> Source code in <code>src/dummyxarray/validation.py</code> <pre><code>class ValidationMixin:\n    \"\"\"Mixin providing dataset validation capabilities.\"\"\"\n\n    def validate(self, strict_coords=False):\n        \"\"\"\n        Validate the entire dataset structure.\n\n        Parameters\n        ----------\n        strict_coords : bool, default False\n            If True, require that all variable dimensions have corresponding coordinates\n\n        Raises\n        ------\n        ValueError\n            If validation fails\n        \"\"\"\n        errors = []\n\n        # 1. Dimensions must be known\n        all_dims = set(self.dims.keys())\n\n        for name, arr in {**self.coords, **self.variables}.items():\n            if arr.dims is None:\n                continue\n            for d in arr.dims:\n                if d not in all_dims:\n                    errors.append(f\"{name}: Unknown dimension '{d}'.\")\n\n        # 2. Data shapes must match dims\n        for name, arr in {**self.coords, **self.variables}.items():\n            if arr.data is not None and arr.dims is not None:\n                shape = np.asarray(arr.data).shape\n                dim_sizes = [self.dims[d] for d in arr.dims]\n                if tuple(dim_sizes) != shape:\n                    errors.append(f\"{name}: Data shape {shape} does not match dims {dim_sizes}.\")\n\n        # 3. Variables reference coords?\n        if strict_coords:\n            coord_names = set(self.coords.keys())\n            for name, arr in self.variables.items():\n                if arr.dims:\n                    for d in arr.dims:\n                        if d not in coord_names:\n                            errors.append(f\"{name}: Missing coordinate for dimension '{d}'.\")\n\n        if errors:\n            raise ValueError(\"Dataset validation failed:\\n\" + \"\\n\".join(errors))\n\n    def _infer_and_register_dims(self, arr):\n        \"\"\"\n        Infer dimension sizes from data and register them.\n\n        Parameters\n        ----------\n        arr : DummyArray\n            Array to infer dimensions from\n\n        Raises\n        ------\n        ValueError\n            If dimension sizes conflict\n        \"\"\"\n        inferred = arr.infer_dims_from_data()\n\n        for dim, size in inferred.items():\n            if dim in self.dims:\n                if self.dims[dim] != size:\n                    raise ValueError(\n                        f\"Dimension mismatch for '{dim}': existing={self.dims[dim]} new={size}\"\n                    )\n            else:\n                self.dims[dim] = size\n</code></pre>"},{"location":"api/mixins/validation/#dummyxarray.validation.ValidationMixin.validate","title":"validate","text":"<pre><code>validate(strict_coords=False)\n</code></pre> <p>Validate the entire dataset structure.</p> <p>Parameters:</p> Name Type Description Default <code>strict_coords</code> <code>bool</code> <p>If True, require that all variable dimensions have corresponding coordinates</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If validation fails</p> Source code in <code>src/dummyxarray/validation.py</code> <pre><code>def validate(self, strict_coords=False):\n    \"\"\"\n    Validate the entire dataset structure.\n\n    Parameters\n    ----------\n    strict_coords : bool, default False\n        If True, require that all variable dimensions have corresponding coordinates\n\n    Raises\n    ------\n    ValueError\n        If validation fails\n    \"\"\"\n    errors = []\n\n    # 1. Dimensions must be known\n    all_dims = set(self.dims.keys())\n\n    for name, arr in {**self.coords, **self.variables}.items():\n        if arr.dims is None:\n            continue\n        for d in arr.dims:\n            if d not in all_dims:\n                errors.append(f\"{name}: Unknown dimension '{d}'.\")\n\n    # 2. Data shapes must match dims\n    for name, arr in {**self.coords, **self.variables}.items():\n        if arr.data is not None and arr.dims is not None:\n            shape = np.asarray(arr.data).shape\n            dim_sizes = [self.dims[d] for d in arr.dims]\n            if tuple(dim_sizes) != shape:\n                errors.append(f\"{name}: Data shape {shape} does not match dims {dim_sizes}.\")\n\n    # 3. Variables reference coords?\n    if strict_coords:\n        coord_names = set(self.coords.keys())\n        for name, arr in self.variables.items():\n            if arr.dims:\n                for d in arr.dims:\n                    if d not in coord_names:\n                        errors.append(f\"{name}: Missing coordinate for dimension '{d}'.\")\n\n    if errors:\n        raise ValueError(\"Dataset validation failed:\\n\" + \"\\n\".join(errors))\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#using-pixi-recommended","title":"Using Pixi (Recommended)","text":"<p>Pixi is a fast package manager that handles both Python and system dependencies.</p> <pre><code># Clone the repository\ngit clone https://github.com/siligam/dummyxarray.git\ncd dummyxarray\n\n# Install dependencies with pixi\npixi install\n\n# Run tests to verify installation\npixi run test\n</code></pre>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<p>If you prefer using pip, you can install the dependencies manually:</p> <pre><code># Clone the repository\ngit clone https://github.com/siligam/dummyxarray.git\ncd dummyxarray\n\n# Install dependencies\npip install -r requirements.txt\n\n# Install the package in development mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Test your installation by running:</p> <pre><code>from dummyxarray import DummyDataset\n\nds = DummyDataset()\nprint(\"Installation successful!\")\n</code></pre>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.9</li> <li>numpy &gt;= 1.20.0</li> <li>xarray &gt;= 0.19.0</li> <li>pyyaml &gt;= 5.4.0</li> <li>zarr &gt;= 2.10.0</li> </ul>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development, you'll also want pytest:</p> <pre><code>pixi install  # pytest is already included\n</code></pre> <p>Or with pip:</p> <pre><code>pip install pytest\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will walk you through the basic usage of Dummy Xarray.</p>"},{"location":"getting-started/quickstart/#creating-your-first-dataset","title":"Creating Your First Dataset","text":"<pre><code>from dummyxarray import DummyDataset\n\n# Create an empty dataset\nds = DummyDataset()\n\n# Set global attributes (xarray-compatible API)\nds.assign_attrs(\n    title=\"My First Dataset\",\n    institution=\"Research Institute\",\n    experiment=\"test_run\"\n)\n\n# Add dimensions\nds.add_dim(\"time\", 12)\nds.add_dim(\"lat\", 64)\nds.add_dim(\"lon\", 128)\n\nprint(\"Dataset created with dimensions:\", ds.dims)\n</code></pre>"},{"location":"getting-started/quickstart/#adding-coordinates","title":"Adding Coordinates","text":"<pre><code>import numpy as np\n\n# Add time coordinate\ntime_data = np.arange(12)\nds.add_coord(\n    \"time\",\n    dims=[\"time\"],\n    data=time_data,\n    attrs={\"units\": \"months since 2000-01-01\", \"calendar\": \"standard\"}\n)\n\n# Add latitude coordinate\nlat_data = np.linspace(-90, 90, 64)\nds.add_coord(\n    \"lat\",\n    dims=[\"lat\"],\n    data=lat_data,\n    attrs={\"units\": \"degrees_north\", \"standard_name\": \"latitude\"}\n)\n\n# Add longitude coordinate\nlon_data = np.linspace(-180, 180, 128)\nds.add_coord(\n    \"lon\",\n    dims=[\"lon\"],\n    data=lon_data,\n    attrs={\"units\": \"degrees_east\", \"standard_name\": \"longitude\"}\n)\n</code></pre>"},{"location":"getting-started/quickstart/#adding-variables","title":"Adding Variables","text":"<pre><code># Add a temperature variable with data\ntemp_data = np.random.rand(12, 64, 128) * 20 + 273.15\n\nds.add_variable(\n    \"temperature\",\n    dims=[\"time\", \"lat\", \"lon\"],\n    data=temp_data,\n    attrs={\n        \"long_name\": \"Near-Surface Air Temperature\",\n        \"units\": \"K\",\n        \"standard_name\": \"air_temperature\"\n    }\n)\n</code></pre>"},{"location":"getting-started/quickstart/#exporting-to-yaml","title":"Exporting to YAML","text":"<pre><code># View the dataset structure\nprint(ds.to_yaml())\n\n# Save to file\nds.save_yaml(\"my_dataset_spec.yaml\")\n</code></pre>"},{"location":"getting-started/quickstart/#converting-to-xarray","title":"Converting to xarray","text":"<pre><code># Convert to a real xarray.Dataset\nxr_dataset = ds.to_xarray()\nprint(xr_dataset)\n</code></pre>"},{"location":"getting-started/quickstart/#writing-to-zarr","title":"Writing to Zarr","text":"<pre><code># Write directly to Zarr format\nds.to_zarr(\"output.zarr\")\n\n# Load it back with xarray\nimport xarray as xr\nloaded = xr.open_zarr(\"output.zarr\")\n</code></pre>"},{"location":"getting-started/quickstart/#automatic-dimension-inference","title":"Automatic Dimension Inference","text":"<p>If you provide data without specifying dimensions, they will be inferred automatically:</p> <pre><code>ds2 = DummyDataset()\n\n# Dimensions will be auto-generated as dim_0, dim_1, dim_2\ndata = np.random.rand(10, 20, 30)\nds2.add_variable(\"my_var\", data=data)\n\nprint(ds2.dims)  # {'dim_0': 10, 'dim_1': 20, 'dim_2': 30}\n</code></pre>"},{"location":"getting-started/quickstart/#adding-encoding-for-zarrnetcdf","title":"Adding Encoding for Zarr/NetCDF","text":"<p>Specify encoding parameters for compression and chunking:</p> <pre><code>ds.add_variable(\n    \"temperature\",\n    [\"time\", \"lat\", \"lon\"],\n    data=temp_data,\n    attrs={\"long_name\": \"Temperature\", \"units\": \"K\"},\n    encoding={\n        \"dtype\": \"float32\",\n        \"chunks\": (6, 32, 64),\n        \"compressor\": None,  # Can use zarr.Blosc() or similar\n    }\n)\n</code></pre>"},{"location":"getting-started/quickstart/#validation","title":"Validation","text":"<p>Check for dimension mismatches and structural issues:</p> <pre><code># Validate the dataset structure\nds.validate()\n</code></pre>"},{"location":"getting-started/quickstart/#create-from-existing-xarraydataset","title":"Create from Existing xarray.Dataset","text":"<p>Extract metadata from an existing xarray dataset:</p> <pre><code>import xarray as xr\n\n# Load existing dataset and extract metadata\nexisting_ds = xr.open_dataset(\"my_data.nc\")\ndummy_ds = DummyDataset.from_xarray(existing_ds, include_data=False)\n\n# Save as template for reuse\ndummy_ds.save_yaml(\"template.yaml\")\n</code></pre>"},{"location":"getting-started/quickstart/#populate-with-random-data","title":"Populate with Random Data","text":"<p>For testing, automatically generate meaningful random data:</p> <pre><code># Create structure without data\nds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.add_dim(\"lat\", 5)\nds.add_coord(\"lat\", [\"lat\"], attrs={\"units\": \"degrees_north\"})\nds.add_variable(\"temperature\", [\"time\", \"lat\"], \n                attrs={\"units\": \"K\", \"standard_name\": \"air_temperature\"})\n\n# Populate with meaningful random data\nds.populate_with_random_data(seed=42)\n\n# Now has realistic data: temperature in Kelvin, lat from -90 to 90\nprint(ds.variables[\"temperature\"].data.mean())  # ~280 K\n</code></pre>"},{"location":"getting-started/quickstart/#xarray-style-attribute-access","title":"xarray-style Attribute Access","text":"<p>Access coordinates and variables using dot notation:</p> <pre><code># Access coordinates and variables as attributes (like xarray)\nds.time                    # Same as ds.coords['time']\nds.temperature             # Same as ds.variables['temperature']\n\n# Modify via attribute access\nds.time.data = np.arange(10)\nds.time.assign_attrs(standard_name=\"time\", calendar=\"gregorian\")\n\n# Or use dictionary-style access\nds.temperature.attrs[\"cell_methods\"] = \"time: mean\"\n\n# Inspect with rich repr\nprint(ds.time)             # Shows dimensions, shape, dtype, data, attrs\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about validation</li> <li>Explore encoding options</li> <li>Check out the API reference</li> </ul>"},{"location":"user-guide/basic-usage/","title":"Basic Usage","text":"<p>This guide covers the fundamental operations in Dummy Xarray.</p>"},{"location":"user-guide/basic-usage/#creating-a-dataset","title":"Creating a Dataset","text":"<p>Start by importing and creating a <code>DummyDataset</code>:</p> <pre><code>from dummyxarray import DummyDataset\n\nds = DummyDataset()\n</code></pre>"},{"location":"user-guide/basic-usage/#setting-global-attributes","title":"Setting Global Attributes","text":"<p>Global attributes provide metadata about the entire dataset:</p> <pre><code># Use assign_attrs (xarray-compatible API)\nds.assign_attrs(\n    title=\"My Climate Dataset\",\n    institution=\"Research Institute\",\n    source=\"Model v2.0\",\n    Conventions=\"CF-1.8\"\n)\n\n# You can also update attributes later with method chaining\nds.assign_attrs(experiment=\"historical\").assign_attrs(version=\"1.0\")\n\n# Or use set_global_attrs (legacy method, equivalent)\nds.set_global_attrs(experiment=\"historical\")\n</code></pre>"},{"location":"user-guide/basic-usage/#defining-dimensions","title":"Defining Dimensions","text":"<p>Dimensions define the shape of your data:</p> <pre><code>ds.add_dim(\"time\", 365)   # 365 time steps\nds.add_dim(\"lat\", 180)    # 180 latitude points\nds.add_dim(\"lon\", 360)    # 360 longitude points\n\n# Check defined dimensions\nprint(ds.dims)  # {'time': 365, 'lat': 180, 'lon': 360}\n</code></pre>"},{"location":"user-guide/basic-usage/#adding-coordinates","title":"Adding Coordinates","text":"<p>Coordinates are special variables that label dimension values:</p> <pre><code>import numpy as np\n\n# Time coordinate\ntime_values = np.arange(365)\nds.add_coord(\n    \"time\",\n    dims=[\"time\"],\n    data=time_values,\n    attrs={\n        \"units\": \"days since 2020-01-01\",\n        \"calendar\": \"standard\",\n        \"long_name\": \"time\"\n    }\n)\n\n# Latitude coordinate\nlat_values = np.linspace(-89.5, 89.5, 180)\nds.add_coord(\n    \"lat\",\n    dims=[\"lat\"],\n    data=lat_values,\n    attrs={\n        \"units\": \"degrees_north\",\n        \"standard_name\": \"latitude\",\n        \"long_name\": \"latitude\"\n    }\n)\n\n# Longitude coordinate\nlon_values = np.linspace(0.5, 359.5, 360)\nds.add_coord(\n    \"lon\",\n    dims=[\"lon\"],\n    data=lon_values,\n    attrs={\n        \"units\": \"degrees_east\",\n        \"standard_name\": \"longitude\",\n        \"long_name\": \"longitude\"\n    }\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#adding-variables","title":"Adding Variables","text":"<p>Variables contain your actual data:</p> <pre><code># Create some sample data\ntemperature_data = np.random.rand(365, 180, 360) * 30 + 273.15\n\nds.add_variable(\n    \"temperature\",\n    dims=[\"time\", \"lat\", \"lon\"],\n    data=temperature_data,\n    attrs={\n        \"long_name\": \"Near-Surface Air Temperature\",\n        \"units\": \"K\",\n        \"standard_name\": \"air_temperature\",\n        \"cell_methods\": \"time: mean\"\n    }\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#working-without-data","title":"Working Without Data","text":"<p>You can define the structure without providing data initially:</p> <pre><code>ds = DummyDataset()\nds.add_dim(\"time\", 12)\nds.add_dim(\"lat\", 64)\n\n# Define variable structure without data\nds.add_variable(\n    \"precipitation\",\n    dims=[\"time\", \"lat\"],\n    attrs={\n        \"units\": \"kg m-2 s-1\",\n        \"long_name\": \"Precipitation Flux\"\n    }\n)\n\n# Export specification\nds.save_yaml(\"dataset_template.yaml\")\n\n# Later, load and add data\nds_loaded = DummyDataset.load_yaml(\"dataset_template.yaml\")\nprecip_data = np.random.rand(12, 64)\nds_loaded.variables[\"precipitation\"].data = precip_data\n</code></pre>"},{"location":"user-guide/basic-usage/#xarray-style-attribute-access","title":"xarray-style Attribute Access","text":"<p>Access coordinates and variables using dot notation, just like in xarray:</p> <pre><code># Access coordinates and variables as attributes\nds.time                    # Same as ds.coords['time']\nds.temperature             # Same as ds.variables['temperature']\nds.lat                     # Same as ds.coords['lat']\n\n# Modify data via attribute access\nds.time.data = np.arange(10)\n\n# Assign attributes using xarray-compatible API\nds.time.assign_attrs(standard_name=\"time\", calendar=\"gregorian\")\n\n# Or use dictionary-style access\nds.temperature.attrs[\"cell_methods\"] = \"time: mean\"\n\n# Inspect with rich repr\nprint(ds.time)\n# Output:\n# &lt;dummyxarray.DummyArray&gt;\n# Dimensions: (time)\n# Shape: (10,)\n# dtype: int64\n# Data: [0 1 2 3 4 5 6 7 8 9]\n# Attributes:\n#     units: days since 2020-01-01\n#     standard_name: time\n\n# View the full dataset\nprint(ds)\n# Shows dimensions, coordinates, variables with data indicators (\u2713/\u2717)\n</code></pre> <p>Note: Coordinates take precedence over variables if both have the same name (just like xarray).</p>"},{"location":"user-guide/basic-usage/#populating-with-random-data","title":"Populating with Random Data","text":"<p>For testing purposes, you can automatically populate all coordinates and variables with meaningful random data:</p> <pre><code>ds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.add_dim(\"lat\", 5)\n\nds.add_coord(\"time\", [\"time\"], attrs={\"units\": \"days\"})\nds.add_coord(\"lat\", [\"lat\"], attrs={\"units\": \"degrees_north\"})\nds.add_variable(\"temperature\", [\"time\", \"lat\"], \n                attrs={\"units\": \"K\", \"standard_name\": \"air_temperature\"})\n\n# Populate with meaningful random data\nds.populate_with_random_data(seed=42)  # seed for reproducibility\n\n# Now all coords and variables have realistic data\nprint(ds.coords[\"lat\"].data)  # e.g., [-90, -45, 0, 45, 90]\nprint(ds.variables[\"temperature\"].data.mean())  # e.g., ~280 K\n</code></pre> <p>The <code>populate_with_random_data()</code> method generates data based on metadata hints: - Temperature: 250-310 K - Precipitation: 0-10 mm/day - Pressure: 500-1050 hPa - Latitude: -90 to 90 - Longitude: -180 to 180 - Time: Sequential integers</p>"},{"location":"user-guide/basic-usage/#viewing-dataset-structure","title":"Viewing Dataset Structure","text":"<pre><code># As YAML\nprint(ds.to_yaml())\n\n# As JSON\nprint(ds.to_json())\n\n# As dictionary\nspec_dict = ds.to_dict()\n\n# Interactive repr (shows data presence indicators)\nprint(ds)\n</code></pre>"},{"location":"user-guide/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about validation</li> <li>Explore encoding options</li> <li>See how to export to YAML</li> </ul>"},{"location":"user-guide/cf-compliance/","title":"CF Compliance","text":"<p>dummyxarray provides comprehensive support for CF (Climate and Forecast) conventions, helping you create metadata-compliant datasets.</p>"},{"location":"user-guide/cf-compliance/#overview","title":"Overview","text":"<p>The CF conventions define standard metadata for climate and forecast data. dummyxarray helps you:</p> <ul> <li>Detect axis types automatically (X, Y, Z, T)</li> <li>Validate datasets against CF conventions</li> <li>Set standard attributes following CF guidelines</li> <li>Check dimension ordering (recommended: T, Z, Y, X)</li> </ul>"},{"location":"user-guide/cf-compliance/#axis-detection","title":"Axis Detection","text":""},{"location":"user-guide/cf-compliance/#automatic-inference","title":"Automatic Inference","text":"<p>dummyxarray can automatically infer axis types based on: - Coordinate names (time, lat, lon, lev, etc.) - Units attributes (degrees_north, degrees_east, days since, etc.) - Standard_name attributes (latitude, longitude, time, etc.)</p> <pre><code>from dummyxarray import DummyDataset\n\nds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.add_dim(\"lat\", 64)\nds.add_dim(\"lon\", 128)\n\n# Add coordinates with CF-compliant attributes\nds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\nds.add_coord(\"lat\", dims=[\"lat\"], attrs={\"units\": \"degrees_north\"})\nds.add_coord(\"lon\", dims=[\"lon\"], attrs={\"units\": \"degrees_east\"})\n\n# Infer axis types\naxes = ds.infer_axis()\nprint(axes)\n# Output: {'time': 'T', 'lat': 'Y', 'lon': 'X'}\n</code></pre>"},{"location":"user-guide/cf-compliance/#setting-axis-attributes","title":"Setting Axis Attributes","text":"<p>Once axes are inferred, you can set them on the coordinates:</p> <pre><code># Set axis attributes on all coordinates\nds.set_axis_attributes()\n\n# Check the result\nprint(ds.coords[\"time\"].attrs[\"axis\"])  # 'T'\nprint(ds.coords[\"lat\"].attrs[\"axis\"])   # 'Y'\nprint(ds.coords[\"lon\"].attrs[\"axis\"])   # 'X'\n</code></pre>"},{"location":"user-guide/cf-compliance/#inference-rules","title":"Inference Rules","text":""},{"location":"user-guide/cf-compliance/#time-axis-t","title":"Time Axis (T)","text":"<ul> <li>Names: time, t, date</li> <li>Units: Contains \"since\", \"days\", \"hours\", \"minutes\", \"seconds\"</li> <li>Standard names: time</li> </ul>"},{"location":"user-guide/cf-compliance/#latitude-axis-y","title":"Latitude Axis (Y)","text":"<ul> <li>Names: lat, latitude, y, j, yc</li> <li>Units: degrees_north, degree_north</li> <li>Standard names: latitude, projection_y_coordinate, grid_latitude</li> </ul>"},{"location":"user-guide/cf-compliance/#longitude-axis-x","title":"Longitude Axis (X)","text":"<ul> <li>Names: lon, longitude, x, i, xc</li> <li>Units: degrees_east, degree_east</li> <li>Standard names: longitude, projection_x_coordinate, grid_longitude</li> </ul>"},{"location":"user-guide/cf-compliance/#vertical-axis-z","title":"Vertical Axis (Z)","text":"<ul> <li>Names: lev, level, plev, height, depth, alt, z, k</li> <li>Units: pa, hpa, mbar, m, km, level, sigma</li> <li>Standard names: altitude, height, depth, air_pressure, model_level_number</li> </ul>"},{"location":"user-guide/cf-compliance/#cf-validation","title":"CF Validation","text":""},{"location":"user-guide/cf-compliance/#basic-validation","title":"Basic Validation","text":"<p>Check your dataset for CF compliance issues:</p> <pre><code># Validate the dataset\nresult = ds.validate_cf()\n\n# Check for errors and warnings\nprint(f\"Errors: {len(result['errors'])}\")\nprint(f\"Warnings: {len(result['warnings'])}\")\n\n# Print warnings\nfor warning in result['warnings']:\n    print(f\"  - {warning}\")\n</code></pre>"},{"location":"user-guide/cf-compliance/#validation-checks","title":"Validation Checks","text":"<p>The validator checks for:</p> <ol> <li>Missing axis attributes - Coordinates without axis attribute</li> <li>Missing units - Coordinates/variables without units</li> <li>Missing standard_name - Coordinates without standard_name</li> <li>Missing Conventions - Global attribute not set</li> <li>Dimension ordering - Not following T, Z, Y, X recommendation</li> <li>Missing long_name - Variables without descriptive name</li> </ol>"},{"location":"user-guide/cf-compliance/#strict-mode","title":"Strict Mode","text":"<p>Use strict mode to raise an error on any CF violation:</p> <pre><code>try:\n    ds.validate_cf(strict=True)\nexcept ValueError as e:\n    print(f\"CF validation failed: {e}\")\n</code></pre>"},{"location":"user-guide/cf-compliance/#complete-cf-compliant-example","title":"Complete CF-Compliant Example","text":"<p>Here's a complete example creating a CF-compliant dataset:</p> <pre><code>from dummyxarray import DummyDataset\n\n# Create dataset with CF conventions\nds = DummyDataset()\nds.assign_attrs(\n    Conventions=\"CF-1.8\",\n    title=\"Example CF-Compliant Dataset\",\n    institution=\"Example Institution\",\n    source=\"dummyxarray example\",\n    history=\"Created with dummyxarray\"\n)\n\n# Add dimensions\nds.add_dim(\"time\", 12)\nds.add_dim(\"lev\", 5)\nds.add_dim(\"lat\", 64)\nds.add_dim(\"lon\", 128)\n\n# Add coordinates with full CF metadata\nds.add_coord(\n    \"time\",\n    dims=[\"time\"],\n    attrs={\n        \"standard_name\": \"time\",\n        \"long_name\": \"time\",\n        \"units\": \"days since 2000-01-01 00:00:00\",\n        \"calendar\": \"gregorian\"\n    }\n)\n\nds.add_coord(\n    \"lev\",\n    dims=[\"lev\"],\n    attrs={\n        \"standard_name\": \"air_pressure\",\n        \"long_name\": \"pressure level\",\n        \"units\": \"hPa\",\n        \"positive\": \"down\"\n    }\n)\n\nds.add_coord(\n    \"lat\",\n    dims=[\"lat\"],\n    attrs={\n        \"standard_name\": \"latitude\",\n        \"long_name\": \"latitude\",\n        \"units\": \"degrees_north\"\n    }\n)\n\nds.add_coord(\n    \"lon\",\n    dims=[\"lon\"],\n    attrs={\n        \"standard_name\": \"longitude\",\n        \"long_name\": \"longitude\",\n        \"units\": \"degrees_east\"\n    }\n)\n\n# Infer and set axis attributes\nds.infer_axis()\nds.set_axis_attributes()\n\n# Add variable with CF metadata\nds.add_variable(\n    \"temperature\",\n    dims=[\"time\", \"lev\", \"lat\", \"lon\"],\n    attrs={\n        \"standard_name\": \"air_temperature\",\n        \"long_name\": \"Air Temperature\",\n        \"units\": \"K\",\n        \"cell_methods\": \"time: mean\"\n    }\n)\n\n# Validate\nresult = ds.validate_cf()\nif result['warnings']:\n    print(\"Warnings:\")\n    for warning in result['warnings']:\n        print(f\"  - {warning}\")\nelse:\n    print(\"\u2713 Dataset is CF-compliant!\")\n\n# Populate with data\nds.populate_with_random_data(seed=42)\n\n# Export\nds.save_yaml(\"cf_compliant_template.yaml\")\nxr_ds = ds.to_xarray()\n</code></pre>"},{"location":"user-guide/cf-compliance/#query-axis-coordinates","title":"Query Axis Coordinates","text":"<p>Find all coordinates with a specific axis:</p> <pre><code># Get all X-axis coordinates\nx_coords = ds.get_axis_coordinates(\"X\")\nprint(f\"X-axis coordinates: {x_coords}\")\n\n# Get all time coordinates\nt_coords = ds.get_axis_coordinates(\"T\")\nprint(f\"Time coordinates: {t_coords}\")\n</code></pre>"},{"location":"user-guide/cf-compliance/#resetting-history","title":"Resetting History","text":"<p>After importing from xarray or making many changes, you can reset the history:</p> <pre><code># Import from existing xarray dataset\nimport xarray as xr\nxr_ds = xr.open_dataset(\"existing_data.nc\")\nds = DummyDataset.from_xarray(xr_ds)\n\n# Reset history to start fresh\nds.reset_history()\n\n# Now only track new modifications\nds.infer_axis()\nds.set_axis_attributes()\n</code></pre>"},{"location":"user-guide/cf-compliance/#best-practices","title":"Best Practices","text":"<ol> <li>Always set Conventions - Include <code>Conventions=\"CF-1.8\"</code> in global attributes</li> <li>Use standard_name - Follow CF standard name table</li> <li>Include units - All coordinates and variables should have units</li> <li>Set axis attributes - Use <code>set_axis_attributes()</code> for automatic setting</li> <li>Validate early - Run <code>validate_cf()</code> before generating data</li> <li>Follow dimension order - Use T, Z, Y, X ordering when possible</li> <li>Add long_name - Provide human-readable descriptions</li> </ol>"},{"location":"user-guide/cf-compliance/#cf-resources","title":"CF Resources","text":"<ul> <li>CF Conventions - Official CF conventions documentation</li> <li>CF Standard Names - Standard name table</li> <li>CF Checker - External CF compliance checker</li> </ul>"},{"location":"user-guide/cf-compliance/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about History Tracking to record dataset modifications</li> <li>See Examples for more CF compliance workflows</li> <li>Check the API Reference for all CF-related methods</li> </ul>"},{"location":"user-guide/cf-standards/","title":"CF Standards with cf_xarray","text":"<p>dummyxarray uses <code>cf_xarray</code> to apply community-agreed CF standards to your datasets.</p>"},{"location":"user-guide/cf-standards/#overview","title":"Overview","text":"<p>dummyxarray integrates <code>cf_xarray</code> to provide:</p> <ul> <li>Community standards - Based on official CF conventions</li> <li>Automatic detection - Uses criteria from MetPy and Iris</li> <li>Ecosystem integration - Consistent with xarray tools</li> <li>Comprehensive validation - Beyond basic checks</li> </ul>"},{"location":"user-guide/cf-standards/#why-cf_xarray","title":"Why cf_xarray?","text":"<p>Instead of creating our own interpretation of CF standards, we use <code>cf_xarray</code> which implements community-agreed criteria for:</p> <ul> <li>Axis detection (X, Y, Z, T)</li> <li>Coordinate identification</li> <li>Attribute requirements</li> <li>Standard names</li> </ul> <p>This ensures your datasets are compatible with the broader scientific Python ecosystem.</p>"},{"location":"user-guide/cf-standards/#installation","title":"Installation","text":"<p>cf_xarray is automatically installed as a dependency when you install dummyxarray:</p> <pre><code>pip install dummyxarray\n# or\npixi install\n</code></pre>"},{"location":"user-guide/cf-standards/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/cf-standards/#apply-cf-standards","title":"Apply CF Standards","text":"<pre><code>from dummyxarray import DummyDataset\n\nds = DummyDataset()\nds.add_dim(\"time\", 12)\nds.add_coord(\n    \"time\",\n    dims=[\"time\"],\n    attrs={\"units\": \"days since 2000-01-01\"}\n)\n\n# Apply CF standards using cf_xarray (no data needed!)\nresult = ds.apply_cf_standards()\n\n# Check what was detected\nprint(result['axes_detected'])  # {'time': 'T'}\nprint(result['attrs_added'])    # {'time': {'axis': 'T', ...}}\n\n# Coordinate now has proper CF attributes\nprint(ds.coords['time'].attrs)\n# {'units': 'days since 2000-01-01', 'axis': 'T', 'standard_name': 'time'}\n</code></pre>"},{"location":"user-guide/cf-standards/#validate-cf-metadata","title":"Validate CF Metadata","text":"<pre><code># Validate against CF standards (no data needed!)\nresult = ds.validate_cf_metadata()\n\nprint(f\"Valid: {result['valid']}\")\nprint(f\"Errors: {result['errors']}\")\nprint(f\"Warnings: {result['warnings']}\")\nprint(f\"Suggestions: {result['suggestions']}\")\n</code></pre>"},{"location":"user-guide/cf-standards/#works-without-data","title":"Works Without Data! \ud83c\udf89","text":"<p>Great news: Both <code>apply_cf_standards()</code> and <code>validate_cf_metadata()</code> now work without requiring data to be populated!</p>"},{"location":"user-guide/cf-standards/#how","title":"How?","text":"<p>The functions automatically create temporary dummy arrays (zeros) just for cf_xarray processing, then discard them - keeping only the detected metadata. Your actual dataset remains metadata-only.</p> <pre><code># Works perfectly without data!\nds = DummyDataset()\nds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\nds.add_variable(\"temperature\", dims=[\"time\"], attrs={\"units\": \"K\"})\n\n# No data needed - metadata-only!\nresult = ds.apply_cf_standards()\nprint(result['axes_detected'])  # {'time': 'T'}\n\n# Data is still None\nprint(ds.coords['time'].data)  # None\n</code></pre>"},{"location":"user-guide/cf-standards/#optional-use-real-data","title":"Optional: Use Real Data","text":"<p>If you already have data or want to populate it, that works too:</p> <pre><code># Optional: populate with data\nds.populate_with_random_data(seed=42)\n\n# Works the same way\nds.apply_cf_standards()\n</code></pre>"},{"location":"user-guide/cf-standards/#complete-example","title":"Complete Example","text":"<pre><code>from dummyxarray import DummyDataset\n\n# Create dataset with minimal metadata\nds = DummyDataset()\nds.assign_attrs(Conventions=\"CF-1.8\", title=\"Climate Data\")\n\n# Add coordinates with basic attributes\nds.add_dim(\"time\", 365)\nds.add_dim(\"lat\", 64)\nds.add_dim(\"lon\", 128)\n\nds.add_coord(\"time\", dims=[\"time\"], \n             attrs={\"units\": \"days since 2000-01-01\"})\nds.add_coord(\"lat\", dims=[\"lat\"], \n             attrs={\"units\": \"degrees_north\"})\nds.add_coord(\"lon\", dims=[\"lon\"], \n             attrs={\"units\": \"degrees_east\"})\n\nds.add_variable(\"temperature\", dims=[\"time\", \"lat\", \"lon\"],\n                attrs={\"standard_name\": \"air_temperature\", \"units\": \"K\"})\n\n# Apply CF standards\nif ds.check_cf_standards_available():\n    result = ds.apply_cf_standards()\n    print(f\"Detected axes: {result['axes_detected']}\")\n    # Output: {'time': 'T', 'lat': 'Y', 'lon': 'X'}\n\n    # Validate\n    validation = ds.validate_cf_metadata()\n    if validation['valid']:\n        print(\"\u2713 CF compliant!\")\nelse:\n    print(\"Install cf_xarray for CF standards support\")\n</code></pre>"},{"location":"user-guide/cf-standards/#what-gets-added","title":"What Gets Added","text":"<p>cf_xarray automatically adds appropriate attributes based on detection:</p>"},{"location":"user-guide/cf-standards/#time-coordinates","title":"Time Coordinates","text":"<pre><code># Before\nattrs = {\"units\": \"days since 2000-01-01\"}\n\n# After apply_cf_standards()\nattrs = {\n    \"units\": \"days since 2000-01-01\",\n    \"axis\": \"T\",\n    \"standard_name\": \"time\"\n}\n</code></pre>"},{"location":"user-guide/cf-standards/#latitude-coordinates","title":"Latitude Coordinates","text":"<pre><code># Before\nattrs = {\"units\": \"degrees_north\"}\n\n# After\nattrs = {\n    \"units\": \"degrees_north\",\n    \"axis\": \"Y\",\n    \"standard_name\": \"latitude\"\n}\n</code></pre>"},{"location":"user-guide/cf-standards/#longitude-coordinates","title":"Longitude Coordinates","text":"<pre><code># Before\nattrs = {\"units\": \"degrees_east\"}\n\n# After\nattrs = {\n    \"units\": \"degrees_east\",\n    \"axis\": \"X\",\n    \"standard_name\": \"longitude\"\n}\n</code></pre>"},{"location":"user-guide/cf-standards/#vertical-coordinates","title":"Vertical Coordinates","text":"<pre><code># Before\nattrs = {\"units\": \"hPa\", \"positive\": \"down\"}\n\n# After\nattrs = {\n    \"units\": \"hPa\",\n    \"positive\": \"down\",\n    \"axis\": \"Z\",\n    \"standard_name\": \"air_pressure\"\n}\n</code></pre>"},{"location":"user-guide/cf-standards/#detection-criteria","title":"Detection Criteria","text":"<p>cf_xarray uses sophisticated criteria to detect coordinates:</p>"},{"location":"user-guide/cf-standards/#by-units","title":"By Units","text":"<ul> <li><code>degrees_north</code>, <code>degree_north</code>, <code>degrees_N</code> \u2192 Latitude (Y)</li> <li><code>degrees_east</code>, <code>degree_east</code>, <code>degrees_E</code> \u2192 Longitude (X)</li> <li>Time units like <code>days since YYYY-MM-DD</code> \u2192 Time (T)</li> <li>Pressure units like <code>hPa</code>, <code>Pa</code>, <code>mbar</code> \u2192 Vertical (Z)</li> </ul>"},{"location":"user-guide/cf-standards/#by-standard-name","title":"By Standard Name","text":"<ul> <li><code>latitude</code>, <code>grid_latitude</code> \u2192 Y axis</li> <li><code>longitude</code>, <code>grid_longitude</code> \u2192 X axis</li> <li><code>time</code> \u2192 T axis</li> <li><code>air_pressure</code>, <code>altitude</code>, <code>height</code> \u2192 Z axis</li> </ul>"},{"location":"user-guide/cf-standards/#by-axis-attribute","title":"By Axis Attribute","text":"<ul> <li><code>axis=\"X\"</code> \u2192 X axis</li> <li><code>axis=\"Y\"</code> \u2192 Y axis</li> <li><code>axis=\"Z\"</code> \u2192 Z axis</li> <li><code>axis=\"T\"</code> \u2192 T axis</li> </ul>"},{"location":"user-guide/cf-standards/#by-name-pattern","title":"By Name Pattern","text":"<ul> <li>Names like <code>lat</code>, <code>latitude</code>, <code>y</code> \u2192 Y axis</li> <li>Names like <code>lon</code>, <code>longitude</code>, <code>x</code> \u2192 X axis</li> <li>Names like <code>time</code>, <code>t</code> \u2192 T axis</li> <li>Names like <code>lev</code>, <code>level</code>, <code>z</code>, <code>height</code> \u2192 Z axis</li> </ul>"},{"location":"user-guide/cf-standards/#built-in-vs-cf_xarray","title":"Built-in vs cf_xarray","text":"Feature Built-in cf_xarray Dependencies None cf_xarray Speed Fast Moderate Standards Basic Community-agreed Detection Simple patterns Comprehensive criteria Validation Essential checks Thorough Ecosystem Standalone xarray-compatible"},{"location":"user-guide/cf-standards/#when-to-use-built-in","title":"When to Use Built-in","text":"<ul> <li>Quick prototyping</li> <li>No external dependencies needed</li> <li>Simple datasets</li> <li>Fast iteration</li> </ul>"},{"location":"user-guide/cf-standards/#when-to-use-cf_xarray","title":"When to Use cf_xarray","text":"<ul> <li>Production datasets</li> <li>Publishing data</li> <li>Ecosystem compatibility</li> <li>Comprehensive validation</li> <li>Following community standards</li> </ul>"},{"location":"user-guide/cf-standards/#workflow-recommendations","title":"Workflow Recommendations","text":""},{"location":"user-guide/cf-standards/#development-workflow","title":"Development Workflow","text":"<pre><code># During development: use built-in for speed\nds.infer_axis()\nds.set_axis_attributes()\nresult = ds.validate_cf()\n</code></pre>"},{"location":"user-guide/cf-standards/#production-workflow","title":"Production Workflow","text":"<pre><code># For production: use cf_xarray for standards\nif ds.check_cf_standards_available():\n    ds.apply_cf_standards()\n    result = ds.validate_cf_metadata(strict=True)\n\n    if not result['valid']:\n        print(\"Fix these issues:\")\n        for error in result['errors']:\n            print(f\"  - {error}\")\nelse:\n    # Fallback to built-in\n    ds.infer_axis()\n    ds.set_axis_attributes()\n</code></pre>"},{"location":"user-guide/cf-standards/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/cf-standards/#verbose-mode","title":"Verbose Mode","text":"<pre><code># See what cf_xarray is doing\nresult = ds.apply_cf_standards(verbose=True)\n</code></pre>"},{"location":"user-guide/cf-standards/#strict-validation","title":"Strict Validation","text":"<pre><code># Treat warnings as errors\nresult = ds.validate_cf_metadata(strict=True)\n</code></pre>"},{"location":"user-guide/cf-standards/#check-availability","title":"Check Availability","text":"<pre><code># Check before using\nif ds.check_cf_standards_available():\n    ds.apply_cf_standards()\nelse:\n    print(\"Install: pip install cf_xarray\")\n</code></pre>"},{"location":"user-guide/cf-standards/#integration-with-other-tools","title":"Integration with Other Tools","text":"<p>cf_xarray makes your datasets compatible with:</p> <ul> <li>xarray - Native integration</li> <li>MetPy - Meteorological calculations</li> <li>Iris - Climate data analysis</li> <li>Cartopy - Map projections</li> <li>Dask - Parallel computing</li> </ul>"},{"location":"user-guide/cf-standards/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/cf-standards/#cf_xarray-not-available","title":"cf_xarray Not Available","text":"<pre><code># Check if installed\nimport importlib.util\nif importlib.util.find_spec(\"cf_xarray\") is None:\n    print(\"Install: pip install cf_xarray\")\n</code></pre>"},{"location":"user-guide/cf-standards/#attributes-not-detected","title":"Attributes Not Detected","text":"<p>If cf_xarray doesn't detect your coordinates:</p> <ol> <li>Check units are CF-compliant</li> <li>Add <code>standard_name</code> attribute</li> <li>Use recognized coordinate names</li> <li>Add <code>axis</code> attribute explicitly</li> </ol>"},{"location":"user-guide/cf-standards/#validation-warnings","title":"Validation Warnings","text":"<p>Common warnings and fixes:</p> <pre><code># Warning: Missing standard_name\nds.coords['time'].attrs['standard_name'] = 'time'\n\n# Warning: Missing axis\nds.coords['lat'].attrs['axis'] = 'Y'\n\n# Warning: Non-standard units\nds.coords['lat'].attrs['units'] = 'degrees_north'  # Not 'deg N'\n</code></pre>"},{"location":"user-guide/cf-standards/#best-practices","title":"Best Practices","text":"<ol> <li>Start minimal - Add basic attributes, let cf_xarray fill the rest</li> <li>Use standard names - Follow CF standard name table</li> <li>Validate early - Check compliance during development</li> <li>Document choices - Use history tracking for decisions</li> <li>Test compatibility - Verify with xarray and other tools</li> </ol>"},{"location":"user-guide/cf-standards/#resources","title":"Resources","text":"<ul> <li>cf_xarray Documentation</li> <li>CF Conventions</li> <li>CF Standard Names</li> <li>MetPy Documentation</li> </ul>"},{"location":"user-guide/cf-standards/#see-also","title":"See Also","text":"<ul> <li>CF Compliance - Built-in CF features</li> <li>Validation - Dataset validation</li> <li>Examples - More CF standards examples</li> </ul>"},{"location":"user-guide/encoding/","title":"Encoding","text":"<p>Configure how your data is stored with encoding parameters.</p>"},{"location":"user-guide/encoding/#what-is-encoding","title":"What is Encoding?","text":"<p>Encoding parameters control how data is written to disk formats like Zarr or NetCDF. This includes:</p> <ul> <li>Data type (dtype)</li> <li>Chunking strategy</li> <li>Compression</li> <li>Fill values</li> <li>Scale factors</li> </ul>"},{"location":"user-guide/encoding/#basic-encoding","title":"Basic Encoding","text":"<pre><code>from dummyxarray import DummyDataset\nimport numpy as np\n\nds = DummyDataset()\nds.add_dim(\"time\", 100)\n\ndata = np.random.rand(100)\nds.add_variable(\n    \"temperature\",\n    dims=[\"time\"],\n    data=data,\n    encoding={\n        \"dtype\": \"float32\",  # Store as 32-bit float\n    }\n)\n</code></pre>"},{"location":"user-guide/encoding/#chunking","title":"Chunking","text":"<p>Chunking is crucial for performance with large datasets:</p> <pre><code>ds = DummyDataset()\nds.add_dim(\"time\", 365)\nds.add_dim(\"lat\", 180)\nds.add_dim(\"lon\", 360)\n\ndata = np.random.rand(365, 180, 360)\nds.add_variable(\n    \"temperature\",\n    dims=[\"time\", \"lat\", \"lon\"],\n    data=data,\n    encoding={\n        \"dtype\": \"float32\",\n        \"chunks\": (30, 90, 180),  # Chunk size for each dimension\n    }\n)\n</code></pre>"},{"location":"user-guide/encoding/#compression","title":"Compression","text":"<p>Add compression to reduce file size:</p> <pre><code>import zarr\n\nds.add_variable(\n    \"temperature\",\n    dims=[\"time\", \"lat\", \"lon\"],\n    data=data,\n    encoding={\n        \"dtype\": \"float32\",\n        \"chunks\": (30, 90, 180),\n        \"compressor\": zarr.Blosc(cname='zstd', clevel=3, shuffle=2)\n    }\n)\n</code></pre>"},{"location":"user-guide/encoding/#fill-values","title":"Fill Values","text":"<p>Specify fill values for missing data:</p> <pre><code>ds.add_variable(\n    \"temperature\",\n    dims=[\"time\"],\n    data=data,\n    encoding={\n        \"dtype\": \"float32\",\n        \"_FillValue\": -9999.0\n    }\n)\n</code></pre>"},{"location":"user-guide/encoding/#scale-and-offset","title":"Scale and Offset","text":"<p>Use scale_factor and add_offset for packed data:</p> <pre><code>ds.add_variable(\n    \"temperature\",\n    dims=[\"time\"],\n    data=data,\n    encoding={\n        \"dtype\": \"int16\",\n        \"scale_factor\": 0.01,\n        \"add_offset\": 273.15,\n        \"_FillValue\": -32768\n    }\n)\n</code></pre>"},{"location":"user-guide/encoding/#encoding-for-coordinates","title":"Encoding for Coordinates","text":"<p>Coordinates can also have encoding:</p> <pre><code>time_data = np.arange(365)\nds.add_coord(\n    \"time\",\n    dims=[\"time\"],\n    data=time_data,\n    attrs={\"units\": \"days since 2020-01-01\"},\n    encoding={\n        \"dtype\": \"int32\",\n        \"calendar\": \"standard\"\n    }\n)\n</code></pre>"},{"location":"user-guide/encoding/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/encoding/#chunking-strategy","title":"Chunking Strategy","text":"<ul> <li>Time series data: Large chunks in time, smaller in space</li> <li>Spatial data: Smaller chunks in time, larger in space</li> <li>General rule: Aim for chunk sizes of 10-100 MB</li> </ul> <pre><code># For time series analysis\nencoding={\"chunks\": (365, 45, 90)}  # Full year, quarter lat/lon\n\n# For spatial analysis\nencoding={\"chunks\": (1, 180, 360)}  # Single time, full spatial\n</code></pre>"},{"location":"user-guide/encoding/#data-types","title":"Data Types","text":"<ul> <li>Use <code>float32</code> instead of <code>float64</code> when precision allows</li> <li>Use <code>int16</code> or <code>int32</code> for integer data</li> <li>Consider packed integers with scale/offset for large datasets</li> </ul>"},{"location":"user-guide/encoding/#compression_1","title":"Compression","text":"<ul> <li><code>zstd</code> (level 3): Good balance of speed and compression</li> <li><code>lz4</code>: Fastest, moderate compression</li> <li><code>blosc</code>: Good for scientific data</li> </ul>"},{"location":"user-guide/encoding/#applying-encoding","title":"Applying Encoding","text":"<p>Encoding is automatically applied when converting to xarray or writing to Zarr:</p> <pre><code># Encoding is preserved in xarray\nxr_ds = ds.to_xarray()\nprint(xr_ds[\"temperature\"].encoding)\n\n# Encoding is used when writing to Zarr\nds.to_zarr(\"output.zarr\")\n</code></pre>"},{"location":"user-guide/history-tracking/","title":"History Tracking","text":"<p>dummyxarray automatically tracks all operations performed on datasets, enabling reproducible workflows and dataset provenance.</p>"},{"location":"user-guide/history-tracking/#overview","title":"Overview","text":"<p>Every operation on a <code>DummyDataset</code> is automatically recorded, including: - Dataset initialization - Adding dimensions, coordinates, and variables - Setting attributes - Renaming operations - And more...</p> <p>This history can be: - Exported as Python code, JSON, or YAML - Visualized as text, DOT graphs, or Mermaid diagrams - Replayed to recreate datasets - Reset to start tracking from a clean state</p>"},{"location":"user-guide/history-tracking/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/history-tracking/#getting-history","title":"Getting History","text":"<pre><code>from dummyxarray import DummyDataset\n\nds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days\"})\nds.assign_attrs(title=\"My Dataset\")\n\n# Get the operation history\nhistory = ds.get_history()\nprint(history)\n</code></pre> <p>Output: <pre><code>[\n    {'func': '__init__', 'args': {}},\n    {'func': 'add_dim', 'args': {'name': 'time', 'size': 10}},\n    {'func': 'add_coord', 'args': {'name': 'time', 'dims': ['time'], 'attrs': {'units': 'days'}}},\n    {'func': 'assign_attrs', 'args': {'title': 'My Dataset'}}\n]\n</code></pre></p>"},{"location":"user-guide/history-tracking/#exporting-history","title":"Exporting History","text":""},{"location":"user-guide/history-tracking/#as-python-code","title":"As Python Code","text":"<p>Export history as executable Python code:</p> <pre><code>python_code = ds.export_history('python')\nprint(python_code)\n</code></pre> <p>Output: <pre><code>ds = DummyDataset()\nds.add_dim(name='time', size=10)\nds.add_coord(name='time', dims=['time'], attrs={'units': 'days'})\nds.assign_attrs(title='My Dataset')\n</code></pre></p>"},{"location":"user-guide/history-tracking/#as-json","title":"As JSON","text":"<pre><code>json_history = ds.export_history('json')\nprint(json_history)\n</code></pre>"},{"location":"user-guide/history-tracking/#as-yaml","title":"As YAML","text":"<pre><code>yaml_history = ds.export_history('yaml')\nprint(yaml_history)\n</code></pre>"},{"location":"user-guide/history-tracking/#visualizing-history","title":"Visualizing History","text":""},{"location":"user-guide/history-tracking/#text-format","title":"Text Format","text":"<pre><code>print(ds.visualize_history(format='text'))\n</code></pre> <p>Output: <pre><code>Dataset Construction History\n============================\n1. __init__()\n2. add_dim(name='time', size=10)\n3. add_coord(name='time', dims=['time'], attrs={'units': 'days'})\n4. assign_attrs(title='My Dataset')\n\nSummary:\n  Total operations: 4\n  Operation breakdown:\n    __init__: 1\n    add_dim: 1\n    add_coord: 1\n    assign_attrs: 1\n</code></pre></p>"},{"location":"user-guide/history-tracking/#dot-format-graphviz","title":"DOT Format (Graphviz)","text":"<p>Generate a graph visualization:</p> <pre><code>dot_graph = ds.visualize_history(format='dot')\nprint(dot_graph)\n\n# Save to file and render\nwith open('history.dot', 'w') as f:\n    f.write(dot_graph)\n\n# Render with: dot -Tpng history.dot -o history.png\n</code></pre>"},{"location":"user-guide/history-tracking/#mermaid-format","title":"Mermaid Format","text":"<p>Generate a Mermaid diagram (works in GitHub, GitLab, documentation):</p> <pre><code>mermaid_diagram = ds.visualize_history(format='mermaid')\nprint(mermaid_diagram)\n</code></pre> <p>Output: <pre><code>graph TD\n    A[__init__] --&gt; B[add_dim: time=10]\n    B --&gt; C[add_coord: time]\n    C --&gt; D[assign_attrs: title]\n</code></pre></p>"},{"location":"user-guide/history-tracking/#replaying-history","title":"Replaying History","text":""},{"location":"user-guide/history-tracking/#from-history-list","title":"From History List","text":"<p>Recreate a dataset from its history:</p> <pre><code># Get history from original dataset\nhistory = ds.get_history()\n\n# Create new dataset by replaying history\nnew_ds = DummyDataset.replay_history(history)\n\n# Verify they're equivalent\nassert new_ds.dims == ds.dims\nassert new_ds.attrs == ds.attrs\n</code></pre>"},{"location":"user-guide/history-tracking/#from-jsonyaml","title":"From JSON/YAML","text":"<pre><code># Export history\njson_history = ds.export_history('json')\n\n# Later, replay from JSON string\nrecreated_ds = DummyDataset.replay_history(json_history)\n</code></pre>"},{"location":"user-guide/history-tracking/#use-cases-for-replay","title":"Use Cases for Replay","text":"<ol> <li>Version Control - Store history in git, replay to recreate datasets</li> <li>Documentation - Include history in papers/reports</li> <li>Debugging - Replay to understand how a dataset was created</li> <li>Templates - Share dataset creation workflows</li> </ol>"},{"location":"user-guide/history-tracking/#provenance-tracking","title":"Provenance Tracking","text":"<p>Track what changed in each operation:</p> <pre><code>ds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.add_coord(\"time\", dims=[\"time\"])\nds.rename_dims(time=\"t\")\n\n# Get provenance information\nprovenance = ds.get_provenance()\nprint(provenance)\n</code></pre> <p>Output: <pre><code>[\n    {\n        'index': 1,\n        'func': 'add_dim',\n        'provenance': {'added': ['time']}\n    },\n    {\n        'index': 2,\n        'func': 'add_coord',\n        'provenance': {'added': ['time']}\n    },\n    {\n        'index': 3,\n        'func': 'rename_dims',\n        'provenance': {\n            'renamed': {'time': 't'},\n            'removed': ['time'],\n            'added': ['t']\n        }\n    }\n]\n</code></pre></p>"},{"location":"user-guide/history-tracking/#visualizing-provenance","title":"Visualizing Provenance","text":"<pre><code># Compact view\nprint(ds.visualize_provenance(format='compact'))\n\n# Detailed view\nprint(ds.visualize_provenance(format='detailed'))\n</code></pre>"},{"location":"user-guide/history-tracking/#resetting-history","title":"Resetting History","text":"<p>Start fresh while keeping the current dataset state:</p> <pre><code># Create dataset with many operations\nds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.add_dim(\"lat\", 64)\nds.add_coord(\"time\", dims=[\"time\"])\n# ... many more operations ...\n\n# Reset history\nds.reset_history()\n\n# Now history only contains __init__\nprint(len(ds.get_history()))  # 1\n\n# New operations are tracked from this point\nds.add_variable(\"temp\", dims=[\"time\", \"lat\"])\nprint(len(ds.get_history()))  # 2\n</code></pre>"},{"location":"user-guide/history-tracking/#when-to-reset","title":"When to Reset","text":"<ul> <li>After importing from xarray</li> <li>When starting a new workflow phase</li> <li>To simplify history for documentation</li> <li>Before sharing templates</li> </ul>"},{"location":"user-guide/history-tracking/#complete-example","title":"Complete Example","text":"<pre><code>from dummyxarray import DummyDataset\n\n# Create dataset with tracked operations\nds = DummyDataset()\nds.assign_attrs(Conventions=\"CF-1.8\", title=\"Temperature Data\")\n\nds.add_dim(\"time\", 12)\nds.add_dim(\"lat\", 64)\nds.add_dim(\"lon\", 128)\n\nds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\nds.add_coord(\"lat\", dims=[\"lat\"], attrs={\"units\": \"degrees_north\"})\nds.add_coord(\"lon\", dims=[\"lon\"], attrs={\"units\": \"degrees_east\"})\n\nds.add_variable(\n    \"temperature\",\n    dims=[\"time\", \"lat\", \"lon\"],\n    attrs={\"standard_name\": \"air_temperature\", \"units\": \"K\"}\n)\n\n# Export history as Python code\npython_code = ds.export_history('python')\nwith open('create_dataset.py', 'w') as f:\n    f.write(python_code)\n\n# Visualize history\nprint(ds.visualize_history(format='text'))\n\n# Save history as JSON for version control\nimport json\nhistory = ds.get_history()\nwith open('dataset_history.json', 'w') as f:\n    json.dump(history, f, indent=2)\n\n# Get provenance information\nprovenance = ds.get_provenance()\nprint(f\"\\nTotal operations with provenance: {len(provenance)}\")\nprint(f\"Items added: {sum(len(p['provenance'].get('added', [])) for p in provenance)}\")\n</code></pre>"},{"location":"user-guide/history-tracking/#history-for-dummyarray","title":"History for DummyArray","text":"<p>Individual arrays also track their history:</p> <pre><code>from dummyxarray import DummyArray\n\narr = DummyArray(dims=[\"time\"], attrs={\"units\": \"K\"})\narr.assign_attrs(long_name=\"Temperature\", standard_name=\"air_temperature\")\n\n# Get array history\nhistory = arr.get_history()\nprint(history)\n\n# Replay array history\nnew_arr = DummyArray.replay_history(history)\n</code></pre>"},{"location":"user-guide/history-tracking/#best-practices","title":"Best Practices","text":"<ol> <li>Export history regularly - Save to version control</li> <li>Use meaningful operation names - Makes history easier to understand</li> <li>Reset after imports - Clean history when importing from xarray</li> <li>Visualize for documentation - Include Mermaid diagrams in docs</li> <li>Replay for testing - Verify dataset creation is reproducible</li> <li>Track provenance - Understand what changed and when</li> </ol>"},{"location":"user-guide/history-tracking/#advanced-custom-history-recording","title":"Advanced: Custom History Recording","text":"<p>History recording can be disabled:</p> <pre><code># Create dataset without history tracking\nds = DummyDataset(_record_history=False)\n\n# No history is recorded\nds.add_dim(\"time\", 10)\nprint(ds.get_history())  # None\n</code></pre> <p>This can be useful for: - Performance-critical code - Temporary datasets - Internal operations</p>"},{"location":"user-guide/history-tracking/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about CF Compliance validation</li> <li>See Examples for history tracking workflows</li> <li>Check the API Reference for all history methods</li> </ul>"},{"location":"user-guide/intake-catalogs/","title":"Intake Catalogs","text":"<p>dummyxarray provides comprehensive support for Intake catalogs, allowing you to both export  dataset specifications to Intake catalog format and import existing Intake catalogs back  into DummyDataset objects. This enables complete round-trip compatibility with the Intake  data cataloging ecosystem.</p>"},{"location":"user-guide/intake-catalogs/#overview","title":"Overview","text":"<p>Intake is a data cataloging system that provides a unified interface for discovering  and accessing data. dummyxarray's Intake catalog support allows you to:</p> <ul> <li>Export DummyDataset structures to Intake catalog YAML files</li> <li>Import Intake catalogs to recreate DummyDataset objects</li> <li>Preserve complete metadata including dimensions, coordinates, variables, and encoding</li> <li>Integrate with the broader Intake ecosystem for data discovery and sharing</li> </ul>"},{"location":"user-guide/intake-catalogs/#exporting-to-intake-catalogs","title":"Exporting to Intake Catalogs","text":""},{"location":"user-guide/intake-catalogs/#basic-export","title":"Basic Export","text":"<pre><code>from dummyxarray import DummyDataset\n\n# Create a dataset\nds = DummyDataset()\nds.add_dim(\"time\", 12)\nds.add_dim(\"lat\", 180)\nds.add_dim(\"lon\", 360)\nds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\nds.add_variable(\n    \"temperature\",\n    dims=[\"time\", \"lat\", \"lon\"],\n    attrs={\"units\": \"K\", \"standard_name\": \"air_temperature\"},\n    encoding={\"dtype\": \"float32\", \"chunks\": [6, 32, 64]}\n)\n\n# Generate catalog YAML string\ncatalog_yaml = ds.to_intake_catalog()\nprint(catalog_yaml)\n</code></pre>"},{"location":"user-guide/intake-catalogs/#customized-export","title":"Customized Export","text":"<pre><code># Export with custom parameters\ncatalog_yaml = ds.to_intake_catalog(\n    name=\"climate_data\",\n    description=\"Climate model output with temperature and precipitation\",\n    driver=\"zarr\",\n    data_path=\"data/climate_model_output.zarr\",\n    chunks={\"time\": 6}  # Additional driver arguments\n)\n</code></pre>"},{"location":"user-guide/intake-catalogs/#save-to-file","title":"Save to File","text":"<pre><code># Save catalog directly to file\nds.save_intake_catalog(\n    \"catalog.yaml\",\n    name=\"climate_data\",\n    description=\"Climate model output\",\n    driver=\"zarr\",\n    data_path=\"data/climate.zarr\"\n)\n</code></pre>"},{"location":"user-guide/intake-catalogs/#catalog-structure","title":"Catalog Structure","text":"<p>The generated Intake catalog includes:</p> <pre><code>metadata:\n  version: 1\n  description: Intake catalog for climate_data\n  dataset_attrs:\n    title: Climate Model Output\n    institution: Example Climate Center\n    Conventions: CF-1.8\n\nsources:\n  climate_data:\n    description: Climate model output with temperature and precipitation\n    driver: zarr\n    args:\n      urlpath: data/climate_model_output.zarr\n    metadata:\n      dimensions:\n        time: 12\n        lat: 180\n        lon: 360\n      coordinates:\n        time:\n          dims: [time]\n          attrs:\n            units: days since 2000-01-01\n      variables:\n        temperature:\n          dims: [time, lat, lon]\n          attrs:\n            units: K\n            standard_name: air_temperature\n          encoding:\n            dtype: float32\n            chunks: [6, 32, 64]\n</code></pre>"},{"location":"user-guide/intake-catalogs/#importing-from-intake-catalogs","title":"Importing from Intake Catalogs","text":""},{"location":"user-guide/intake-catalogs/#load-from-file","title":"Load from File","text":"<pre><code># Load from catalog file\nloaded_ds = DummyDataset.from_intake_catalog(\"catalog.yaml\", \"climate_data\")\n\n# Or use the convenience method\nloaded_ds = DummyDataset.load_intake_catalog(\"catalog.yaml\", \"climate_data\")\n</code></pre>"},{"location":"user-guide/intake-catalogs/#load-from-dictionary","title":"Load from Dictionary","text":"<pre><code>import yaml\n\n# Load catalog YAML and parse to dictionary\nwith open(\"catalog.yaml\") as f:\n    catalog_dict = yaml.safe_load(f)\n\n# Create DummyDataset from dictionary\nloaded_ds = DummyDataset.from_intake_catalog(catalog_dict, \"climate_data\")\n</code></pre>"},{"location":"user-guide/intake-catalogs/#automatic-source-selection","title":"Automatic Source Selection","text":"<pre><code># If catalog contains only one source, you can omit the source name\nsingle_source_ds = DummyDataset.from_intake_catalog(\"single_source_catalog.yaml\")\n</code></pre>"},{"location":"user-guide/intake-catalogs/#round-trip-workflow","title":"Round-Trip Workflow","text":"<p>Create a complete round-trip workflow:</p> <pre><code>from dummyxarray import DummyDataset\nimport tempfile\nimport yaml\n\n# 1. Create original dataset\noriginal_ds = DummyDataset()\noriginal_ds.assign_attrs(\n    title=\"Climate Model Output\",\n    institution=\"Example Climate Center\",\n    Conventions=\"CF-1.8\"\n)\noriginal_ds.add_dim(\"time\", 12)\noriginal_ds.add_dim(\"lat\", 180)\noriginal_ds.add_dim(\"lon\", 360)\noriginal_ds.add_coord(\"time\", dims=[\"time\"], attrs={\"units\": \"days since 2000-01-01\"})\noriginal_ds.add_variable(\n    \"temperature\",\n    dims=[\"time\", \"lat\", \"lon\"],\n    attrs={\"units\": \"K\"},\n    encoding={\"dtype\": \"float32\"}\n)\n\n# 2. Export to catalog\ncatalog_yaml = original_ds.to_intake_catalog(\n    name=\"climate_data\",\n    description=\"Climate model output\",\n    driver=\"zarr\"\n)\n\n# 3. Save to temporary file\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:\n    catalog_path = f.name\n    f.write(catalog_yaml)\n\n# 4. Load from catalog\nrestored_ds = DummyDataset.from_intake_catalog(catalog_path, \"climate_data\")\n\n# 5. Verify round-trip integrity\nassert restored_ds.dims == original_ds.dims\nassert set(restored_ds.variables.keys()) == set(original_ds.variables.keys())\nassert restored_ds.attrs[\"title\"] == original_ds.attrs[\"title\"]\n\nprint(\"Round-trip successful!\")\n</code></pre>"},{"location":"user-guide/intake-catalogs/#advanced-features","title":"Advanced Features","text":""},{"location":"user-guide/intake-catalogs/#multiple-sources-in-catalog","title":"Multiple Sources in Catalog","text":"<p>When working with catalogs containing multiple data sources:</p> <pre><code># Catalog with multiple sources\nmulti_source_catalog = {\n    \"metadata\": {\"version\": 1},\n    \"sources\": {\n        \"temperature\": {\n            \"driver\": \"zarr\",\n            \"args\": {\"urlpath\": \"data/temperature.zarr\"},\n            \"metadata\": {\"dimensions\": {\"time\": 12, \"lat\": 180, \"lon\": 360}}\n        },\n        \"precipitation\": {\n            \"driver\": \"zarr\", \n            \"args\": {\"urlpath\": \"data/precipitation.zarr\"},\n            \"metadata\": {\"dimensions\": {\"time\": 12, \"lat\": 180, \"lon\": 360}}\n        }\n    }\n}\n\n# Must specify which source to load\ntemp_ds = DummyDataset.from_intake_catalog(multi_source_catalog, \"temperature\")\nprecip_ds = DummyDataset.from_intake_catalog(multi_source_catalog, \"precipitation\")\n</code></pre>"},{"location":"user-guide/intake-catalogs/#driver-configuration","title":"Driver Configuration","text":"<p>Different data formats and drivers:</p> <pre><code># NetCDF driver\nds.to_intake_catalog(\n    name=\"netcdf_data\",\n    driver=\"netcdf\",\n    data_path=\"data/output.nc\",\n    engine=\"netcdf4\"\n)\n\n# Xarray driver with custom arguments\nds.to_intake_catalog(\n    name=\"xarray_data\", \n    driver=\"xarray\",\n    data_path=\"data/*.nc\",\n    combine=\"by_coords\",\n    parallel=True\n)\n</code></pre>"},{"location":"user-guide/intake-catalogs/#metadata-preservation","title":"Metadata Preservation","text":"<p>All dataset metadata is preserved in the catalog:</p> <pre><code># Dataset attributes become catalog metadata\nds.assign_attrs(\n    title=\"My Dataset\",\n    institution=\"My Organization\",\n    project=\"Climate Research\",\n    version=\"1.0\"\n)\n\n# After round-trip, attributes are preserved\nloaded_ds = DummyDataset.from_intake_catalog(\"catalog.yaml\", \"my_data\")\nassert loaded_ds.attrs[\"title\"] == \"My Dataset\"\nassert loaded_ds.attrs[\"institution\"] == \"My Organization\"\n\n# Catalog-specific attributes are also added\nassert loaded_ds.attrs[\"intake_catalog_source\"] == \"my_data\"\nassert loaded_ds.attrs[\"intake_driver\"] == \"zarr\"\n</code></pre>"},{"location":"user-guide/intake-catalogs/#error-handling","title":"Error Handling","text":"<p>The import functionality includes comprehensive error handling:</p> <pre><code>try:\n    # File not found\n    ds = DummyDataset.from_intake_catalog(\"nonexistent.yaml\")\nexcept FileNotFoundError as e:\n    print(f\"Catalog file not found: {e}\")\n\ntry:\n    # Invalid catalog format\n    ds = DummyDataset.from_intake_catalog({\"invalid\": \"structure\"})\nexcept ValueError as e:\n    print(f\"Invalid catalog: {e}\")\n\ntry:\n    # Source not found in multi-source catalog\n    ds = DummyDataset.from_intake_catalog(multi_source_catalog, \"nonexistent_source\")\nexcept ValueError as e:\n    print(f\"Source not found: {e}\")\n</code></pre>"},{"location":"user-guide/intake-catalogs/#integration-with-intake-ecosystem","title":"Integration with Intake Ecosystem","text":"<p>The generated catalogs are fully compatible with the Intake ecosystem:</p> <pre><code>import intake\n\n# Load catalog with Intake\ncatalog = intake.open_catalog(\"catalog.yaml\")\n\n# Access data source\ndata_source = catalog.climate_data\n\n# Get metadata\nprint(data_source.description)\nprint(data_source.metadata)\n\n# Load actual data (when available)\n# ds = data_source.read()\n</code></pre>"},{"location":"user-guide/intake-catalogs/#best-practices","title":"Best Practices","text":"<ol> <li>Descriptive Names: Use meaningful source names that reflect the data content</li> <li>Complete Metadata: Include comprehensive dataset attributes for better discoverability</li> <li>Consistent Paths: Use relative paths with <code>{{ CATALOG_DIR }}</code> template for portability</li> <li>Driver Selection: Choose appropriate drivers for your data format and access patterns</li> <li>Version Control: Track catalog files alongside your code for reproducibility</li> </ol>"},{"location":"user-guide/intake-catalogs/#examples","title":"Examples","text":"<p>See the Intake Catalog Example for a complete working demonstration of round-trip catalog functionality.</p>"},{"location":"user-guide/mfdataset/","title":"Multi-file Dataset Support","text":"<p>The <code>open_mfdataset()</code> feature allows you to work with multiple NetCDF files as a single dataset, tracking which files contain specific coordinate ranges. This is particularly useful for large climate datasets split across multiple files.</p>"},{"location":"user-guide/mfdataset/#overview","title":"Overview","text":"<p>Unlike xarray's <code>open_mfdataset()</code> which loads data into memory, DummyDataset's version only reads metadata from files. This makes it ideal for:</p> <ul> <li>Planning data access patterns</li> <li>Understanding dataset structure across multiple files</li> <li>Generating metadata specifications</li> <li>Tracking file provenance</li> </ul>"},{"location":"user-guide/mfdataset/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/mfdataset/#opening-multiple-files","title":"Opening Multiple Files","text":"<pre><code>from dummyxarray import DummyDataset\n\n# Using a glob pattern\nds = DummyDataset.open_mfdataset(\"data/*.nc\", concat_dim=\"time\")\n\n# Using a list of files\nfiles = [\"data_2020.nc\", \"data_2021.nc\", \"data_2022.nc\"]\nds = DummyDataset.open_mfdataset(files, concat_dim=\"time\")\n</code></pre>"},{"location":"user-guide/mfdataset/#querying-source-files","title":"Querying Source Files","text":"<p>Once files are loaded, you can query which files contain specific coordinate ranges:</p> <pre><code># Get all tracked files\nall_files = ds.get_source_files()\n\n# Query files for a specific time range\n# Note: Use coordinate types compatible with your data\nfiles = ds.get_source_files(time=slice(None, None))\n</code></pre>"},{"location":"user-guide/mfdataset/#getting-file-information","title":"Getting File Information","text":"<pre><code># Get detailed information about a specific file\ninfo = ds.get_file_info(\"data_2020.nc\")\n\nprint(f\"Coordinate range: {info['coord_range']}\")\nprint(f\"Variables: {info['metadata']['variables']}\")\nprint(f\"Dimensions: {info['metadata']['dims']}\")\n</code></pre>"},{"location":"user-guide/mfdataset/#automatic-frequency-inference","title":"Automatic Frequency Inference","text":"<p>When opening files with time coordinates, DummyDataset automatically infers and stores the time frequency in the coordinate attributes:</p> <pre><code># Open files with time coordinates\nds = DummyDataset.open_mfdataset(\"hourly_*.nc\", concat_dim=\"time\")\n\n# Frequency is automatically detected and stored\nprint(ds.coords['time'].attrs['frequency'])  # \"1H\"\n\n# Works with various frequencies\n# - Hourly: \"1H\", \"3H\", \"6H\", \"12H\"\n# - Daily: \"1D\"\n# - Monthly: \"1M\"\n# - Sub-hourly: \"15T\" (minutes), \"30S\" (seconds)\n</code></pre> <p>Requirements for frequency inference:</p> <ul> <li>Time coordinate must have CF-compliant <code>units</code> attribute (e.g., \"hours since 2000-01-01\")</li> <li>Time values must be regularly spaced</li> <li>At least 2 time values in the coordinate</li> </ul> <p>Calendar support:</p> <p>The frequency inference respects the <code>calendar</code> attribute if present, supporting all cftime calendars:</p> <ul> <li><code>standard</code> (Gregorian, default)</li> <li><code>noleap</code> (365-day)</li> <li><code>360_day</code></li> <li><code>julian</code></li> <li>And all other cftime calendars</li> </ul>"},{"location":"user-guide/mfdataset/#time-based-grouping","title":"Time-Based Grouping","text":"<p>Once files are opened with frequency inference, you can group the dataset by time periods using <code>groupby_time()</code>. This creates multiple metadata-only datasets, each representing a time period:</p> <pre><code># Open 100 years of hourly data\nds = DummyDataset.open_mfdataset(\"hourly_*.nc\", concat_dim=\"time\")\n\n# Group into decades\ndecades = ds.groupby_time('10Y')\n\nprint(f\"Number of decades: {len(decades)}\")  # 10\n\n# Each decade is a separate DummyDataset\ndecade_0 = decades[0]\nprint(decade_0.coords['time'].attrs['units'])\n# \"hours since 2000-01-01 00:00:00\"\n\nprint(decade_0.dims['time'])\n# ~87600 (10 years * 365.25 days * 24 hours)\n</code></pre>"},{"location":"user-guide/mfdataset/#supported-grouping-frequencies","title":"Supported Grouping Frequencies","text":"<pre><code># Years\ndecades = ds.groupby_time('10Y')\nquinquennials = ds.groupby_time('5Y')\nannual = ds.groupby_time('1Y')\n\n# Months\nquarterly = ds.groupby_time('3M')\nmonthly = ds.groupby_time('1M')\n\n# Days\nweekly = ds.groupby_time('7D')\ndaily = ds.groupby_time('1D')\n\n# Hours (for high-frequency data)\nsix_hourly = ds.groupby_time('6H')\n</code></pre>"},{"location":"user-guide/mfdataset/#unit-normalization","title":"Unit Normalization","text":"<p>By default, <code>groupby_time()</code> normalizes the time units for each group to start at the group's beginning:</p> <pre><code># With normalization (default)\ndecades = ds.groupby_time('10Y', normalize_units=True)\nprint(decades[0].coords['time'].attrs['units'])\n# \"hours since 2000-01-01 00:00:00\"\n\nprint(decades[1].coords['time'].attrs['units'])\n# \"hours since 2010-01-01 00:00:00\"\n\n# Without normalization (keeps original units)\ndecades = ds.groupby_time('10Y', normalize_units=False)\n# All groups keep the original units from the first file\n</code></pre> <p>Difference from xarray</p> <p>This behavior differs from xarray's <code>groupby()</code>, which preserves the original time units and time values across all groups. DummyDataset's default (<code>normalize_units=True</code>) is intentional for generating independent dataset specifications where each group is self-contained.</p> <p>When using <code>populate_with_random_data()</code> to generate actual data, time values start from 0 for each group. With <code>normalize_units=True</code>, this is correct because the units reference each group's start date. With <code>normalize_units=False</code>, the generated time values would not match xarray's behavior (which would have offsets like <code>[365, 366, ...]</code> for year 2).</p>"},{"location":"user-guide/mfdataset/#file-tracking-in-groups","title":"File Tracking in Groups","text":"<p>File tracking information is preserved in grouped datasets:</p> <pre><code>decades = ds.groupby_time('10Y')\n\n# Query which files are in the first decade\nfiles = decades[0].get_source_files()\nprint(f\"Files in decade 0: {files}\")\n# ['hourly_2000.nc', 'hourly_2001.nc', ..., 'hourly_2009.nc']\n</code></pre>"},{"location":"user-guide/mfdataset/#use-cases","title":"Use Cases","text":"<p>Grouping files by temporal frequency with updated units:</p> <p>A common workflow is to group files by a temporal frequency (e.g., yearly, monthly) and identify which files belong to each group\u2014with time units automatically updated for each group:</p> <pre><code># 1. Open multiple files spanning several years\nds = DummyDataset.open_mfdataset(\"hourly_*.nc\", concat_dim=\"time\")\nprint(f\"Original units: {ds.coords['time'].attrs['units']}\")\n# \"hours since 2000-01-01 00:00:00\"\n\n# 2. Group by year\nyearly_groups = ds.groupby_time('1Y')\nprint(f\"Created {len(yearly_groups)} yearly groups\")\n\n# 3. Each group has: identified files + updated temporal units\nfor i, group in enumerate(yearly_groups):\n    files = group.get_source_files()\n    units = group.coords['time'].attrs['units']\n    print(f\"\\nYear {i + 2000}:\")\n    print(f\"  Files: {files}\")\n    print(f\"  Time units: {units}\")\n    print(f\"  Time steps: {group.dims['time']}\")\n\n# Output:\n# Year 2000:\n#   Files: ['hourly_2000.nc']\n#   Time units: hours since 2000-01-01 00:00:00\n#   Time steps: 8760\n# Year 2001:\n#   Files: ['hourly_2001.nc']\n#   Time units: hours since 2001-01-01 00:00:00\n#   Time steps: 8760\n# ...\n</code></pre> <p>This is useful for:</p> <ul> <li>Data partitioning: Split large datasets into manageable chunks for parallel processing</li> <li>Archive organization: Identify which source files belong to each time period</li> <li>Metadata generation: Create per-period specifications with correct temporal references</li> </ul> <p>Climate data analysis planning:</p> <pre><code># Open century of daily climate data\nds = DummyDataset.open_mfdataset(\"tas_day_*.nc\", concat_dim=\"time\")\n\n# Group into decades for analysis\ndecades = ds.groupby_time('10Y')\n\n# Plan processing for each decade\nfor i, decade in enumerate(decades):\n    start_year = 1900 + i * 10\n    print(f\"Decade {start_year}s:\")\n    print(f\"  Time steps: {decade.dims['time']}\")\n    print(f\"  Files: {len(decade.get_source_files())}\")\n    print(f\"  Variables: {list(decade.variables.keys())}\")\n</code></pre> <p>Seasonal grouping:</p> <pre><code># Open annual data\nds = DummyDataset.open_mfdataset(\"data_*.nc\", concat_dim=\"time\")\n\n# Group by season (3 months)\nseasons = ds.groupby_time('3M')\n\n# Process each season\nfor i, season in enumerate(seasons):\n    season_name = ['DJF', 'MAM', 'JJA', 'SON'][i % 4]\n    print(f\"{season_name}: {season.dims['time']} timesteps\")\n</code></pre>"},{"location":"user-guide/mfdataset/#manual-file-tracking","title":"Manual File Tracking","text":"<p>You can also manually track files without opening them:</p> <pre><code>ds = DummyDataset()\nds.enable_file_tracking(concat_dim=\"time\")\n\n# Add file sources with coordinate ranges\nds.add_file_source(\n    \"model_run_001.nc\",\n    coord_range=(0, 365),\n    metadata={\"institution\": \"DKRZ\", \"model\": \"ICON\"}\n)\n\nds.add_file_source(\n    \"model_run_002.nc\",\n    coord_range=(365, 730),\n    metadata={\"institution\": \"DKRZ\", \"model\": \"ICON\"}\n)\n\n# Query files\nfiles = ds.get_source_files()\n</code></pre>"},{"location":"user-guide/mfdataset/#file-validation","title":"File Validation","text":"<p><code>open_mfdataset()</code> validates that files are compatible for concatenation:</p> <pre><code>try:\n    ds = DummyDataset.open_mfdataset(files, concat_dim=\"time\")\nexcept ValueError as e:\n    print(f\"Files are incompatible: {e}\")\n</code></pre> <p>Validation checks:</p> <ul> <li>All files must have the concatenation dimension</li> <li>All files must have the same variables</li> <li>Variables must have compatible dimensions</li> </ul>"},{"location":"user-guide/mfdataset/#properties","title":"Properties","text":""},{"location":"user-guide/mfdataset/#is_file_tracking_enabled","title":"<code>is_file_tracking_enabled</code>","text":"<p>Check if file tracking is enabled:</p> <pre><code>if ds.is_file_tracking_enabled:\n    print(\"File tracking is active\")\n</code></pre>"},{"location":"user-guide/mfdataset/#concat_dim","title":"<code>concat_dim</code>","text":"<p>Get the concatenation dimension:</p> <pre><code>print(f\"Files are concatenated along: {ds.concat_dim}\")\n</code></pre>"},{"location":"user-guide/mfdataset/#file_sources","title":"<code>file_sources</code>","text":"<p>Access all tracked file information:</p> <pre><code>for filepath, info in ds.file_sources.items():\n    print(f\"{filepath}: {info['coord_range']}\")\n</code></pre>"},{"location":"user-guide/mfdataset/#important-notes","title":"Important Notes","text":""},{"location":"user-guide/mfdataset/#coordinate-type-compatibility","title":"Coordinate Type Compatibility","text":"<p>When querying files with <code>get_source_files()</code>, use coordinate types compatible with your data:</p> <ul> <li>If your time coordinate is <code>datetime64</code>, query with datetime objects</li> <li>If your time coordinate is numeric, query with numbers</li> <li>Type mismatches will return all files as a safe default</li> </ul> <pre><code># Example with datetime coordinates\nimport numpy as np\n\n# This works if time is datetime64\nfiles = ds.get_source_files(\n    time=slice(\n        np.datetime64('2020-01-01'),\n        np.datetime64('2020-12-31')\n    )\n)\n\n# This works if time is numeric (e.g., days since epoch)\nfiles = ds.get_source_files(time=slice(0, 365))\n</code></pre>"},{"location":"user-guide/mfdataset/#metadata-only","title":"Metadata Only","text":"<p>Remember that <code>open_mfdataset()</code> only reads metadata:</p> <ul> <li>No data arrays are loaded into memory</li> <li>Coordinate values are read to determine ranges</li> <li>This keeps memory usage minimal</li> </ul>"},{"location":"user-guide/mfdataset/#file-ordering","title":"File Ordering","text":"<p>Files are processed in the order provided (or sorted alphabetically for glob patterns):</p> <pre><code># Glob patterns are sorted\nds = DummyDataset.open_mfdataset(\"data/*.nc\")  # Alphabetical order\n\n# Lists maintain order\nds = DummyDataset.open_mfdataset([\"file3.nc\", \"file1.nc\", \"file2.nc\"])\n</code></pre>"},{"location":"user-guide/mfdataset/#complete-example","title":"Complete Example","text":"<pre><code>from dummyxarray import DummyDataset\nfrom pathlib import Path\n\n# Open multiple climate model files\nds = DummyDataset.open_mfdataset(\n    \"climate_model_output/*.nc\",\n    concat_dim=\"time\"\n)\n\n# Inspect the combined structure\nprint(ds)\nprint(f\"\\nTotal time steps: {ds.dims['time']}\")\nprint(f\"Number of files: {len(ds.file_sources)}\")\n\n# Get file information\nfor filepath in ds.file_sources:\n    info = ds.get_file_info(filepath)\n    filename = Path(filepath).name\n    time_range = info['coord_range']\n    print(f\"\\n{filename}:\")\n    print(f\"  Time range: {time_range[0]} to {time_range[1]}\")\n    print(f\"  Variables: {', '.join(info['metadata']['variables'])}\")\n\n# Query which files contain a specific time range\nrelevant_files = ds.get_source_files()\nprint(f\"\\nFiles to process: {len(relevant_files)}\")\n</code></pre>"},{"location":"user-guide/mfdataset/#api-reference","title":"API Reference","text":"<p>See the API documentation for detailed parameter descriptions.</p>"},{"location":"user-guide/mfdataset/#see-also","title":"See Also","text":"<ul> <li>Basic Usage - General DummyDataset usage</li> <li>Examples - More code examples</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"user-guide/ncdump-import/","title":"Importing from ncdump Headers","text":"<p>dummyxarray can parse <code>ncdump -h</code> output to create dataset structures from existing NetCDF files.</p>"},{"location":"user-guide/ncdump-import/#overview","title":"Overview","text":"<p>The <code>from_ncdump_header()</code> function parses the output of <code>ncdump -h</code> and creates a <code>DummyDataset</code> with the same structure. This is useful for:</p> <ul> <li>Replicating structures - Copy metadata from existing datasets</li> <li>Template creation - Use real datasets as templates</li> <li>Quick inspection - Understand dataset structure without loading data</li> <li>Documentation - Extract and document dataset schemas</li> </ul>"},{"location":"user-guide/ncdump-import/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/ncdump-import/#step-1-get-ncdump-header","title":"Step 1: Get ncdump Header","text":"<pre><code># Generate header from NetCDF file\nncdump -h your_file.nc &gt; header.txt\n</code></pre>"},{"location":"user-guide/ncdump-import/#step-2-import-to-dummydataset","title":"Step 2: Import to DummyDataset","text":"<pre><code>from dummyxarray import from_ncdump_header\n\n# Read the header file\nwith open('header.txt', 'r') as f:\n    header_text = f.read()\n\n# Create DummyDataset\nds = from_ncdump_header(header_text)\n\n# The dataset now has the same structure\nprint(ds)\n</code></pre>"},{"location":"user-guide/ncdump-import/#what-gets-imported","title":"What Gets Imported","text":"<p>The parser extracts:</p> <ul> <li>\u2705 Dimensions - Including UNLIMITED dimensions</li> <li>\u2705 Variables - With dimensions and data types</li> <li>\u2705 Coordinates - Automatically detected</li> <li>\u2705 Attributes - Variable and global attributes</li> <li>\u2705 Metadata - All CF-compliant metadata</li> </ul> <p>Note: Data arrays are NOT imported, only the structure and metadata.</p>"},{"location":"user-guide/ncdump-import/#complete-example","title":"Complete Example","text":"<pre><code>from dummyxarray import from_ncdump_header\n\n# Example ncdump output\nheader = \"\"\"\nnetcdf climate_data {\ndimensions:\n    time = UNLIMITED ; // (365 currently)\n    lat = 64 ;\n    lon = 128 ;\nvariables:\n    double time(time) ;\n        time:units = \"days since 2000-01-01\" ;\n        time:calendar = \"gregorian\" ;\n        time:axis = \"T\" ;\n    double lat(lat) ;\n        lat:units = \"degrees_north\" ;\n        lat:standard_name = \"latitude\" ;\n        lat:axis = \"Y\" ;\n    double lon(lon) ;\n        lon:units = \"degrees_east\" ;\n        lon:standard_name = \"longitude\" ;\n        lon:axis = \"X\" ;\n    float temperature(time, lat, lon) ;\n        temperature:units = \"K\" ;\n        temperature:standard_name = \"air_temperature\" ;\n        temperature:long_name = \"Air Temperature\" ;\n\n// global attributes:\n        :Conventions = \"CF-1.8\" ;\n        :title = \"Climate Model Output\" ;\n}\n\"\"\"\n\n# Import structure\nds = from_ncdump_header(header)\n\n# Check what was imported\nprint(f\"Dimensions: {ds.dims}\")\nprint(f\"Coordinates: {list(ds.coords.keys())}\")\nprint(f\"Variables: {list(ds.variables.keys())}\")\nprint(f\"Global attrs: {ds.attrs}\")\n</code></pre>"},{"location":"user-guide/ncdump-import/#coordinate-detection","title":"Coordinate Detection","text":"<p>The parser automatically identifies coordinates using this rule:</p> <p>A variable is a coordinate if: - It has exactly one dimension - The dimension name matches the variable name</p> <pre><code># These are detected as coordinates:\ndouble time(time) ;      # \u2713 Coordinate\ndouble lat(lat) ;        # \u2713 Coordinate\ndouble lon(lon) ;        # \u2713 Coordinate\n\n# These are detected as variables:\nfloat temperature(time, lat, lon) ;  # \u2717 Variable (multi-dim)\nfloat bounds(lat, nv) ;              # \u2717 Variable (dims don't match name)\n</code></pre>"},{"location":"user-guide/ncdump-import/#handling-unlimited-dimensions","title":"Handling UNLIMITED Dimensions","text":"<p>UNLIMITED dimensions are handled automatically:</p> <pre><code># ncdump shows:\n# time = UNLIMITED ; // (365 currently)\n\n# Parser extracts the current size (365)\nds = from_ncdump_header(header)\nprint(ds.dims['time'])  # 365\n</code></pre> <p>If no current size is specified, the dimension size will be <code>None</code>:</p> <pre><code># time = UNLIMITED ;  (no current size)\n# ds.dims['time'] will be None\n</code></pre>"},{"location":"user-guide/ncdump-import/#working-with-imported-datasets","title":"Working with Imported Datasets","text":"<p>Once imported, you can work with the dataset normally:</p>"},{"location":"user-guide/ncdump-import/#populate-with-data","title":"Populate with Data","text":"<pre><code># Add random data for testing\nds.populate_with_random_data(seed=42)\n\n# Now convert to xarray\nxr_ds = ds.to_xarray()\n</code></pre>"},{"location":"user-guide/ncdump-import/#validate-cf-compliance","title":"Validate CF Compliance","text":"<pre><code># Check CF compliance\nresult = ds.validate_cf()\nprint(f\"Warnings: {len(result['warnings'])}\")\n</code></pre>"},{"location":"user-guide/ncdump-import/#modify-structure","title":"Modify Structure","text":"<pre><code># Add new variables\nds.add_variable(\n    \"humidity\",\n    dims=[\"time\", \"lat\", \"lon\"],\n    attrs={\"units\": \"%\", \"standard_name\": \"relative_humidity\"}\n)\n\n# Update attributes\nds.assign_attrs(history=\"Modified with dummyxarray\")\n</code></pre>"},{"location":"user-guide/ncdump-import/#export-as-template","title":"Export as Template","text":"<pre><code># Save as YAML template\nds.save_yaml(\"template.yaml\")\n\n# Later, load and reuse\nds2 = DummyDataset.load_yaml(\"template.yaml\")\n</code></pre>"},{"location":"user-guide/ncdump-import/#history-tracking","title":"History Tracking","text":"<p>By default, history is recorded when importing:</p> <pre><code>ds = from_ncdump_header(header, record_history=True)\n\n# View construction history\nhistory = ds.get_history()\nprint(f\"Operations: {len(history)}\")\n\n# Export as Python code\npython_code = ds.export_history('python')\nprint(python_code)\n</code></pre> <p>Disable history if not needed:</p> <pre><code>ds = from_ncdump_header(header, record_history=False)\n</code></pre>"},{"location":"user-guide/ncdump-import/#supported-features","title":"Supported Features","text":""},{"location":"user-guide/ncdump-import/#dimensions","title":"Dimensions","text":"<ul> <li>\u2705 Fixed-size dimensions</li> <li>\u2705 UNLIMITED dimensions with current size</li> <li>\u2705 UNLIMITED dimensions without size (\u2192 None)</li> </ul>"},{"location":"user-guide/ncdump-import/#variables","title":"Variables","text":"<ul> <li>\u2705 All NetCDF data types (double, float, int, etc.)</li> <li>\u2705 Multi-dimensional variables</li> <li>\u2705 Coordinate variables</li> <li>\u2705 Variable attributes</li> </ul>"},{"location":"user-guide/ncdump-import/#attributes","title":"Attributes","text":"<ul> <li>\u2705 String attributes</li> <li>\u2705 Numeric attributes (int, float)</li> <li>\u2705 Array attributes</li> <li>\u2705 Global attributes</li> </ul>"},{"location":"user-guide/ncdump-import/#not-supported","title":"Not Supported","text":"<ul> <li>\u274c Data arrays (only structure)</li> <li>\u274c Groups (NetCDF-4 feature)</li> <li>\u274c User-defined types</li> <li>\u274c Compound types</li> </ul>"},{"location":"user-guide/ncdump-import/#practical-workflows","title":"Practical Workflows","text":""},{"location":"user-guide/ncdump-import/#workflow-1-replicate-existing-dataset","title":"Workflow 1: Replicate Existing Dataset","text":"<pre><code># Get structure from existing file\n!ncdump -h existing_data.nc &gt; structure.txt\n\n# Import structure\nwith open('structure.txt') as f:\n    ds = from_ncdump_header(f.read())\n\n# Populate with new data\nds.populate_with_random_data()\n\n# Save as new file\nds.to_zarr(\"new_data.zarr\")\n</code></pre>"},{"location":"user-guide/ncdump-import/#workflow-2-document-dataset-schema","title":"Workflow 2: Document Dataset Schema","text":"<pre><code># Import structure\nds = from_ncdump_header(header_text)\n\n# Export as YAML documentation\nds.save_yaml(\"dataset_schema.yaml\")\n\n# Export history as Python script\nwith open('create_dataset.py', 'w') as f:\n    f.write(ds.export_history('python'))\n</code></pre>"},{"location":"user-guide/ncdump-import/#workflow-3-validate-and-fix-metadata","title":"Workflow 3: Validate and Fix Metadata","text":"<pre><code># Import existing structure\nds = from_ncdump_header(header_text)\n\n# Check CF compliance\nresult = ds.validate_cf()\n\n# Fix issues\nds.infer_axis()\nds.set_axis_attributes()\n\n# Re-validate\nresult = ds.validate_cf()\nprint(f\"Warnings: {len(result['warnings'])}\")\n</code></pre>"},{"location":"user-guide/ncdump-import/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li>Always validate - Run <code>validate_cf()</code> after importing</li> <li>Check coordinates - Verify coordinate detection is correct</li> <li>Handle UNLIMITED - Be aware of None dimension sizes</li> <li>Add encoding - Set chunks and compression for new data</li> <li>Document changes - Use history tracking for reproducibility</li> </ol>"},{"location":"user-guide/ncdump-import/#limitations","title":"Limitations","text":"<ul> <li>Only parses metadata, not data</li> <li>Assumes standard ncdump format</li> <li>May not handle all edge cases</li> <li>Groups and complex types not supported</li> </ul>"},{"location":"user-guide/ncdump-import/#see-also","title":"See Also","text":"<ul> <li>CF Compliance - Validate imported datasets</li> <li>History Tracking - Track modifications</li> <li>YAML Export - Save as templates</li> <li>Examples - More ncdump import examples</li> </ul>"},{"location":"user-guide/validation/","title":"Validation","text":"<p>Learn how to validate your dataset structure to catch errors early.</p>"},{"location":"user-guide/validation/#basic-validation","title":"Basic Validation","text":"<pre><code>from dummyxarray import DummyDataset\nimport numpy as np\n\nds = DummyDataset()\nds.add_dim(\"time\", 10)\ndata = np.random.rand(10)\nds.add_variable(\"temperature\", dims=[\"time\"], data=data)\n\n# Validate the dataset\nds.validate()  # Raises ValueError if validation fails\n</code></pre>"},{"location":"user-guide/validation/#what-gets-validated","title":"What Gets Validated","text":"<p>The validation checks for:</p> <ol> <li>Unknown dimensions: Variables reference dimensions that don't exist</li> <li>Shape mismatches: Data shape doesn't match declared dimensions</li> <li>Missing coordinates (optional): Variables use dimensions without corresponding coordinates</li> </ol>"},{"location":"user-guide/validation/#dimension-checks","title":"Dimension Checks","text":"<pre><code>ds = DummyDataset()\nds.add_variable(\"temp\", dims=[\"unknown_dim\"])\n\ntry:\n    ds.validate()\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    # Error: Dataset validation failed:\n    # temp: Unknown dimension 'unknown_dim'.\n</code></pre>"},{"location":"user-guide/validation/#shape-validation","title":"Shape Validation","text":"<p>Shape mismatches are caught when adding variables:</p> <pre><code>ds = DummyDataset()\nds.add_dim(\"time\", 10)\nds.add_dim(\"lat\", 5)\n\n# This will raise an error immediately\ndata = np.random.rand(10, 6)  # Wrong shape!\ntry:\n    ds.add_variable(\"test\", dims=[\"time\", \"lat\"], data=data)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    # Error: Dimension mismatch for 'lat': existing=5 new=6\n</code></pre>"},{"location":"user-guide/validation/#strict-coordinate-validation","title":"Strict Coordinate Validation","text":"<p>Enable strict mode to require coordinates for all dimensions:</p> <pre><code>ds = DummyDataset()\nds.add_dim(\"time\", 10)\ndata = np.random.rand(10)\nds.add_variable(\"temp\", dims=[\"time\"], data=data)\n\n# Strict validation requires coordinates\ntry:\n    ds.validate(strict_coords=True)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    # Error: temp: Missing coordinate for dimension 'time'.\n</code></pre>"},{"location":"user-guide/validation/#validation-in-workflows","title":"Validation in Workflows","text":"<p>Validation is automatically called before conversion:</p> <pre><code># Validation happens automatically\nxr_ds = ds.to_xarray(validate=True)  # Default\n\n# Skip validation if needed (not recommended)\nxr_ds = ds.to_xarray(validate=False)\n</code></pre>"},{"location":"user-guide/yaml-export/","title":"YAML Export","text":"<p>Export and import dataset specifications using YAML format.</p>"},{"location":"user-guide/yaml-export/#exporting-to-yaml","title":"Exporting to YAML","text":"<pre><code>from dummyxarray import DummyDataset\n\nds = DummyDataset()\nds.set_global_attrs(title=\"My Dataset\")\nds.add_dim(\"time\", 12)\nds.add_variable(\"temperature\", [\"time\"], attrs={\"units\": \"K\"})\n\n# Get YAML string\nyaml_str = ds.to_yaml()\nprint(yaml_str)\n\n# Save to file\nds.save_yaml(\"dataset_spec.yaml\")\n</code></pre>"},{"location":"user-guide/yaml-export/#yaml-structure","title":"YAML Structure","text":"<p>The exported YAML contains:</p> <pre><code>dimensions:\n  time: 12\n  lat: 64\n  lon: 128\ncoordinates:\n  time:\n    dims:\n    - time\n    attrs:\n      units: days since 2000-01-01\n    encoding:\n      dtype: int32\n    has_data: true\nvariables:\n  temperature:\n    dims:\n    - time\n    - lat\n    - lon\n    attrs:\n      long_name: Temperature\n      units: K\n    encoding:\n      dtype: float32\n      chunks: [6, 32, 64]\n    has_data: true\nattrs:\n  title: My Dataset\n  institution: DKRZ\n</code></pre>"},{"location":"user-guide/yaml-export/#loading-from-yaml","title":"Loading from YAML","text":"<pre><code>from dummyxarray import DummyDataset\n\n# Load specification\nds = DummyDataset.load_yaml(\"dataset_spec.yaml\")\n\n# The structure is loaded, but not the data\nprint(ds.dims)\nprint(ds.variables.keys())\n\n# Add data later\nimport numpy as np\nds.variables[\"temperature\"].data = np.random.rand(12, 64, 128)\n\n# Convert to xarray\nxr_ds = ds.to_xarray()\n</code></pre>"},{"location":"user-guide/yaml-export/#use-cases","title":"Use Cases","text":""},{"location":"user-guide/yaml-export/#1-documentation","title":"1. Documentation","text":"<p>Export dataset specifications for documentation:</p> <pre><code>ds.save_yaml(\"docs/dataset_specification.yaml\")\n</code></pre>"},{"location":"user-guide/yaml-export/#2-templates","title":"2. Templates","text":"<p>Create reusable templates:</p> <pre><code># Create template\ntemplate = DummyDataset()\ntemplate.set_global_attrs(Conventions=\"CF-1.8\")\ntemplate.add_dim(\"time\", None)  # Placeholder\ntemplate.add_variable(\"temperature\", [\"time\"], attrs={\"units\": \"K\"})\ntemplate.save_yaml(\"templates/temperature_timeseries.yaml\")\n\n# Use template\nds = DummyDataset.load_yaml(\"templates/temperature_timeseries.yaml\")\nds.dims[\"time\"] = 365  # Set actual size\n</code></pre>"},{"location":"user-guide/yaml-export/#3-version-control","title":"3. Version Control","text":"<p>Track dataset structure changes in git:</p> <pre><code>git add dataset_spec.yaml\ngit commit -m \"Update dataset structure\"\n</code></pre>"},{"location":"user-guide/yaml-export/#4-collaboration","title":"4. Collaboration","text":"<p>Share specifications with collaborators:</p> <pre><code># Person A creates spec\nds = DummyDataset()\nds.set_global_attrs(title=\"Shared Dataset\")\nds.add_dim(\"time\", 100)\nds.save_yaml(\"shared_spec.yaml\")\n\n# Person B loads and uses\nds_loaded = DummyDataset.load_yaml(\"shared_spec.yaml\")\n# Add their data...\n</code></pre>"},{"location":"user-guide/yaml-export/#json-export","title":"JSON Export","text":"<p>You can also export to JSON:</p> <pre><code># Get JSON string\njson_str = ds.to_json()\n\n# Or as dictionary\nspec_dict = ds.to_dict()\n</code></pre>"},{"location":"user-guide/yaml-export/#limitations","title":"Limitations","text":"<p>Note that the actual data arrays are not saved to YAML, only the metadata and structure. The <code>has_data</code> field indicates whether data was present when the spec was created.</p>"}]}